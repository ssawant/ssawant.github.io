[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Santosh Sawant",
    "section": "",
    "text": "I am a Machine Learning Architect working on Generative AI platform and products in the Data Analytics, Healthcare and ESG domain."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "",
    "section": "",
    "text": "Instruction-Tune Llama2 with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Fine-Tune LLMs with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMerge Model using Mergekit\n\n\n\n\n\n\ntools\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "title": "Merge Model using Mergekit",
    "section": "",
    "text": "Model merging is a technique that combines two or more LLMs into a single model. It‚Äôs a relatively new and experimental method to create new models for cheap (no GPU required). Model merging works surprisingly well and produced many state-of-the-art models on the Open LLM Leaderboard.\nIn this tutorial, we will implement it using the mergekit library. More specifically, we will review four merge methods and provide examples of configurations. Then, we will use mergekit to create our own model"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "title": "Merge Model using Mergekit",
    "section": "Merge models",
    "text": "Merge models\n\n!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle\n\nFetching 8 files: 100% 8/8 [00:00&lt;00:00, 18275.83it/s]\nFetching 11 files: 100% 11/11 [00:00&lt;00:00, 21670.90it/s]\n  0% 0/291 [00:00&lt;?, ?it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for model.embed_tokens.weight\n 70% 203/291 [02:58&lt;00:44,  1.98it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for lm_head.weight\n100% 291/291 [04:22&lt;00:00,  1.11it/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n!pip install -qU huggingface_hub\n\nfrom huggingface_hub import ModelCard, ModelCardData\nfrom jinja2 import Template\n\nusername = \"santoshsawant\"\n\ntemplate_text = \"\"\"\n---\nlicense: apache-2.0\ntags:\n- merge\n- mergekit\n- lazymergekit\n{%- for model in models %}\n- {{ model }}\n{%- endfor %}\n---\n\n# {{ model_name }}\n\n{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n\n{%- for model in models %}\n* [{{ model }}](https://huggingface.co/{{ model }})\n{%- endfor %}\n\n## Configuration\n\n```yaml\n{{- yaml_config -}}\n```\n\"\"\"\n\n# Create a Jinja template object\njinja_template = Template(template_text.strip())\n\n# Get list of models from config\ndata = yaml.safe_load(yaml_config)\nif \"models\" in data:\n    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\nelif \"parameters\" in data:\n    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\nelif \"slices\" in data:\n    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\nelse:\n    raise Exception(\"No models or slices found in yaml config\")\n\n# Fill the template\ncontent = jinja_template.render(\n    model_name=MODEL_NAME,\n    models=models,\n    yaml_config=yaml_config,\n    username=username,\n)\n\n# Save the model card\ncard = ModelCard(content)\ncard.save('merge/README.md')\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/330.1 kB ? eta -:--:--     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 61.4/330.1 kB 2.2 MB/s eta 0:00:01     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 327.7/330.1 kB 5.3 MB/s eta 0:00:01     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 330.1/330.1 kB 4.6 MB/s eta 0:00:00\n\n\n\nfrom google.colab import userdata\nfrom huggingface_hub import HfApi\n\nusername = \"santoshsawant\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=userdata.get(\"huggingface\"))\n\napi.create_repo(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    repo_type=\"model\"\n)\napi.upload_folder(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    folder_path=\"merge\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/santoshsawant/NeuralHermes-7B-slerp/commit/57dae104809557372cabe688eb608448d32fa485', commit_message='Upload folder using huggingface_hub', commit_description='', oid='57dae104809557372cabe688eb608448d32fa485', pr_url=None, pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "1. Define our use case",
    "text": "1. Define our use case\nWhen fine-tuning LLMs, it is important you know your use case and the task you want to solve. This will help you to choose the right model or help you to create a dataset to fine-tune your model. If you haven‚Äôt defined your use case yet. You might want to go back to the drawing board. I want to mention that not all use cases require fine-tuning and it is always recommended to evaluate and try out already fine-tuned models or API-based models before fine-tuning your own model.\nAs an example, we are going to use the following use case:\n\nWe want to fine-tune a model, which can generate SQL queries based on a natural language instruction, which can then be integrated into our BI tool. The goal is to reduce the time it takes to create a SQL query and make it easier for non-technical users to create SQL queries.\n\nText to SQL can be a good use case for fine-tuning LLMs, as it is a complex task that requires a lot of (internal) knowledge about the data and the SQL language."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "2. Setup development environment",
    "text": "2. Setup development environment\nOur first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven‚Äôt heard of trl yet, don‚Äôt worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n\n# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  # \"trl==0.7.10\" # \\\n  # \"peft==0.7.1\" \\\n\n# install peft & trl from github\n!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f--upgrade\n\nIf you are using a GPU with Ampere architecture (e.g.¬†NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention.\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\n\nimport torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\n# install flash-attn\n!pip install ninja packaging\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nWe will use the Hugging Face Hub as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the Hugging Face for this. After you have an account, we will use the login util from the huggingface_hub package to log into our account and store our token (access key) on the disk.\n\nfrom huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "3. Create and prepare the dataset",
    "text": "3. Create and prepare the dataset\nOnce you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. The dataset should be a diverse set of demonstrations of the task you want to solve. There are several ways to create such a dataset, including: * Using existing open-source datasets, e.g., Spider * Using LLMs to create synthetically datasets, e.g., Alpaca * Using Humans to create datasets, e.g., Dolly. * Using a combination of the above methods, e.g., Orca\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nIn our example we will use an already existing dataset called sql-create-context, which contains samples of natural language instructions, schema definitions and the corresponding SQL query.\nWith the latest release of trl we now support popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and trl will take care of the rest. Those formats include: * conversational format\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n\ninstruction format\n\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\nIn our example we are going to load our open-source dataset using the ü§ó Datasets library and then convert it into the the conversational format, where we include the schema definition in the system message for our assistant. We‚Äôll then save the dataset as jsonl file, which we can then use to fine-tune our model. We are randomly downsampling the dataset to only 10,000 samples.\nNote: This step can be different for your use case. For example, if you have already a dataset from, e.g.¬†working with OpenAI, you can skip this step and go directly to the fine-tuning step.\n\nfrom datasets import load_dataset\n\n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\nSCHEMA:\n{schema}\"\"\"\n\ndef create_conversation(sample):\n  return {\n    \"messages\": [\n      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n      {\"role\": \"user\", \"content\": sample[\"question\"]},\n      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n    ]\n  }  \n\n# Load dataset from the hub\ndataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\n\n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=2500/12500)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk \ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Fine-tune LLM using trl and the SFTTrainer",
    "text": "4. Fine-tune LLM using trl and the SFTTrainer\nWe are now ready to fine-tune our model. We will use the SFTTrainer from trl to fine-tune our model. The SFTTrainer makes it straightfoward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including: * Dataset formatting, including conversational and instruction format * Training on completions only, ignoring prompts * Packing datasets for more efficient training * PEFT (parameter-efficient fine-tuning) support including Q-LoRA * Preparing the model and tokenizer for conversational fine-tuning (e.g.¬†adding special tokens)\nWe will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out¬†Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA¬†blog post.\nNow, lets get started! üöÄ\nFirst, we need to load our dataset from disk.\n\nfrom datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n\nNext, we will load our LLM. For our use case we are going to use CodeLlama 7B. CodeLlama is a Llama model trained for general code synthesis and understanding. But we can easily swap out the model for another model, e.g.¬†Mistral or Mixtral models, TII Falcon, or any other LLMs by changing our model_id variable. We will use bitsandbytes to quantize our model to 4-bit.\nNote: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU.\nCorrectly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In trl we have a convinient method called setup_chat_format, which: * Adds special tokens to the tokenizer, e.g.¬†&lt;|im_start|&gt; and &lt;|im_end|&gt;, to indicate the start and end of a conversation. * Resizes the model‚Äôs embedding layer to accommodate the new tokens. * Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import setup_chat_format\n\n# Hugging Face model id\nmodel_id = \"codellama/CodeLlama-7b-hf\" # or `mistralai/Mistral-7B-v0.1`\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'right' # to prevent warnings\n\n# # set chat template to OAI chatML, remove if you start from a fine-tuned model\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\nThe¬†SFTTrainer¬† supports a native integration with¬†peft, which makes it super easy to efficiently tune LLMs using, e.g.¬†QLoRA. We only need to create our¬†LoraConfig¬†and provide it to the trainer. Our LoraConfig parameters are defined based on the qlora paper and sebastian‚Äôs blog post.\n\nfrom peft import LoraConfig\n\n# LoRA config based on QLoRA paper & Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=128,\n        lora_dropout=0.05,\n        r=256,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\", \n)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n    num_train_epochs=3,                     # number of training epochs\n    per_device_train_batch_size=3,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=10,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to hub\n    report_to=\"tensorboard\",                # report metrics to tensorboard\n)\n\nWe now have every building block we need to create our¬†SFTTrainer¬†to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 3072 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n\nStart training our model by calling the train() method on our Trainer instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n\n# start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model \ntrainer.save_model()\n\nThe training with Flash Attention for 3 epochs with a dataset of 10k samples took 01:29:58 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of only 1.8$.\n\n# free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n\n\nOptional: Merge LoRA adapter in to the original model\nWhen using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. This will save a default model, which can be used for inference.\nNote: You might require &gt; 30GB CPU Memory.\n\n\n#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import PeftModel, PeftConfig\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import AutoPeftModelForCausalLM\n\n# # Load PEFT model on CPU\n# config = PeftConfig.from_pretrained(args.output_dir)\n# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\n# tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n# model.resize_token_embeddings(len(tokenizer))\n# model = PeftModel.from_pretrained(model, args.output_dir)\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     args.output_dir,\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )  \n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\nNote: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out Evaluate LLMs and RAG a practical example using Langchain and Hugging Face blog post.\n\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline \n\npeft_model_id = \"./code-llama-7b-text-to-sql\"\n# peft_model_id = args.output_dir\n\n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nLet‚Äôs load our test dataset try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randint\n\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Test on sample \nprompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n\nNice! Our model was able to generate a SQL query based on the natural language instruction. Lets evaluate our model on the full 2,500 samples of our test dataset. Note: As mentioned above, evaluating generative models is not a trivial task. In our example we used the accuracy of the generated SQL based on the ground truth SQL query as our metric. An alternative way could be to automatically execute the generated SQL query and compare the results with the ground truth. This would be a more accurate metric but requires more work to setup.\n\nfrom tqdm import tqdm\n\n\ndef evaluate(sample):\n    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n        return 1 \n    else:\n        return 0\n\nsuccess_rate = []\nnumber_of_eval_samples = 1000\n# iterate over eval dataset and predict\nfor s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n    success_rate.append(evaluate(s))\n\n# compute accuracy\naccuracy = sum(success_rate)/len(success_rate)\n\nprint(f\"Accuracy: {accuracy*100:.2f}%\")  \n        \n\nWe evaluated our model on 1000 samples from the evaluation dataset and got an accuracy of 79.50%, which took ~25 minutes. This is quite good, but as mentioned you need to take this metric with a grain of salt. It would be better if we could evaluate our model by running the qureies against a real database and compare the results. Since there might be different ‚Äúcorrect‚Äù SQL queries for the same instruction. There are also several ways on how we could improve the performance by using few-shot learning, using RAG, Self-healing to generate the SQL query."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "6. Deploy the LLM for Production",
    "text": "6. Deploy the LLM for Production\nYou can now deploy your model to production. For deploying open LLMs into production we recommend using Text Generation Inference (TGI). TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching for the most popular open LLMs, including Llama, Mistral, Mixtral, StarCoder, T5 and more. Text Generation Inference is used by companies as IBM, Grammarly, Uber, Deutsche Telekom, and many more. There are several ways to deploy your model, including:\n\nDeploy LLMs with Hugging Face Inference Endpoints\nHugging Face LLM Inference Container for Amazon SageMaker\nDIY\n\nIf you have docker installed you can use the following command to start the inference server.\nNote: Make sure that you have enough GPU memory to run the container. Restart kernel to remove all allocated GPU memory from the notebook.\n\n%%bash \n# model=$PWD/{args.output_dir} # path to model\nmodel=$(pwd)/code-llama-7b-text-to-sql # path to model\nnum_shard=1             # number of shards\nmax_input_length=1024   # max input length\nmax_total_tokens=2048   # max total tokens\n\ndocker run -d --name tgi --gpus all -ti -p 8080:80 \\\n  -e MODEL_ID=/workspace \\\n  -e NUM_SHARD=$num_shard \\\n  -e MAX_INPUT_LENGTH=$max_input_length \\\n  -e MAX_TOTAL_TOKENS=$max_total_tokens \\\n  -v $model:/workspace \\\n  ghcr.io/huggingface/text-generation-inference:latest\n\nOnce your container is running you can send requests.\n\nimport requests as r \nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom random import randint\n\n# Load our test dataset and Tokenizer again\ntokenizer = AutoTokenizer.from_pretrained(\"code-llama-7b-text-to-sql\")\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# generate the same prompt as for the first local test\nprompt = tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\nrequest= {\"inputs\":prompt,\"parameters\":{\"temperature\":0.2, \"top_p\": 0.95, \"max_new_tokens\": 256}}\n\n# send request to inference server\nresp = r.post(\"http://127.0.0.1:8080/generate\", json=request)\n\noutput = resp.json()[\"generated_text\"].strip()\ntime_per_token = resp.headers.get(\"x-time-per-token\")\ntime_prompt_tokens = resp.headers.get(\"x-prompt-tokens\")\n\n# Print results\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{output}\")\nprint(f\"Latency per token: {time_per_token}ms\")\nprint(f\"Latency prompt encoding: {time_prompt_tokens}ms\")\n\nAwesome, Don‚Äôt forget to stop your container once you are done.\n\n!docker stop tgi"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "1. Define our use case in detail and create a template for our instructions",
    "text": "1. Define our use case in detail and create a template for our instructions\nBefore we describe our use case, we need to better understand what even is an instruction.\n\nAn instruction is a piece of text or prompt that is provided to an LLM, like Llama, GPT-4, or Claude, to guide it to generate a response. Instructions allow humans to steer the conversation and constrain the language model‚Äôs output to be more natural, useful, and aligned with the user‚Äôs goals. Crafting clear, well-formulated instructions is key to productive conversations.\n\nExamples of instructions are listed below in the table.\n\n\n\n\n\n\n\nCapability\nExample Instruction\n\n\n\n\nBrainstorming\nProvide a diverse set of creative ideas for new flavors of ice cream.\n\n\nClassification\nCategorize these movies as either comedy, drama, or horror based on the plot summary.\n\n\nClosed QA\nAnswer the question ‚ÄòWhat is the capital of France?‚Äô with a single word.\n\n\nGeneration\nWrite a poem in the style of Robert Frost about nature and the changing seasons.\n\n\nInformation Extraction\nExtract the names of the main characters from this short story.\n\n\nOpen QA\nWhy do leaves change color in autumn? Explain the scientific reasons.\n\n\nSummarization\nSummarize this article on recent advancements in renewable energy in 2-3 sentences.\n\n\n\nAs described in the beginning, we want to fine-tune a model to be able to generate instructions based on input. (output). We want to use this as a way to create synthetic datasets to personalize LLMs and Agents.\nConverting the idea into a basic prompt template following the Alpaca format we get.\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nDear [boss name],\n\nI'm writing to request next week, August 1st through August 4th,\noff as paid time off.\n\nI have some personal matters to attend to that week that require \nme to be out of the office. I wanted to give you as much advance \nnotice as possible so you can plan accordingly while I am away.\n\nPlease let me know if you need any additional information from me \nor have any concerns with me taking next week off. I appreciate you \nconsidering this request.\n\nThank you, [Your name]\n\n### Response:\nWrite an email to my boss that I need next week 08/01 - 08/04 off."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "2. Create an instruction dataset",
    "text": "2. Create an instruction dataset\nAfter we defined our use case and prompt template, we need to create our instruction dataset. Creating a high-quality instruction dataset is key for a good-performing model. Research shows that ‚ÄúLess Is More for Alignment‚Äù shows that creating a high-quality, low-quantity (~1000 samples) dataset can achieve the same performance as less-quality and high-quantity datasets.\nThere are several ways to create an instruction dataset, including:\n\nUsing an existing dataset and converting it into an instruction dataset, e.g., FLAN\nUse existing LLMs to create synthetically instruction datasets, e.g., Alpaca\nUse Humans to create instructions datasets, e.g., Dolly.\n\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nTo keep it simple, we are going to use Dolly¬†an open-source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the¬†InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nLet‚Äôs start coding, but first, let‚Äôs install our dependencies.\n\n!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors&gt;=0.3.1\" --upgrade\n\nTo load the¬†databricks/databricks-dolly-15k¬†dataset, we use the¬†load_dataset()¬†method from the ü§ó Datasets library.\n\nfrom datasets import load_dataset\nfrom random import randrange\n\n# Load dataset from the hub\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\nprint(f\"dataset size: {len(dataset)}\")\nprint(dataset[randrange(len(dataset))])\n# dataset size: 15011\n\nFound cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n\n\ndataset size: 15011\n{'instruction': 'On what month and day was Antwan Deon Odom born?', 'context': 'Antwan Deon Odom (born September 24, 1981) is a former American football defensive end. He was drafted by the Tennessee Titans in the second round of the 2004 NFL Draft. He played college football at Alabama. He has also played for the Cincinnati Bengals.', 'response': 'September 24', 'category': 'closed_qa'}\n\n\nTo instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a¬†formatting_function¬†that takes a sample and returns a string with our format instruction.\n\ndef format_instruction(sample):\n    return f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n{sample['instruction']}\n\"\"\"\n\nLet‚Äôs test our formatting function on a random example.\n\nfrom random import randrange\n\nprint(format_instruction(dataset[randrange(len(dataset))]))\n\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nSir Dorabji Tata and Allied Trusts and Sir Ratan Tata Trust\n\n### Response:\nWhat are the names of Tata trusts which Ratan Tata heads?"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "3. Instruction-tune Llama 2 using trl and the SFTTrainer",
    "text": "3. Instruction-tune Llama 2 using trl and the SFTTrainer\nWe will use the recently introduced method in the paper ‚ÄúQLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation‚Äù by Tim Dettmers et al.¬†QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:\n\nQuantize the pre-trained model to 4 bits and freeze it.\nAttach small, trainable adapter layers. (LoRA)\nFinetune only the adapter layers while using the frozen quantized model for context.\n\nIf you want to learn more about QLoRA and how it works, I recommend you to read the¬†Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA¬†blog post.\n\nFlash Attention\nFlash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper ‚ÄúFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness‚Äù. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention. Flash Attention is currently only available for Ampere (A10, A40, A100, ‚Ä¶) & Hopper (H100, ‚Ä¶) GPUs. You can check if your GPU is supported and install it using the following command:\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\npython -c \"import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\"\npip install ninja packaging\nMAX_JOBS=4 pip install flash-attn --no-build-isolation\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nThe example supports the use of Flash Attention for all Llama checkpoints, but is not enabled by default. To use Flash Attention change the value of use_flash_attentin to True\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nuse_flash_attention = False\n\n# Hugging Face model id\nmodel_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    use_flash_attention_2=use_flash_attention,\n    device_map=\"auto\",\n)\nmodel.config.pretraining_tp = 1\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nThe¬†SFTTrainer¬† supports a native integration with¬†peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our¬†LoRAConfig¬†and provide it to the trainer.\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# LoRA config based on QLoRA paper\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\", \n)\n\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"llama-7-int4-dolly\",\n    num_train_epochs=3,\n    per_device_train_batch_size=6 if use_flash_attention else 4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    bf16=True,\n    fp16=False,\n    tf32=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    disable_tqdm=False,  # disable tqdm since with packing values are in correct\n)\n\n\n# Upcast layer for flash attnetion\nif use_flash_attention:\n    from utils.llama_patch import upcast_layer_for_flash_attention\n    torch_dtype = torch.bfloat16 if args.bf16 else torch.float16 if args.fp16 else torch.float32\n    model = upcast_layer_for_flash_attention(model, torch_dtype)\n\nmodel = get_peft_model(model, peft_config)\n\nWe now have every building block we need to create our¬†SFTTrainer¬†to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=format_instruction, \n    args=args,\n)\n\nStart training our model by calling the train() method on our Trainer instance.\n\n# train\ntrainer.train() # there will not be a progress bar since tqdm is disabled\n\n# save model\ntrainer.save_model()\n\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\nThe training without Flash Attention enabled took 03:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 3.7$. The training with Flash Attention enabled took 02:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 2.6$.\nThe results using Flash Attention are mind blowing and impressive, 1.5x faster and 30% cheaper."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model.\n\nif use_flash_attention:\n    # unpatch flash attention\n    from utils.llama_patch import unplace_flash_attn_with_attn\n    unplace_flash_attn_with_attn()\n    \nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\nargs.output_dir = \"llama-7-int4-dolly\"\n\n# load base LLM model and tokenizer\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n) \ntokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n\nLet‚Äôs load the dataset again with a random sample to try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\nsample = dataset[randrange(len(dataset))]\n\nprompt = f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n\nprint(f\"Prompt:\\n{sample['response']}\\n\")\nprint(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\nprint(f\"Ground truth:\\n{sample['instruction']}\")\n\nNice! our model works! If want to accelerate our model we can deploy it with Text Generation Inference. Therefore we would need to merge our adapter weights into the base model.\n\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n) \n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\n\n# push merged model to the hub\n# merged_model.push_to_hub(\"user/repo\")\n# tokenizer.push_to_hub(\"user/repo\")"
  }
]