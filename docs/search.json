[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Santosh Sawant",
    "section": "",
    "text": "I am a Machine Learning Architect working on Generative AI platform and products in the Data Analytics, Healthcare and ESG domain."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "",
    "section": "",
    "text": "Graph Mamba: Towards Learning on Graphs with State Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFiddler: CPU-GPU Orchestration for Fast Local Inference of MoE Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPHATGOOSE: Learning to Route Among Specialized Experts for Zero-Shot Generalization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTag-LLM: Repurposing General-Purpose LLMs for Specialized Domains\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHydragen: High-Throughput LLM Inference with Shared Prefixes\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMambaFormer: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBlackMamba: Mixture of Experts for State-Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRepeat After Me: Transformers are Better than State Space Models at Copying\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRe3val: Reinforced and Reranked Generative Retrieval\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFIND: INterface for Foundation models‚Äô embeDDings\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEAGLE: Extrapolation Algorithm for Greater Language-model Efficiency\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMambaByte: Token-free Selective State Space Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInstruction-Tune Llama2 with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTowards Conversational Diagnostic AI\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChatQA: Building GPT-4 Level Conversational QA Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Fine-Tune LLMs with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMerge Model using Mergekit\n\n\n\n\n\n\ntools\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTuning Language Models by Proxy\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Evaluation Improves Selective Generation in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReciprocal Rank Fusion (RRF) with LambdaMART: Context Tuning for Retrieval Augmented Generation (RAG)\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChain of Thought (CoT): The Impact of Reasoning Step Length on Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInfinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSoaring from 4K to 400K: Extending LLM‚Äôs Context with Activation Beacon\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDOCLLM: A Layout Aware Generative Language Models for Multi model document understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Play Fine-Tuning (SPIN): Converts Weak Language Models to Strong Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nA Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMamba-Chat: A Chat LLM based on State Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nKwaiAgents: Generalized Information-seeking Agent System with LLMs - 2 Open-source models fine tuned for agent systems! Better than GPT-3.5 turbo as an agent!\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "title": "Merge Model using Mergekit",
    "section": "",
    "text": "Model merging is a technique that combines two or more LLMs into a single model. It‚Äôs a relatively new and experimental method to create new models for cheap (no GPU required). Model merging works surprisingly well and produced many state-of-the-art models on the Open LLM Leaderboard.\nIn this tutorial, we will implement it using the mergekit library. More specifically, we will review four merge methods and provide examples of configurations. Then, we will use mergekit to create our own model"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "title": "Merge Model using Mergekit",
    "section": "Merge models",
    "text": "Merge models\n\n!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle\n\nFetching 8 files: 100% 8/8 [00:00&lt;00:00, 18275.83it/s]\nFetching 11 files: 100% 11/11 [00:00&lt;00:00, 21670.90it/s]\n  0% 0/291 [00:00&lt;?, ?it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for model.embed_tokens.weight\n 70% 203/291 [02:58&lt;00:44,  1.98it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for lm_head.weight\n100% 291/291 [04:22&lt;00:00,  1.11it/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n!pip install -qU huggingface_hub\n\nfrom huggingface_hub import ModelCard, ModelCardData\nfrom jinja2 import Template\n\nusername = \"santoshsawant\"\n\ntemplate_text = \"\"\"\n---\nlicense: apache-2.0\ntags:\n- merge\n- mergekit\n- lazymergekit\n{%- for model in models %}\n- {{ model }}\n{%- endfor %}\n---\n\n# {{ model_name }}\n\n{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n\n{%- for model in models %}\n* [{{ model }}](https://huggingface.co/{{ model }})\n{%- endfor %}\n\n## Configuration\n\n```yaml\n{{- yaml_config -}}\n```\n\"\"\"\n\n# Create a Jinja template object\njinja_template = Template(template_text.strip())\n\n# Get list of models from config\ndata = yaml.safe_load(yaml_config)\nif \"models\" in data:\n    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\nelif \"parameters\" in data:\n    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\nelif \"slices\" in data:\n    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\nelse:\n    raise Exception(\"No models or slices found in yaml config\")\n\n# Fill the template\ncontent = jinja_template.render(\n    model_name=MODEL_NAME,\n    models=models,\n    yaml_config=yaml_config,\n    username=username,\n)\n\n# Save the model card\ncard = ModelCard(content)\ncard.save('merge/README.md')\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/330.1 kB ? eta -:--:--     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 61.4/330.1 kB 2.2 MB/s eta 0:00:01     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 327.7/330.1 kB 5.3 MB/s eta 0:00:01     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 330.1/330.1 kB 4.6 MB/s eta 0:00:00\n\n\n\nfrom google.colab import userdata\nfrom huggingface_hub import HfApi\n\nusername = \"santoshsawant\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=userdata.get(\"huggingface\"))\n\napi.create_repo(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    repo_type=\"model\"\n)\napi.upload_folder(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    folder_path=\"merge\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/santoshsawant/NeuralHermes-7B-slerp/commit/57dae104809557372cabe688eb608448d32fa485', commit_message='Upload folder using huggingface_hub', commit_description='', oid='57dae104809557372cabe688eb608448d32fa485', pr_url=None, pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "1. Define our use case",
    "text": "1. Define our use case\nWhen fine-tuning LLMs, it is important you know your use case and the task you want to solve. This will help you to choose the right model or help you to create a dataset to fine-tune your model. If you haven‚Äôt defined your use case yet. You might want to go back to the drawing board. I want to mention that not all use cases require fine-tuning and it is always recommended to evaluate and try out already fine-tuned models or API-based models before fine-tuning your own model.\nAs an example, we are going to use the following use case:\n\nWe want to fine-tune a model, which can generate SQL queries based on a natural language instruction, which can then be integrated into our BI tool. The goal is to reduce the time it takes to create a SQL query and make it easier for non-technical users to create SQL queries.\n\nText to SQL can be a good use case for fine-tuning LLMs, as it is a complex task that requires a lot of (internal) knowledge about the data and the SQL language."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "2. Setup development environment",
    "text": "2. Setup development environment\nOur first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven‚Äôt heard of trl yet, don‚Äôt worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n\n# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  # \"trl==0.7.10\" # \\\n  # \"peft==0.7.1\" \\\n\n# install peft & trl from github\n!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f--upgrade\n\nIf you are using a GPU with Ampere architecture (e.g.¬†NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention.\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\n\nimport torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\n# install flash-attn\n!pip install ninja packaging\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nWe will use the Hugging Face Hub as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the Hugging Face for this. After you have an account, we will use the login util from the huggingface_hub package to log into our account and store our token (access key) on the disk.\n\nfrom huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "3. Create and prepare the dataset",
    "text": "3. Create and prepare the dataset\nOnce you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. The dataset should be a diverse set of demonstrations of the task you want to solve. There are several ways to create such a dataset, including: * Using existing open-source datasets, e.g., Spider * Using LLMs to create synthetically datasets, e.g., Alpaca * Using Humans to create datasets, e.g., Dolly. * Using a combination of the above methods, e.g., Orca\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nIn our example we will use an already existing dataset called sql-create-context, which contains samples of natural language instructions, schema definitions and the corresponding SQL query.\nWith the latest release of trl we now support popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and trl will take care of the rest. Those formats include: * conversational format\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n\ninstruction format\n\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\nIn our example we are going to load our open-source dataset using the ü§ó Datasets library and then convert it into the the conversational format, where we include the schema definition in the system message for our assistant. We‚Äôll then save the dataset as jsonl file, which we can then use to fine-tune our model. We are randomly downsampling the dataset to only 10,000 samples.\nNote: This step can be different for your use case. For example, if you have already a dataset from, e.g.¬†working with OpenAI, you can skip this step and go directly to the fine-tuning step.\n\nfrom datasets import load_dataset\n\n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\nSCHEMA:\n{schema}\"\"\"\n\ndef create_conversation(sample):\n  return {\n    \"messages\": [\n      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n      {\"role\": \"user\", \"content\": sample[\"question\"]},\n      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n    ]\n  }  \n\n# Load dataset from the hub\ndataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\n\n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=2500/12500)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk \ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Fine-tune LLM using trl and the SFTTrainer",
    "text": "4. Fine-tune LLM using trl and the SFTTrainer\nWe are now ready to fine-tune our model. We will use the SFTTrainer from trl to fine-tune our model. The SFTTrainer makes it straightfoward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including: * Dataset formatting, including conversational and instruction format * Training on completions only, ignoring prompts * Packing datasets for more efficient training * PEFT (parameter-efficient fine-tuning) support including Q-LoRA * Preparing the model and tokenizer for conversational fine-tuning (e.g.¬†adding special tokens)\nWe will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out¬†Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA¬†blog post.\nNow, lets get started! üöÄ\nFirst, we need to load our dataset from disk.\n\nfrom datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n\nNext, we will load our LLM. For our use case we are going to use CodeLlama 7B. CodeLlama is a Llama model trained for general code synthesis and understanding. But we can easily swap out the model for another model, e.g.¬†Mistral or Mixtral models, TII Falcon, or any other LLMs by changing our model_id variable. We will use bitsandbytes to quantize our model to 4-bit.\nNote: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU.\nCorrectly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In trl we have a convinient method called setup_chat_format, which: * Adds special tokens to the tokenizer, e.g.¬†&lt;|im_start|&gt; and &lt;|im_end|&gt;, to indicate the start and end of a conversation. * Resizes the model‚Äôs embedding layer to accommodate the new tokens. * Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import setup_chat_format\n\n# Hugging Face model id\nmodel_id = \"codellama/CodeLlama-7b-hf\" # or `mistralai/Mistral-7B-v0.1`\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'right' # to prevent warnings\n\n# # set chat template to OAI chatML, remove if you start from a fine-tuned model\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\nThe¬†SFTTrainer¬† supports a native integration with¬†peft, which makes it super easy to efficiently tune LLMs using, e.g.¬†QLoRA. We only need to create our¬†LoraConfig¬†and provide it to the trainer. Our LoraConfig parameters are defined based on the qlora paper and sebastian‚Äôs blog post.\n\nfrom peft import LoraConfig\n\n# LoRA config based on QLoRA paper & Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=128,\n        lora_dropout=0.05,\n        r=256,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\", \n)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n    num_train_epochs=3,                     # number of training epochs\n    per_device_train_batch_size=3,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=10,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to hub\n    report_to=\"tensorboard\",                # report metrics to tensorboard\n)\n\nWe now have every building block we need to create our¬†SFTTrainer¬†to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 3072 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n\nStart training our model by calling the train() method on our Trainer instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n\n# start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model \ntrainer.save_model()\n\nThe training with Flash Attention for 3 epochs with a dataset of 10k samples took 01:29:58 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of only 1.8$.\n\n# free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n\n\nOptional: Merge LoRA adapter in to the original model\nWhen using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. This will save a default model, which can be used for inference.\nNote: You might require &gt; 30GB CPU Memory.\n\n\n#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import PeftModel, PeftConfig\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import AutoPeftModelForCausalLM\n\n# # Load PEFT model on CPU\n# config = PeftConfig.from_pretrained(args.output_dir)\n# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\n# tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n# model.resize_token_embeddings(len(tokenizer))\n# model = PeftModel.from_pretrained(model, args.output_dir)\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     args.output_dir,\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )  \n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\nNote: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out Evaluate LLMs and RAG a practical example using Langchain and Hugging Face blog post.\n\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline \n\npeft_model_id = \"./code-llama-7b-text-to-sql\"\n# peft_model_id = args.output_dir\n\n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nLet‚Äôs load our test dataset try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randint\n\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Test on sample \nprompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n\nNice! Our model was able to generate a SQL query based on the natural language instruction. Lets evaluate our model on the full 2,500 samples of our test dataset. Note: As mentioned above, evaluating generative models is not a trivial task. In our example we used the accuracy of the generated SQL based on the ground truth SQL query as our metric. An alternative way could be to automatically execute the generated SQL query and compare the results with the ground truth. This would be a more accurate metric but requires more work to setup.\n\nfrom tqdm import tqdm\n\n\ndef evaluate(sample):\n    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n        return 1 \n    else:\n        return 0\n\nsuccess_rate = []\nnumber_of_eval_samples = 1000\n# iterate over eval dataset and predict\nfor s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n    success_rate.append(evaluate(s))\n\n# compute accuracy\naccuracy = sum(success_rate)/len(success_rate)\n\nprint(f\"Accuracy: {accuracy*100:.2f}%\")  \n        \n\nWe evaluated our model on 1000 samples from the evaluation dataset and got an accuracy of 79.50%, which took ~25 minutes. This is quite good, but as mentioned you need to take this metric with a grain of salt. It would be better if we could evaluate our model by running the qureies against a real database and compare the results. Since there might be different ‚Äúcorrect‚Äù SQL queries for the same instruction. There are also several ways on how we could improve the performance by using few-shot learning, using RAG, Self-healing to generate the SQL query."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "6. Deploy the LLM for Production",
    "text": "6. Deploy the LLM for Production\nYou can now deploy your model to production. For deploying open LLMs into production we recommend using Text Generation Inference (TGI). TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching for the most popular open LLMs, including Llama, Mistral, Mixtral, StarCoder, T5 and more. Text Generation Inference is used by companies as IBM, Grammarly, Uber, Deutsche Telekom, and many more. There are several ways to deploy your model, including:\n\nDeploy LLMs with Hugging Face Inference Endpoints\nHugging Face LLM Inference Container for Amazon SageMaker\nDIY\n\nIf you have docker installed you can use the following command to start the inference server.\nNote: Make sure that you have enough GPU memory to run the container. Restart kernel to remove all allocated GPU memory from the notebook.\n\n%%bash \n# model=$PWD/{args.output_dir} # path to model\nmodel=$(pwd)/code-llama-7b-text-to-sql # path to model\nnum_shard=1             # number of shards\nmax_input_length=1024   # max input length\nmax_total_tokens=2048   # max total tokens\n\ndocker run -d --name tgi --gpus all -ti -p 8080:80 \\\n  -e MODEL_ID=/workspace \\\n  -e NUM_SHARD=$num_shard \\\n  -e MAX_INPUT_LENGTH=$max_input_length \\\n  -e MAX_TOTAL_TOKENS=$max_total_tokens \\\n  -v $model:/workspace \\\n  ghcr.io/huggingface/text-generation-inference:latest\n\nOnce your container is running you can send requests.\n\nimport requests as r \nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom random import randint\n\n# Load our test dataset and Tokenizer again\ntokenizer = AutoTokenizer.from_pretrained(\"code-llama-7b-text-to-sql\")\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# generate the same prompt as for the first local test\nprompt = tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\nrequest= {\"inputs\":prompt,\"parameters\":{\"temperature\":0.2, \"top_p\": 0.95, \"max_new_tokens\": 256}}\n\n# send request to inference server\nresp = r.post(\"http://127.0.0.1:8080/generate\", json=request)\n\noutput = resp.json()[\"generated_text\"].strip()\ntime_per_token = resp.headers.get(\"x-time-per-token\")\ntime_prompt_tokens = resp.headers.get(\"x-prompt-tokens\")\n\n# Print results\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{output}\")\nprint(f\"Latency per token: {time_per_token}ms\")\nprint(f\"Latency prompt encoding: {time_prompt_tokens}ms\")\n\nAwesome, Don‚Äôt forget to stop your container once you are done.\n\n!docker stop tgi"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "1. Define our use case in detail and create a template for our instructions",
    "text": "1. Define our use case in detail and create a template for our instructions\nBefore we describe our use case, we need to better understand what even is an instruction.\n\nAn instruction is a piece of text or prompt that is provided to an LLM, like Llama, GPT-4, or Claude, to guide it to generate a response. Instructions allow humans to steer the conversation and constrain the language model‚Äôs output to be more natural, useful, and aligned with the user‚Äôs goals. Crafting clear, well-formulated instructions is key to productive conversations.\n\nExamples of instructions are listed below in the table.\n\n\n\n\n\n\n\nCapability\nExample Instruction\n\n\n\n\nBrainstorming\nProvide a diverse set of creative ideas for new flavors of ice cream.\n\n\nClassification\nCategorize these movies as either comedy, drama, or horror based on the plot summary.\n\n\nClosed QA\nAnswer the question ‚ÄòWhat is the capital of France?‚Äô with a single word.\n\n\nGeneration\nWrite a poem in the style of Robert Frost about nature and the changing seasons.\n\n\nInformation Extraction\nExtract the names of the main characters from this short story.\n\n\nOpen QA\nWhy do leaves change color in autumn? Explain the scientific reasons.\n\n\nSummarization\nSummarize this article on recent advancements in renewable energy in 2-3 sentences.\n\n\n\nAs described in the beginning, we want to fine-tune a model to be able to generate instructions based on input. (output). We want to use this as a way to create synthetic datasets to personalize LLMs and Agents.\nConverting the idea into a basic prompt template following the Alpaca format we get.\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nDear [boss name],\n\nI'm writing to request next week, August 1st through August 4th,\noff as paid time off.\n\nI have some personal matters to attend to that week that require \nme to be out of the office. I wanted to give you as much advance \nnotice as possible so you can plan accordingly while I am away.\n\nPlease let me know if you need any additional information from me \nor have any concerns with me taking next week off. I appreciate you \nconsidering this request.\n\nThank you, [Your name]\n\n### Response:\nWrite an email to my boss that I need next week 08/01 - 08/04 off."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "2. Create an instruction dataset",
    "text": "2. Create an instruction dataset\nAfter we defined our use case and prompt template, we need to create our instruction dataset. Creating a high-quality instruction dataset is key for a good-performing model. Research shows that ‚ÄúLess Is More for Alignment‚Äù shows that creating a high-quality, low-quantity (~1000 samples) dataset can achieve the same performance as less-quality and high-quantity datasets.\nThere are several ways to create an instruction dataset, including:\n\nUsing an existing dataset and converting it into an instruction dataset, e.g., FLAN\nUse existing LLMs to create synthetically instruction datasets, e.g., Alpaca\nUse Humans to create instructions datasets, e.g., Dolly.\n\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nTo keep it simple, we are going to use Dolly¬†an open-source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the¬†InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nLet‚Äôs start coding, but first, let‚Äôs install our dependencies.\n\n!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors&gt;=0.3.1\" --upgrade\n\nTo load the¬†databricks/databricks-dolly-15k¬†dataset, we use the¬†load_dataset()¬†method from the ü§ó Datasets library.\n\nfrom datasets import load_dataset\nfrom random import randrange\n\n# Load dataset from the hub\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\nprint(f\"dataset size: {len(dataset)}\")\nprint(dataset[randrange(len(dataset))])\n# dataset size: 15011\n\nFound cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n\n\ndataset size: 15011\n{'instruction': 'On what month and day was Antwan Deon Odom born?', 'context': 'Antwan Deon Odom (born September 24, 1981) is a former American football defensive end. He was drafted by the Tennessee Titans in the second round of the 2004 NFL Draft. He played college football at Alabama. He has also played for the Cincinnati Bengals.', 'response': 'September 24', 'category': 'closed_qa'}\n\n\nTo instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a¬†formatting_function¬†that takes a sample and returns a string with our format instruction.\n\ndef format_instruction(sample):\n    return f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n{sample['instruction']}\n\"\"\"\n\nLet‚Äôs test our formatting function on a random example.\n\nfrom random import randrange\n\nprint(format_instruction(dataset[randrange(len(dataset))]))\n\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nSir Dorabji Tata and Allied Trusts and Sir Ratan Tata Trust\n\n### Response:\nWhat are the names of Tata trusts which Ratan Tata heads?"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "3. Instruction-tune Llama 2 using trl and the SFTTrainer",
    "text": "3. Instruction-tune Llama 2 using trl and the SFTTrainer\nWe will use the recently introduced method in the paper ‚ÄúQLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation‚Äù by Tim Dettmers et al.¬†QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:\n\nQuantize the pre-trained model to 4 bits and freeze it.\nAttach small, trainable adapter layers. (LoRA)\nFinetune only the adapter layers while using the frozen quantized model for context.\n\nIf you want to learn more about QLoRA and how it works, I recommend you to read the¬†Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA¬†blog post.\n\nFlash Attention\nFlash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper ‚ÄúFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness‚Äù. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention. Flash Attention is currently only available for Ampere (A10, A40, A100, ‚Ä¶) & Hopper (H100, ‚Ä¶) GPUs. You can check if your GPU is supported and install it using the following command:\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\npython -c \"import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\"\npip install ninja packaging\nMAX_JOBS=4 pip install flash-attn --no-build-isolation\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nThe example supports the use of Flash Attention for all Llama checkpoints, but is not enabled by default. To use Flash Attention change the value of use_flash_attentin to True\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nuse_flash_attention = False\n\n# Hugging Face model id\nmodel_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    use_flash_attention_2=use_flash_attention,\n    device_map=\"auto\",\n)\nmodel.config.pretraining_tp = 1\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nThe¬†SFTTrainer¬† supports a native integration with¬†peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our¬†LoRAConfig¬†and provide it to the trainer.\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# LoRA config based on QLoRA paper\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\", \n)\n\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"llama-7-int4-dolly\",\n    num_train_epochs=3,\n    per_device_train_batch_size=6 if use_flash_attention else 4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    bf16=True,\n    fp16=False,\n    tf32=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    disable_tqdm=False,  # disable tqdm since with packing values are in correct\n)\n\n\n# Upcast layer for flash attnetion\nif use_flash_attention:\n    from utils.llama_patch import upcast_layer_for_flash_attention\n    torch_dtype = torch.bfloat16 if args.bf16 else torch.float16 if args.fp16 else torch.float32\n    model = upcast_layer_for_flash_attention(model, torch_dtype)\n\nmodel = get_peft_model(model, peft_config)\n\nWe now have every building block we need to create our¬†SFTTrainer¬†to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=format_instruction, \n    args=args,\n)\n\nStart training our model by calling the train() method on our Trainer instance.\n\n# train\ntrainer.train() # there will not be a progress bar since tqdm is disabled\n\n# save model\ntrainer.save_model()\n\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\nThe training without Flash Attention enabled took 03:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 3.7$. The training with Flash Attention enabled took 02:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 2.6$.\nThe results using Flash Attention are mind blowing and impressive, 1.5x faster and 30% cheaper."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model.\n\nif use_flash_attention:\n    # unpatch flash attention\n    from utils.llama_patch import unplace_flash_attn_with_attn\n    unplace_flash_attn_with_attn()\n    \nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\nargs.output_dir = \"llama-7-int4-dolly\"\n\n# load base LLM model and tokenizer\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n) \ntokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n\nLet‚Äôs load the dataset again with a random sample to try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\nsample = dataset[randrange(len(dataset))]\n\nprompt = f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n\nprint(f\"Prompt:\\n{sample['response']}\\n\")\nprint(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\nprint(f\"Ground truth:\\n{sample['instruction']}\")\n\nNice! our model works! If want to accelerate our model we can deploy it with Text Generation Inference. Therefore we would need to merge our adapter weights into the base model.\n\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n) \n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\n\n# push merged model to the hub\n# merged_model.push_to_hub(\"user/repo\")\n# tokenizer.push_to_hub(\"user/repo\")"
  },
  {
    "objectID": "posts/EAGLE/EAGLE.html",
    "href": "posts/EAGLE/EAGLE.html",
    "title": "EAGLE: Extrapolation Algorithm for Greater Language-model Efficiency",
    "section": "",
    "text": "Auto-regressive decoding has become the de facto standard for large language models (LLMs). This process generates output tokens one at a time, which makes the generation by LLMs both costly and slow. Speculative sampling based methods offer a solution to this challenge. They divide the generation process of LLMs into two stages: the draft stage, where potential tokens are conjectured at a low cost, and the verification stage, where these tokens are validated in parallel through a single forward pass of the LLM.\nSpeculative sampling aims to accelerate generation by minimizing time overhead and increasing the acceptance rate of drafts generated by the original Large Language Model (LLM). Popular methods like Lookahead and Medusa achieve this by reducing overhead and enhancing acceptance rates. Nonetheless, their full potential is limited by the lower accuracy of the drafts they generate.\nEAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), is a simple framework for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding.\nCompared with existing speculative sampling-based techniques, the advantages of EAGLE include:\n\nSimplicity: EAGLE adds only a lightweight plug-in (a single transformer decoder layer) to the LLM, which can be easily deployed in a production.\nReliability: EAGLE does not involve any fine-tuning of the original LLM, and the preservation of the output distribution by EAGLE is theoretically guaranteed for both the greedy and non-greedy settings. This is in sharp contrast to Lookahead and Medusa which focuses on greedy settings only.\nSpeed: EAGLE stands out as the fastest framework within the family of speculative sampling. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface‚Äôs implementations.\n\nPaper : https://arxiv.org/pdf/2401.15077.pdf\nCode : https://github.com/SafeAILab/EAGLE"
  },
  {
    "objectID": "posts/MambaByte/MambaByte.html",
    "href": "posts/MambaByte/MambaByte.html",
    "title": "MambaByte: Token-free Selective State Space Model",
    "section": "",
    "text": "In December 2023, ‚ÄúMamba : Linear-Time Sequence Modeling with Selective State Spaces‚Äù paper was release and with it the whole discussion about Mamba (SSM) been a viable replacement for Transformer base model had started as Mamba achieved 4-5x higher throughput than a Transformer of a similar size. To capitalize on this, there is a growing trend of re-implementing various transformer based LLMs on Mamba (SSM) architecture such as MoE-Mamba and VMamba.\nLooks like Token free LLM is also going in this direction. Typically, Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. Look no further, we have MambaByte to save.\nMambaByte, a token-free adaptation of the Mamba state space model, trained autoregressive on byte sequences. MambaByte eliminates the need for patching and achieves better performance and computational efficiency compared to bite level Transformers. MambaByte being a straightforward adaptation of the Mamba architecture, utilizes a linear time approach for sequence modeling by incorporating a selection mechanism that is more effective for discrete data like text section parallel scans for linear recurrences\nMambaByte outperforms other byte-level models over several datasets and shows competitive results with subword Transformers, thus serving as a promising tokenization alternative. SSMs also enable significantly fast text generation due to their recurrent nature, making byte models practical.\nPaper : https://arxiv.org/pdf/2401.13660.pdf"
  },
  {
    "objectID": "posts/Towards Conversational Diagnostic AI/Towards Conversational Diagnostic AI.html",
    "href": "posts/Towards Conversational Diagnostic AI/Towards Conversational Diagnostic AI.html",
    "title": "Towards Conversational Diagnostic AI",
    "section": "",
    "text": "With the Med-PaLM series of LLMs Google is one of the few companies you can claim expertise in building medical domain specific LLMs. The latest addition has been AMIE (Articulate Medical Intelligence Explorer).\nAMIE is a conversational medical AI optimized for diagnostic dialogue. AMIE is instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical reasoning, question answering, and summarization datasets.\nAMIE has a self-play based simulated dialogue environment with automated feedback mechanisms to scale its capabilities across various medical contexts and specialities. There are two types of self-play loops in place : 1. An ‚Äúinner‚Äù self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; 2. An ‚Äúouter‚Äù self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations.\nDuring online inference, AMIE used a chain-of-reasoning (COR) strategy to progressively refine its response conditioned on the current conversation to arrive at an accurate and grounded reply to the patient in each dialogue turn\nAcross multiple axes corresponding to both specialist physician (28 out of 32) and patient actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest. However, the results should be interpreted with appropriate caution. Translating from this limited scope of experimental, towards real-world tools, requires significant additional research and development.\nPaper : https://arxiv.org/pdf/2401.05654.pdf"
  },
  {
    "objectID": "posts/ChatQA/ChatQA.html",
    "href": "posts/ChatQA/ChatQA.html",
    "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
    "section": "",
    "text": "With all open source LLM models trying to outperform GPT-4 one may wonder, which one has truly been successful in Conversational QA - one of the elementary use cases of LLMs.\nIntroducing ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. It proposes a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, it fine-tunes a dense retriever on a multiturn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs.¬†53.90), without relying on any synthetic data from OpenAI GPT models\nIn addition, it demonstrates that fine-tuning a single-turn query retriever using its own curated conversational QA data performs comparable to the state-of-the-art LLM-based query rewriting model, without the need of extra computational time and potential API cost from rewriting.\nPaper : https://arxiv.org/pdf/2401.10225.pdf\n¬† arxiv:2401.10225"
  },
  {
    "objectID": "posts/Medusa/Medusa.html",
    "href": "posts/Medusa/Medusa.html",
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
    "section": "",
    "text": "@article{cai2024medusa,\n  title   = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},\n  author  = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},\n  year    = {2024},\n  journal = {arXiv preprint arXiv: 2401.10774}\n}\n¬†\nWhy is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge\n\nLarge memory footprint. Both model parameters and intermediate states are needed in memory at inference time. For example, The KV cache should be stored in memory during decoding time; E.g. For a batch size of 512 and context length of 2048, the KV cache totals 3TB, that is 3x the model size. Inference cost from the attention mechanism scales quadratically with input sequence length.\nLow parallelizability. Inference generation is executed in an autoregressive fashion, making the decoding process hard to parallel.\n\nThis paper introduces MEDUSA, a method for improving inference in Large Language Models (LLMs) by adding extra decoding heads to predict multiple tokens in parallel. MEDUSA achieves significant speedup without compromising generation quality.\nMedusa adds extra ‚Äúheads‚Äù to LLMs to predict multiple future tokens simultaneously. When augmenting a model with Medusa, the original model stays untouched, and only the new heads are fine-tuned during training. During generation, these heads each produce multiple likely words for the corresponding position. These options are then combined and processed using a tree-based attention mechanism. Finally, a typical acceptance scheme is employed to pick the longest plausible prefix from the candidates for further decoding.\nSo how does Medusa solve the challenges associated with speculative decoding ?\n\nInstead of introducing a new model, we train multiple decoding heads on the same model.\nThe training is parameter-efficient so that even the ‚ÄúGPU-Poor‚Äù can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.\nRelaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.\n\nDuring experimentation, Medusa delivers approximately a 2x speed (1.94x) increase across a range of Vicuna models. Will be interesting to see Medusa‚Äôs performance with other open source foundational models.\nPaper : https://arxiv.org/pdf/2401.10774.pdf"
  },
  {
    "objectID": "posts/Tuning Language Models by Proxy/Tuning Language Models by Proxy.html",
    "href": "posts/Tuning Language Models by Proxy/Tuning Language Models by Proxy.html",
    "title": "Tuning Language Models by Proxy",
    "section": "",
    "text": "These days capabilities of large pretrained LLMs can be significantly enhanced for specific domains of interest or task using additional fine tuning. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private e.g.¬†OpenAI GPT-4.\nPaper has introduced the Proxy-Tuning method, a lightweight decoding-time algorithm that can be used to customize large pretrained language models without accessing their weights. It achieves similar results to direct tuning and can be applied for domain adaptation and task-specific finetuning.\nIn the experiments, it apply proxy-tuning to steer a large pretrained (base) model (LLAMA2-13B or 70B) using small, cheaper-to-tune (anti-)experts (based on LLAMA2-7B) for instruction-following, domain adaptation, and task fine tuning. When it applies proxy-tuning to LLAMA2-70B using proxies of only 7B size, it can close 88% of the gap between LLAMA2-70B and it‚Äôs truly-tuned CHAT version, when evaluated across knowledge, reasoning, and safety benchmarks.\nPaper : https://arxiv.org/pdf/2401.08565.pdf"
  },
  {
    "objectID": "posts/MoE-LLaVA/MoE-LLaVA.html",
    "href": "posts/MoE-LLaVA/MoE-LLaVA.html",
    "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
    "section": "",
    "text": "For Large Vision-Language Models (LVLMs), scaling the model can effectively improve performance. However, expanding model parameters significantly increases the training and inferring costs, as all model parameters are activated for each token in the calculation.\nIn contrast, sparse Mixtures of Experts (MoE) effectively scale model capacity by using fixed activated parameters to process data, which has thrived in the field of NLP . Recently, Mistral LLM equipped with the MoE layers has gained popularity in LLMs. Mixtral-MoE8√ó7B achieves performance comparable to LLaMA 2-70B with fewer computational resources.\nHowever, directly applying MoE to train sparse LVLMs is challenging as it leads to significant performance degradation. Proper initialization is crucial for sparsifying the LVLM, and that‚Äôs exactly what MoE-tuning does. MoW-tuning - a novel three-stage training strategy for adapting MoE to LVLMs and preventing the model degradation caused by sparsity.\nMoE-LLaVA model operates by using multiple sparse paths, where each token is directed to different experts through a router. These activated experts collaboratively process the tokens, while inactive paths remain dormant. By stacking MoE encoder layers iteratively, the model creates a sparse pathway to a larger and more potent Large Vocabulary Language Model (LVLM). This approach allows for efficient and effective processing of input data by dynamically routing tokens to appropriate experts for processing.\nDuring experimentation MoELLaVA model demonstrates great potential for multi-modal understanding and hallucination inhibition. MoELLaVA achieves comparable performance to state-of-the-art 7B models with only 3B sparse activated parameters on multiple visual understanding datasets, and outperforms LLaVA-1.5-13B by 1.1% on the POPE hallucination benchmark with 2.2B activated parameters.\nPaper : https://arxiv.org/pdf/2401.15947.pdf\n¬†\n@article{lin2023video,\n  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},\n  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},\n  journal={arXiv preprint arXiv:2311.10122},\n  year={2023}\n}"
  },
  {
    "objectID": "posts/DeepSpeed-FastGen/DeepSpeed-FastGen.html",
    "href": "posts/DeepSpeed-FastGen/DeepSpeed-FastGen.html",
    "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference",
    "section": "",
    "text": "Recently Microsoft DeepSpeed launched DeepSpeed-FastGen LLM serving framework, which offers up to 2.3x higher effective throughput compared to state-of-the-art systems like vLLM. DeepSpeed-FastGen leverages the combination of DeepSpeed-MII and DeepSpeed-Inference to provide an easy-to-use serving system.\nDeepSpeed-FastGen is built to leverage continuous batching and non-contiguous KV caches to enable increased occupancy and higher responsivity for serving LLMs in the data center, similar to existing frameworks such as TRT-LLM, TGI, and vLLM. In order to achieve a new level of performance, DeepSpeed-FastGen introduces SplitFuse which leverages dynamic prompt and generation decomposition and unification to further improve continuous batching and system throughput.\nDuring experiment, DeepSpeed-FastGen outperforms vLLM in both throughput and latency. On Llama-2 70B with 4 A100x80GB, DeepSpeed-FastGen demonstrates up to 2x higher throughput (1.36 rps vs.¬†0.67 rps) at identical latency (9 seconds) or up to 50% latency reduction (7 seconds vs.¬†14 seconds) while achieving the same throughput (1.2 rps).\nSupported models : LLaMA and LLaMA-2, Mistral, OPT, Falcon, Mixtral, Phi-2, Qwen\nPaper : https://arxiv.org/pdf/2401.08671.pdf"
  },
  {
    "objectID": "posts/Self-Evaluation Improves Selective Generation in Large Language Models/Self-Evaluation Improves Selective Generation in Large Language Models.html",
    "href": "posts/Self-Evaluation Improves Selective Generation in Large Language Models/Self-Evaluation Improves Selective Generation in Large Language Models.html",
    "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
    "section": "",
    "text": "Trustworthiness of LLMs output is one of the important considerations for safe deployment of LLMs in production.Once of the straightforward way to do so is by measuring quality of selected outputs of LLMs. Reinforcement Learning from Human Feedback (RLHF) is one of the widely used method for better quality-calibrated models.\nSince human feedback data is expensive to obtain, the paper has explored the use of token-level self-evaluation to improve the accuracy and quality of generated content by large language models. Experimental results show that self-evaluation based scores are effective in selective generation. and thereby improving the self-evaluation ability of LLMs to improve quality-calibration.\nThe paper proposes methods to convert open-ended generation into token-level evaluation tasks that the LLM can self-evaluate, such as multi-choice question answering or true/false evaluation. Two main methods are proposed: Sample & Select (multi-choice) and Sample & Eval (true/false).\nExperiments on TRUTHFULQA and TL;DR datasets show the self-evaluation scores significantly improve calibration for selective generation compared to sequence likelihood scores. The hybrid method with a ‚Äúnone of the above ‚Äô‚Äô option performs the best overall on accuracy, calibration AUC, and selective AUC metrics. Self-evaluation provides a way to improve calibration of LLMs for selective text generation, without needing extra training data.\nOne of the pitfalls I can see in this paper is that all experiments are carried out with PALM-2 and GPT-3 rather than GPT-3.5 or GPT-4 models as OpenAI API does not provide output log-probabilities for them. It will be interesting to see how Self-Evaluation holds up against GPT-3.5 + models.\nPaper : https://arxiv.org/pdf/2312.09300.pdf"
  },
  {
    "objectID": "posts/Self-RAG/Self-RAG.html",
    "href": "posts/Self-RAG/Self-RAG.html",
    "title": "Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections",
    "section": "",
    "text": "Self-RAG is a new framework to train an arbitrary LM to learn to retrieve, generate, and critique to enhance the factuality and quality of generations, without hurting the versatility of LLMs. It outperformed ChatGPT and retrieval-augmented LLama2 Chat on six tasks.\nUnlike a widely-adopted Retrieval-Augmented Generation (RAG; Figure left) approach, Self-RAG retrieves on demand (e.g., can retrieve multiple times or completely skip retrieval) given diverse queries, and criticize its own generation from multiple fine-grained aspects by predicting reflection tokens as an integral part of generation. It conducts a segment-wise beam search to select the output that maximizes the utility for diverse preferences.\nEagerly waiting for Self-RAG SciPhi-Self-RAG-Mistral-7B-32k on top of Mistral-7B.\nPaper : https://arxiv.org/pdf/2310.11511.pdf\nModel : https://huggingface.co/selfrag/selfrag_llama2_7b"
  },
  {
    "objectID": "posts/Reciprocal Rank Fusion /Reciprocal Rank Fusion.html",
    "href": "posts/Reciprocal Rank Fusion /Reciprocal Rank Fusion.html",
    "title": "Reciprocal Rank Fusion (RRF) with LambdaMART: Context Tuning for Retrieval Augmented Generation (RAG)",
    "section": "",
    "text": "RAG typically consists of three primary components: Tool Retrieval, Plan Generation, and Execution. Existing RAG methodologies rely heavily on semantic search for tool retrieval, but this approach has limitations, especially when queries lack specificity or context. Context Tuning, can be looked at as a viable solution, a component in RAG that precedes tool retrieval, to provide contextual understanding and context seeking abilities to improve tool retrieval and plan generation.\nPaper proposes a new lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART. Results indicate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accuracy. The lightweight model outperforms other methods and helps reduce hallucinations during planning. However, limitations include the absence of conversation history for multi-turn tasks, constraints on planner context window size affecting performance, and the use of synthetic personas instead of real-world data due to privacy concerns.\nIn summary, Context Tuning enhances RAG showcasing improvements in retrieval, planning accuracy, and hallucination reduction compared to baseline methods.\nPaper : https://arxiv.org/pdf/2312.05708.pdf"
  },
  {
    "objectID": "posts/Infinite-LLM/Infinite-LLM.html",
    "href": "posts/Infinite-LLM/Infinite-LLM.html",
    "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
    "section": "",
    "text": "Introducing DistAttention, a distributed attention algorithm, and DistKV-LLM, a distributed LLM serving system, to improve the performance and resource management of cloud-based LLM services. The system achieved significant throughput improvements and supported longer context lengths compared to existing systems\nTraditionally, serving LLMs with long context lengths poses challenges due to the dynamic and growing memory requirements of the attention layer‚Äôs key-value (KV) cache. This makes efficient resource management difficult. Introducing DistAttention, a novel distributed attention algorithm that partitions the KV cache into smaller blocks (‚ÄúrBlocks‚Äù) to enable distributed processing and storage. The paper also introduces DistKV-LLM, a distributed LLM serving engine that coordinates memory usage across GPUs and CPUs in a data center. It manages the distributed KV cache through two components the rManager and gManager.\nThe rManager virtualizes memory for each instance and handles local and remote memory requests. The gManager maintains a global view of memory usage and facilitates allocation between instances. Techniques like overlapping communication and computation, a memory optimization algorithm (DGFM), and a coordination protocol are proposed to improve performance.\nEvaluation on a 32 GPU cluster shows the system supports context lengths 2-19x longer than prior work, with 1.03-2.4x higher throughput. It achieves efficient resource utilization for long-context LLM serving in distributed environments. In summary, the key novelty lies in DistAttention‚Äôs distributed approach to processing the attention layer, and DistKV-LLM‚Äôs coordinated management of the distributed KV cache memory across\nPaper : https://arxiv.org/pdf/2401.02669.pdf"
  },
  {
    "objectID": "posts/CoT/CoT.html",
    "href": "posts/CoT/CoT.html",
    "title": "Chain of Thought (CoT): The Impact of Reasoning Step Length on Large Language Models",
    "section": "",
    "text": "If you are doing prompt engineering for LLMs then you might have come across Chain of Thought (CoT) prompting, which is significant in improving the reasoning abilities of LLMS. However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown.\nLength of reasoning steps/chains in prompting impacts the performance of large language models (LLMs) on tasks requiring reasoning abilities. Experiments show that increasing the number of reasoning steps in prompts, even without adding new information, significantly improves LLM performance across multiple datasets. Shortening steps diminishes performance. Surprisingly, incorrect rationales can still yield good results if they maintain sufficient step length, suggesting step length is more important than factual accuracy. The benefits of longer steps scale with task complexity: simpler tasks require fewer steps while complex tasks gain more from longer chains. Zero-shot prompting can also be improved by lengthening initial prompts to encourage more reasoning (e.g.¬†‚ÄúThink step-by-step, think more steps‚Äù). Compressing step lengths undermines few-shot CoT (chain-of-thought) performance, regressing it to zero-shot levels. Bigger LLMs require fewer steps to reach peak performance compared to smaller models, showing a relationship between model size and optimal step count. Altering questions within prompts has minimal impact, suggesting step length rather than question details primarily drives reasoning.\nPaper : https://arxiv.org/pdf/2401.04925.pdf"
  },
  {
    "objectID": "posts/Improving Text Embeddings with Large Language Models/Improving Text Embeddings with Large Language Models.html",
    "href": "posts/Improving Text Embeddings with Large Language Models/Improving Text Embeddings with Large Language Models.html",
    "title": "Improving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM",
    "section": "",
    "text": "Check out a groundbreaking paper on improving text embeddings with large language models (LLMs) like GPT-4! The authors propose generating synthetic training data for text embedding tasks using LLMs, instead of relying on human-labeled datasets.\nTheir two-step prompt method generates diverse synthetic data for hundreds of thousands of embedding tasks across 93 languages, covering semantic textual similarity, bitext retrieval, and more. The Mistral-7B LLM is fine-tuned on the synthetic data and achieves state-of-the-art results on the MTEB benchmark, outperforming previous models by 2.4 points on average across task categories.\nIn summary, this paper presents an effective and efficient method for improving text embeddings by leveraging data generation with LLMs.\nPaper : https://arxiv.org/pdf/2401.00368.pdf"
  },
  {
    "objectID": "posts/DocLLM/DocLLM.html",
    "href": "posts/DocLLM/DocLLM.html",
    "title": "DOCLLM: A Layout Aware Generative Language Models for Multi model document understanding",
    "section": "",
    "text": "Introducing DocLLM, a groundbreaking generative language model that can understand visually rich documents without the need for expensive image encoders. DocLLM uses a disentangled attention mechanism that captures the interdependencies between text and layout, making it possible to handle irregular layouts and heterogeneous content in visual documents.\nDocLLM‚Äôs pre-training objective focuses on infilling missing text segments, and the pre-trained model is fine-tuned using instructions from various datasets, including visual question answering, natural language inference, key information extraction, and document classification.\nEvaluated against comparable models, DocLLM outperforms on 14 out of 16 datasets and generalizes to 4 out of 5 unseen datasets. Its awareness of multi-page documents and page breaks enhances its ability to understand long documents.\nDocLLM can enable the use of more types of data for pre-training language models, allowing documents with complex layouts to be used without much preprocessing. Its cohesive text blocks for pre-training enable meaningful infilling.\nPaper : https://arxiv.org/pdf/2401.00908.pdf"
  },
  {
    "objectID": "posts/Soaring from 4K to 400K/Soaring from 4K to 400K.html",
    "href": "posts/Soaring from 4K to 400K/Soaring from 4K to 400K.html",
    "title": "Soaring from 4K to 400K: Extending LLM‚Äôs Context with Activation Beacon",
    "section": "",
    "text": "Activation Beacon is a plug-and-play module for large language models that allows them to process longer contexts with a limited context window, while preserving their original capabilities. It achieves competitive memory and time efficiency and can be trained efficiently with short-sequence data. When dealing with long-sequence data, it resorts to sliding windows for streaming processing, which leads to a superior working efficiency at both inference and training time. With the diversely sampled condensing ratios, it can be effectively learned to support the extensions for a wide scope of context lengths based on short-sequence training data. The experimental study verifies Activation Beacon as an effective, efficient, compatible, low-cost (training) method to extend the context length of LLM.\nPaper : https://arxiv.org/pdf/2401.03462.pdf"
  },
  {
    "objectID": "posts/Comprehensive Survey of Hallucination Mitigation Techniques /Comprehensive Survey of Hallucination Mitigation Techniques .html",
    "href": "posts/Comprehensive Survey of Hallucination Mitigation Techniques /Comprehensive Survey of Hallucination Mitigation Techniques .html",
    "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
    "section": "",
    "text": "The paper provides a comprehensive taxonomy categorizing over 32 techniques for mitigating hallucinations in large language models (LLMs). It groups the techniques into categories such as prompt engineering, self-refinement through feedback and reasoning, prompt tuning, and model development. Key mitigation techniques highlighted include:\n\nRetrieval Augmented Generation (RAG) which enhances LLM responses by retrieving information from authoritative external knowledge bases. This helps ground the responses in facts.\nMethods leveraging iterative feedback loops and self-contradiction detection to refine LLM outputs. For example, the Self-Reflection Methodology employs knowledge acquisition and answer generation over multiple cycles.\nPrompt tuning techniques like UPRISE which tune lightweight retrievers to automatically provide task-specific prompts that reduce hallucinations.\nNovel model decoding strategies such as Context-Aware Decoding that override an LLM‚Äôs biases by amplifying differences between outputs with and without context.\nUtilizing knowledge graphs and adding faithfulness based loss function\nSupervised Fine-tuning\n\nPaper : https://arxiv.org/pdf/2401.01313.pdf"
  },
  {
    "objectID": "posts/KwaiAgents/KwaiAgents.html",
    "href": "posts/KwaiAgents/KwaiAgents.html",
    "title": "KwaiAgents: Generalized Information-seeking Agent System with LLMs - 2 Open-source models fine tuned for agent systems! Better than GPT-3.5 turbo as an agent!",
    "section": "",
    "text": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user‚Äôs query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system‚Äôs performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\nPaper : https://arxiv.org/pdf/2312.04889v1.pdf"
  },
  {
    "objectID": "posts/SPIN/SPIN.html",
    "href": "posts/SPIN/SPIN.html",
    "title": "Self-Play Fine-Tuning (SPIN): Converts Weak Language Models to Strong Language Models",
    "section": "",
    "text": "Self-Play Fine-Tuning (SPIN) is a new fine-tuning method to improve large language models (LLMs) without needing additional human-annotated data.\nThe key idea is to use a self-play mechanism where the LLM plays against itself. Specifically, the LLM from the previous iteration generates synthetic responses. The new LLM being trained tries to discern between the synthetic responses and real human responses. This iterates, with the new LLM becoming the synthetic data generator for the next iteration.\nThe method is shown to significantly enhance LLMs‚Äô performance on a variety of benchmarks: - On the HuggingFace Open LLM Leaderboard, SPIN improves the average score from 58.14 to 63.16 with over 10% gains on some datasets. - On the MT-Bench benchmark, the score improves from 5.94 to 6.78. - The gains match or exceed models trained with additional human preference data.\nTheoretically, it is proven that SPIN converges when the LLM distribution aligns perfectly with the real data distribution.\nOverall, the self-play approach enables iterative improvement of LLMs without needing extra human feedback or data, converting weak models to strong models by unleashing the full potential of existing labeled data.\nPaper : https://arxiv.org/pdf/2401.01335.pdf"
  },
  {
    "objectID": "posts/LLM Maybe LongLM/LLM Maybe LongLM.html",
    "href": "posts/LLM Maybe LongLM/LLM Maybe LongLM.html",
    "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
    "section": "",
    "text": "With only four lines of code modification, the proposed method can effortlessly extend existing LLMs‚Äô context window without any fine-tuning. This work elicits LLMs‚Äô inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.\nIn this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs‚Äô context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs‚Äô long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model‚Äôs self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs‚Äô context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs‚Äô context window‚Äôs length.\nPaper : https://arxiv.org/pdf/2401.01325.pdf"
  },
  {
    "objectID": "posts/Mamba-Chat/Mamba-Chat.html",
    "href": "posts/Mamba-Chat/Mamba-Chat.html",
    "title": "Mamba-Chat: A Chat LLM based on State Space Models",
    "section": "",
    "text": "Mamba-Chat is the first chat language model based on a state-space model architecture, not a transformer.\nThe model is based on Albert Gu‚Äôs and Tri Dao‚Äôs work Mamba: Linear-Time Sequence Modeling with Selective State Spaces (paper) as well as their model implementation. This repository provides training / fine-tuning code for the model based on some modifications of the Huggingface Trainer class. Mamba-Chat is based on Mamba-2.8B and was fine-tuned on 16,000 samples of the HuggingFaceH4/ultrachat_200k dataset.\nMamba: Linear-Time Sequence Modeling with Selective State Spaces\nPaper : https://arxiv.org/pdf/2312.00752.pdf"
  },
  {
    "objectID": "posts/BLIVA/BLIVA.html",
    "href": "posts/BLIVA/BLIVA.html",
    "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
    "section": "",
    "text": "Vision Language Models (VLMs), such as OpenAI‚Äôs GPT-4, Flamingo, BLIP-2 and LLaVA have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios.\nStandard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context.\nTo improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process.\nBLIVA uses a Q-Former to draw out instruction-aware visual features from the patch embeddings generated by a frozen image encoder. These learned query embeddings are then fed as soft prompt inputs into the frozen Language-Learning Model (LLM). Additionally, the system repurposes the originally encoded patch embeddings through a fully connected projection layer, serving as a supplementary source of visual information for the frozen LLM.\nDuring experiment, BLIVA significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence.\nPaper : https://arxiv.org/pdf/2308.09936.pdf"
  },
  {
    "objectID": "posts/FIND/FIND.html",
    "href": "posts/FIND/FIND.html",
    "title": "FIND: INterface for Foundation models‚Äô embeDDings",
    "section": "",
    "text": "Foundation models across the vision and language domains, such as GPT4, DALLE-3, SAM and LLaMA etc., have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) .\nHowever, the process of training individual foundation models has become remarkably costly. Furthermore, the full potential of these models remains untapped due to limitations in their fixed output modalities (i.e.¬†text output for Q&A and visual output for image generation). Although techniques such as prompt engineering and adaptive tuning have shown promising results, these approaches struggle with integrating different foundation models off the shelf, expanding the output types and task objectives.\nPaper proposes FIND - a generalized interface for aligning foundation models‚Äô embeddings. The interface enables task-adaptive prototyping, which means we only need to change the configure file instead of the model architecture when adapting to the new tasks. Because all the vision-language tasks are trained in a unified way, this creates an interleaved shared embedding space where vision and language references are replaceable and addable. The proposed interface has the following favorable attributes: 1. Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights. 2. Prototypable. Different tasks are able to be implemented through prototyping attention masks and embedding types. 3. Extendable. The proposed interface is adaptive to new tasks, and new models. 4. Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space.\nFurthermore, FIND has achieved SoTA performance on interleaved image retrieval and segmentation and shows better or comparable performance on generic/interactive/grounded segmentation and image-text retrieval.\nPaper : https://arxiv.org/pdf/2312.07532.pdf"
  },
  {
    "objectID": "posts/Re3val/Re3val.html",
    "href": "posts/Re3val/Re3val.html",
    "title": "Re3val: Reinforced and Reranked Generative Retrieval",
    "section": "",
    "text": "The primary objective of retrieval models is to enhance the accuracy of answers by selecting the most relevant documents retrieved for a given query, ensuring models have sufficient information to help the downstream reasoning process. However, there are two major limitations: First, the generative retrieval does not account for contextual information. Secondly, the retrieval can‚Äôt be tuned for the downstream readers as decoding the page title is a non-differentiable operation.\nPaper introduces Re3val - Reinforced and Reranked Generative Retrieval, a novel framework specifically designed to address the challenges in neural information retrieval. Re3val uses Dense Passage Retrieval (DPR) contexts for reranking retrieved page titles, leading to improved RPrecision. Re3val enhances performance by integrating generated questions in pre-training and utilizing REINFORCE during distant supervision. Moreover, Re3val achieves more accurate answers by reading reranked contexts retrieved with the reranked page titles. These advancements enable Re3val to achieve state-of-the-art performance while also offering cost savings by reducing training time and minimizing the need for extensive data labeling.\nTypical Re3val Training Pipeline consists of the following. Generated questions after filtering are integrated into pre-training (1), followed by few-shot training (3) with REINFORCE (2, 4). Retrieved DPR contexts (5), perturbed page titles (6), and queries are concatenated for reranker training (7). Gold and negative passages retrieved with BM-25 are employed (8) for context reranker training (9). Contexts are retrieved using the top 5 reranked titles from KILT (10), where missing titles are imputed with BM-25 (11). DPR contexts are imputed (12) if lacking five gold contexts during FiD model pre-training (13). FiD model is fine-tuned using five reranked contexts (14).\nDuring inference Reranker concatenates retrieved DPR contexts (1), page titles (2), and query to rerank page titles (3). Contexts retrieved with the top five reranked page titles (4), including BM-25 imputed titles (5), are reranked (6). The top-5 reranked contexts are used to generate an answer (7).\nExperimental results demonstrate Re3val‚Äôs superiority over the CorpusBrain zero-shot baseline, with an average 8% R-Precision improvement across five tasks using reduced pretraining data. Re3val also achieves an average 1.9% R-Precision increase compared to other generative models via page title reranking with limited taskspecific data. Moreover, by employing a context reranker before grounding, Re3val achieves top-1 KILT scores among generative retrieval models, showing an average 2.1% improvement across five datasets.\nPaper : https://arxiv.org/pdf/2401.16979.pdf"
  },
  {
    "objectID": "posts/Repeat After Me/Repeat After Me.html",
    "href": "posts/Repeat After Me/Repeat After Me.html",
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "section": "",
    "text": "Transformers are the workhorse of modern sequence modeling, achieving remarkable performance on a variety of tasks, but they have unavoidable inefficiencies. Specifically, when it comes to memory and compute requirements to predict the next token of a sequence of length. Recently, there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as ‚Äúgeneralized state space models‚Äù (GSSMs).\nGSSMs have demonstrated impressive performance, but it is not yet clear what these models sacrifice for their improved efficiency, if anything. Well that‚Äôs what exactly this paper has found out. It seems that GSSMs are promising in terms of inference-time efficiency but are limited compared to transformer models on tasks that require copying from the input context.\nTo understand this gap in capabilities experimentation was carried out to copy strings of length that are exponential in the number of heads of the transformer under the following scenarios.\n\nCopying: training efficiency. Here the train models copy strings of length ‚â§ 300 and evaluate string-level accuracy on strings of length 300. Transformers train much faster than GSSMs.\nCopying: length generalization. Here train models copy strings of length ‚â§ 50 until all models are perfect in-distribution and evaluate string-level accuracy. Evaluating on longer inputs, the transformer models dramatically outperform the GSSMs.\nLookup with pretrained models. Here the task requires looking up and retrieving a number from a ‚Äúphone book‚Äù of varying length that is entirely in context. Pythia (a transformer model) substantially outperforms Mamba (a GSSM) across model sizes.\nCopy: natural language strings. It compares pretrained models on their ability to copy natural language strings sampled from C4 of varying lengths and report string-level accuracy. The transformer models substantially outperform the GSSMs.\nCopy: shuffled strings. To test whether it mattered that the strings were in natural language, randomly shuffle the word order of the strings from the previous experiment. it was found that this degrades performance, especially for the Mamba models.\n\nOverall transformers are better than GSSMs at copying from their input context. However, SSMs have many advantages over transformers when it comes to memory and computational complexity as well as generating long consistent text. Future work should focus on building hybrid architectures that endow state space models with an attention-like mechanism, allowing them to retrieve relevant pieces of text from their input. What do you think ?\nSo why size of input context is so much important in LLMs. In order to understand this let‚Äôs look at GPU level. Modern GPUs have a ‚Äúproblem‚Äù: they‚Äôre too fast. GPUs have become very fast at performing calculations, insomuch that the speed of computation (FLOPs) is much higher than the memory bandwidth (GB/s) or speed of data transfer between memory areas. For example, an NVIDIA A100 can perform 19.5 TFLOPs while having a memory bandwidth of 2TB/s, which is 40 times slower considering each operation is 32 bit.\nThis means that sometimes the bottleneck is not how many operations we perform, but how much data transfer our operations need, and that depends on the size and the quantity of the tensors involved in our calculations. For example, computing the same operation on the same tensor N time may be faster than computing the same operation on N different tensors, even if they have the same size, this is because the GPU may need to move the tensors around. That what happens during memory-intensive tasks such as copying long strings, retrieval and few-shot question answering.\nSo the goal should not only be to optimize the number of operations we do, but also minimize the memory access/transfers that we perform."
  },
  {
    "objectID": "posts/BlackMamba/BlackMamba.html",
    "href": "posts/BlackMamba/BlackMamba.html",
    "title": "BlackMamba: Mixture of Experts for State-Space Models",
    "section": "",
    "text": "State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint.\nSo why not combine Mamba with MoE. Well that is what this paper has explored with BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. This Mamba-MoE architecture have the following improvements over a dense transformer:\n\nMamba: Linear computational complexity with respect to input sequence length for both training and inference. Autoregressive generation in constant time and memory.\nMoE: Inference latency and training FLOPs of the equivalent smaller dense base model, while preserving model quality close to an equi-parameter dense model.\n\nBlackMamba architecture simply replaces both the MLP layer in a transformer with an expert layer, and the attention layer with a mamba SSM layer. Further, it used the SwiGLU activation function for the expert MLPs. For the expert router, it used top-1 routing with a Sinkhorn routing function to load-balance between experts. It utilized a novel custom version of the Sinkhorn algorithm which converges substantially faster than vanilla Sinkhorn. Model was trained using Megatron-LM distributed training framework and was trained in bf16 precision on 300B tokens on custom dataset both 340M/1.5B and 630M/2.8B models .\n\n\nBlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. BlackMamba inherits and combines both of the benefits of SSM and MoE architectures, combining linear-complexity generation from SSM with cheap and fast inference from MoE. Moreover, BlackMamba shows that it is capable of rapid generation with both linear time and memory cost.\nPaper: https://arxiv.org/pdf/2402.01771.pdf"
  },
  {
    "objectID": "posts/MambaFormer/MambaFormer.html",
    "href": "posts/MambaFormer/MambaFormer.html",
    "title": "MambaFormer: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks",
    "section": "",
    "text": "State-space models (SSMs), such as Mamba, have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, against Transformer models in standard regression in-context learning (ICL) tasks, it falls short in more complex ICL tasks like Vector-valued MQAR. To address these limitations, the paper has introduced a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.\nBefore jumping to MambaFormer lets us have a deepdive at in-context learning (ICL). In-context learning (ICL) has emerged as one of the most remarkable capabilities of LLMs like GPT-3 and GPT-4. With just a few demonstration examples (few-short learning), these models can rapidly adapt to new tasks and make accurate predictions without any parameter updates. Numerous research studies have been dedicated to understanding the mechanics of Attention in transformer models that enable such meta-in-context-learning capabilities, either through constructive arguments or extensive experimental investigation. Transformer language models are currently the only large models that have been reported to be capable of ICL in practice.\nSo Can attention-free models perform ICL?\nWell that‚Äôs what this paper tries to address. Series of experiment against Transformer, Mumba and MumbaFormer LLMs was carried out for various ICL task such as Linear regression, Sparse linear regression, 2NN regression, Decision Tree, Orthogonal-outlier regression, Many-outlier regression , Sparse parity, Chain-of-Thought I/O and Vector-valued MQAR. MumbaFormer has been the winner in all these tasks. So what is this MambaFormer ?\nMambaFormer is a hybrid architecture that replaces MLP blocks within the transformer with Mamba blocks. Importantly, the architecture also starts with a Mamba block and does not use positional encoding. During ICL evaluations, it was found that MambaFormer consistently achieves a best-of-both-worlds performance compared to Transformer and Mamba.\nPaper: https://arxiv.org/pdf/2402.04248.pdf"
  },
  {
    "objectID": "posts/Hydragen/Hydragen.html",
    "href": "posts/Hydragen/Hydragen.html",
    "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
    "section": "",
    "text": "Transformer-based large language models (LLMs) such as OpenAI GPT3.5 and GPT4 are now deployed to hundreds of millions of users. LLM inference in such scenarios commonly performed on batches of sequences that share a prefix (the system prompt), such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. Even with FlashAttention and PagedAttention models redundantly read the prefix‚Äôs keys and values from GPU memory when computing attention, regardless of whether the prefix is redundantly stored.\nIn order to eliminate these redundant reads paper introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications.\nLet‚Äôs take an example of LLM chat model inference, which processes many sequences that share a large shared prefix (the system prompt). With Hydragen overall attention is decomposed into attention over the shared prefix (batched across all queries in a batch) and attention over the remaining suffixes (independent across sequences, as is normally done). Hydragen‚Äôs attention decomposition allows many matrix vector products to be replaced with fewer matrix-matrix products. Using matrix-matrix products is particularly important as GPUs dedicate an increasingly large ratio of their total FLOPs to tensor cores that are specialized in matrix multiplication\nIn end-to-end benchmarks, Hydragen increases the throughput of CodeLlama-13b by up to 32x over vLLM, a high-performance inference framework that avoids redundant prefix storage but not redundant prefix reads. Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.\nPaper: https://arxiv.org/pdf/2402.05099.pdf"
  },
  {
    "objectID": "posts/Tag-LLM/Tag-LLM.html",
    "href": "posts/Tag-LLM/Tag-LLM.html",
    "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
    "section": "",
    "text": "General-purpose LLMs like LLaMA and GPT-4 have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains such as processing amino acid sequences for proteins (e.g., MTVPDRSEIAG) or SMILES strings for chemical compounds, hampering their adoption for a wide range of scientific problems. Further, Training domain specific LLM requires a significant amount of compute and in-domain data. Fine-tune existing LLMs or perform in-context learning might be another way to go around, but the former is prone to catastrophic forgetting and can hurt the model‚Äôs reasoning abilities\nSo can we effectively repurpose general-purpose LLMs for specialized tasks without compromising their linguistic and reasoning capabilities?\nTo address this issue, the paper has introduced Tag-LLM. A novel model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM‚Äôs embedding layer to condition the LLM. Two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compressing function-solving instructions. Further, a three-stage protocol is also been introduce to learn these tags using auxiliary data and domain knowledge.\nLet‚Äôs consider the task of protein-drug binding affinity prediction. In Tag-LLM method domain tags ‚ü®Protein‚ü©, ‚ü®SMILES‚ü© and a function tag ‚ü®Binding Affinity‚ü© is been injects to the input, which are mapped to specially trained embeddings. The model‚Äôs last hidden state is passed to a task-specific head to generate predictions of the desired type (e.g., a scalar binding affinity value in this case).\nWhile experimenting with the LLaMA-7B model, on a diverse set of ten domains, encompassing eight languages and two specialized scientific domains (protein sequences and SMILES molecule representations). Results demonstrate that the domain tags can act as effective context switchers, and a function tag can be applied to multiple domains to solve different tasks, achieving zero-shot generalization to unseen problems.\nPaper: https://arxiv.org/pdf/2402.05140.pdf\nGithub: https://github.com/sjunhongshen/Tag-LLM"
  },
  {
    "objectID": "posts/PHATGOOSE/PHATGOOSE.html",
    "href": "posts/PHATGOOSE/PHATGOOSE.html",
    "title": "PHATGOOSE: Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "section": "",
    "text": "The availability of Huggingface PEFT modules has made it cheap and easy to modularly adapt a given pre-trained model to a specific task or domain. In the meantime, extremely large-scale language models (LLMs) are now being treated as ‚Äúgeneral-purpose‚Äù and often exhibit strong zero-shot generalization. Relying on zero-shot generalization stands in stark contrast to the aforementioned approach of training specialized models for each task such as PEFT.\nSo can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model ?\nThat‚Äôs what this paper has tried to address with PHATGOOSE : Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts, a post-hoc method that enables zero-shot generalization among specialized models. PHATGOOSE recycles PEFT modules by introducing an additional computationally inexpensive step after training the PEFT-based model itself. Specifically, the entire model (including the newly introduced PEFT modules) is frozen and a per-module gate is trained. This gate (whose parameters are shared across sequence positions) comprises a linear layer followed by a sigmoid nonlinearity that determines whether the activation at a given sequence position should be fed into the module or not. Training this gate only requires a small amount of additional compute compared to performing PEFT. The gates for every module across specialized models are then combined to determine how to route different tokens to different modules during inference using a standard ‚Äútop-k‚Äù routing strategy.\nTo test the effectiveness of PHATGOOSE, T5 family models were used to improve zero-shot generalization on standard benchmarks. Notably, it was found that PHATGOOSE not only outperforms prior methods involving merging experts or retrieving a single expert but can also outperform explicit multitask training in some cases. In qualitative analysis, it was found that PHATGOOSE uses a diverse set of modules to perform a given task, thereby combining abilities from multiple specialized models and, in some cases, producing better performance than the single best-performing expert model. Overall, this work sets the groundwork for a promising new framework for the decentralized development of generalist AI systems.\nPaper : https://arxiv.org/pdf/2402.05859.pdf\nGithub : https://github.com/r-three/phatgoose"
  },
  {
    "objectID": "posts/Fiddler/Fiddler.html",
    "href": "posts/Fiddler/Fiddler.html",
    "title": "Fiddler: CPU-GPU Orchestration for Fast Local Inference of MoE Models",
    "section": "",
    "text": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architectures are showing remarkable performance on various tasks. By activating a subset of experts inside feed-forward layers with a gating mechanism, such models scale up model size and improve model performance with a small computation overhead. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes.\nTo address this paper proposes Fiddler, a fast inference system for LLMs based on Mixture-of-Experts (MoE) architecture at local devices. It allows you to run an unquantized Mixtral-8x7B model (&gt;90GB of parameters) with &gt;3 token/s on a single 24GB GPU. The key idea behind Fiddler is to use the CPU‚Äôs computation power. Existing offloading systems primarily utilize the memory resources available on the CPU, while the computation mainly occurs on the GPU. The typical process involves: (1) When some expert weights are missing from the GPU memory, (2) they are copied from the CPU memory to the GPU memory, then (3) GPU executes the expert layer. Although GPU execution is faster, the data movement introduces significant overhead.\nOn the other hand, Fiddler uses CPU computation resources in addition to memory resources. The process is as follows: (1) when some expert weights are missing on the GPU memory, (2) it copies the activation values from the GPU memory to the CPU memory, instead of copying the weights. (3) The computation of the expert layer then happens on the CPU, and (4) the output activation after the expert is copied back to the GPU.\nThis approach significantly reduces the latency of CPU-GPU communication, especially since the size of activations is considerably smaller than the weight size (batch_size x 4096 versus 3 x 4096 x 14336 per expert for the Mixtral-8x7B) for a small batch size. Despite slower computation speeds on the CPU compared to the GPU, avoiding the weight copying process makes this approach more efficient.\nCompared with DeepSpeed-MII and Mixtral offloading, Fiddler is on average faster by 19.4 and 8.2 times for Environment 1, and by 22.5 and 10.1 times for Environment 2.\nPaper : https://arxiv.org/pdf/2402.07033.pdf\nGithub : https://github.com/efeslab/fiddler"
  },
  {
    "objectID": "posts/GraphMamba/Graph Mamba.html",
    "href": "posts/GraphMamba/Graph Mamba.html",
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "section": "",
    "text": "Graph Transformers (GTs) has shown promising potential in graph representation learning. GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). Recently, Mamba‚Äôs outstanding performance in language modeling, outperforming Transformers of the same size and matching Transformers twice its size, motivates several recent studies to adapt its architecture for different data modalities. Mamba architecture is specifically designed for sequence data and the complex non-causal nature of graphs makes directly applying Mamba on graphs challenging.\nTo address all the above mentioned limitations, the paper presents Graph Mamba Networks (GMNs), a new class of machine learning on graphs based on state space models. Recipe for Graph Mamba Networks is simple : (1) Tokenization: the graph is mapped into a sequence of tokens (m ‚â• 1: subgraph and m = 0: node tokenization) (2) (Optional Step) PE/SE: inductive bias is added to the architecture using information about the position of nodes and the structure of the graph. (3) Local Encoding: local structures around each node are encoded using a subgraph vectorization mechanism. (4) Token Ordering: the sequence of tokens are ordered based on the context. (Subgraph tokenization (m ‚â• 1) has implicit order and does not need this step). (5) (Stack of) Bidirectional Mamba: it scans and selects relevant nodes or subgraphs to flow into the hidden states.\nExperimental evaluations demonstrate that GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets, while consuming less GPU memory. These results show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary.\nPaper : https://arxiv.org/pdf/2402.08678.pdf\nCodes and models will be available soon (Feb 20)."
  }
]