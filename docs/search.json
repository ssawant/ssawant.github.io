[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Santosh Sawant",
    "section": "",
    "text": "LLM Architect learning to innovate, optimize, and scale the next generation of large language models."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "",
    "section": "",
    "text": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models via Reinforcement Learning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 28, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMind Evolution: Evolving Deeper LLM Thinking\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMiniMax-01: Scaling Foundation Models with Lightning Attention\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Large Language Models to Reason in a Continuous Latent Space\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nDec 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLiFT: Leveraging Human Feedback for Text-to-Video Model Alignment\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nDec 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPaliGemma 2: A Family of Versatile VLMs for Transfer\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nOCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Track Anything\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLongKey: Keyphrase Extraction for Long Documents\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVisualLens: Personalization through Visual History\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVisualLens: Personalization through Visual History\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nA Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nIOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSEALONG: Large Language Models Can Self-Improve in Long-context Reasoning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nJanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nNEKO: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBitNet a4.8: 4-bit Activations for 1-bit LLMs\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMemory As Action (MemAct) : Autonomous Context Curation For Long-Horizon Agentic Tasks\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nStructrag: Boosting Knowledge Intensive Reasoning Of Llms Via Inference-Time Hybrid Information Structurization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBacktracking Improves Generation Safety\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRULER : A Model-Agnostic Method to Control Generated Length for Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nStyle over Substance: failure modes of LLM judges in alignment benchmarking.\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMaskLLM: Learnable Semi-Structured Sparsity for Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Text Embedders Few-Shot Learners\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Contextual Retrieval\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Language Models to Self-Correct via Reinforcement Learning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Language Models to Self-Correct via Reinforcement Learning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPromptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Harmonized Chain of Thought\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nOneGen: efficient one-pass unified generation and retrieval for llms\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAgent Workflow Memory (AWM)\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMemoRAG: moving towards next-gen rag via memory-inspired knowledge discovery\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGraphRAG auto-tuning provides rapid adaptation to new domains\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nmPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nSep 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Verifiers: Reward Modeling as Next-Token Prediction\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGEagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Detection of Toxic Prompts in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Pruning and Distillation in Practice: The Minitron Approach\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nStrategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Pruning and Distillation in Practice: The Minitron Approach\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTransfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nxGen-MM (BLIP-3): A Family of Open Large Multimodal Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nrStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPAD: Prioritize Alignment in Dataset Distillation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nCODEXGRAPH: Bridging Large Language Models and Code Repositories via Code Graph Databases\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSynthesizing Text-to-SQL Data from Weak and Strong LLMs\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDiT-MoE : Scaling Diffusion Transformers to 16 Billion Parameters\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDDK: Distilling Domain Knowledge for Efficient Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChain of Diagnosis (CoD): Towards an Interpretable Medical Agent\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLAMBDA: A Large Model Based Data Agent\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVILA2: VILA Augmented VILA\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nThe Llama 3 Herd of Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBOND: Aligning LLMs with Best-of-N Distillation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nQ-Sparse: All Large Language Models can be Fully Sparsely-Activated\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond KV Caching: Shared Attention for Efficient LLMs\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nE5-V: Universal Embeddings with Multimodal Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nNeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAgentInstruct: Toward Generative Teaching with Agentic Flows\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMetron: Holistic Performance Evaluation Framework for LLM Inference Systems\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nComposable Interventions for Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAssociative Recurrent Memory Transformer\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReducing Transformer Key-Value Cache Size with Cross-Layer Attention\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAdam-mini: Use Fewer Learning Rates To Gain More\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSearching for Best Practices in Retrieval-Augmented Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMInference: a Million-token inference on a single A100 machine\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMIRAI: Evaluating LLM Agents for Event Forecasting\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMeta Large Language Model Compiler: Foundation Models of Compiler Optimization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInstruction Pre-Training: Language Models are Supervised Multi Task Learners\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nWhiteboard-of-Thought: Thinking Step-by-Step Across Modalities\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-MoE: Towards Compositional Large Language Models with Self-Specialized Experts\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTHEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTHEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAd Auctions for LLMs via Retrieval Augmented Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Alignment and Robustness with Circuit Breakers\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Alignment and Robustness with Circuit Breakers\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTEXTGRAD : Automatic “Differentiation” via Text\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHUSKY: A Unified, Open-Source Language Agent for Multi-Step Reasoning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMixture-of-Agents : Enhances Large Language Model Capabilities\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBuffer of Thoughts: Thought-Augmented Reasoning with Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBlock Transformer: Global-to-Local Language Modeling for Fast Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nShow, Don’t Tell: Aligning Language Models with Demonstrated Feedback\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nContextual Position Encoding: Learning to Count What’s Important\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi–layered Thoughts\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nNearest Neighbor Speculative Decoding for LLM Generation and Attribution\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nZamba: A Compact 7B SSM Hybrid Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLayer-Condensed KV Cache for Efficient Inference of Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nXmodel-VLM: A Simple Baseline for Multimodal Vision Language Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSUTRA: Scalable Multilingual language model architecture\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLinearizing Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Local to Global: A Graph RAG Approach to Query-Focused Summarization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nIs Flash Attention Stable?\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBetter & Faster Large Language Models via Multi-token Prediction\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nNeMo-Aligner: Scalable Toolkit for Efficient Model Alignment\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nOctopus v4: Graph of language models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReplacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMake Your LLM Fully Utilize the Context\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nCodecLM: Aligning Language Models with Tailored Synthetic Data\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM-R2 : A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTransformerFAM: Feedback attention is working memory\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrentGemma: Moving Past Transformers for Efficient Open Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTrust Region Direct Preference Optimization (TR-DPO) : Learn Your Reference Model for Real Good Alignment\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRHO-1: Not All Tokens Are What You Need\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDirect Nash Optimization: Teaching Language Models to Self-Improve with General Preferences\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nStream of Search (SoS): Learning to Search in Language\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReFT: Representation Finetuning for Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nsDPO: Don’t Use Your Data All at Once\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGecko: Versatile Text Embeddings Distilled from Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nJamba: A Hybrid Transformer-Mamba Language Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Gemini: Mining the Potential of Multi-modality Vision Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRigorLLM: Resilient Guardrails for large language models against undesired content\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSiMBA: Simplified Mamba-based Architecture for Vision and Multivariate Time series\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nCobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary Optimization of Model Merging Recipes\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nmPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPERL: Parameter Efficient Reinforcement Learning from Human Feedback\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRAFT: Adapting Language Model to Domain Specific RAG\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nUSER-LLM: Efficient LLM Contextualization with User Embeddings\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMoAI: Mixture of All Intelligence for Large Language and Vision Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVideoMamba: State Space Model for Efficient Video Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDesign2Code: How Far Are We From Automating Front-End Engineering?\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVisionLLaMA : A Unified LLaMA Interface for Vision Tasks\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond Language Models: Byte Models are Digital World Simulators\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nThe Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChunkLlama : Training-Free Long-Context Scaling of Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTinyLLaVA: A Framework of Small-scale Large Multimodal Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nThe FinBen: An Holistic Financial Benchmark for Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGRIT : Generative Representational Instruction Tuning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAespa: Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Mamba: Towards Learning on Graphs with State Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFiddler: CPU-GPU Orchestration for Fast Local Inference of MoE Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPHATGOOSE: Learning to Route Among Specialized Experts for Zero-Shot Generalization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTag-LLM: Repurposing General-Purpose LLMs for Specialized Domains\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHydragen: High-Throughput LLM Inference with Shared Prefixes\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMambaFormer: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBlackMamba: Mixture of Experts for State-Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRepeat After Me: Transformers are Better than State Space Models at Copying\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRe3val: Reinforced and Reranked Generative Retrieval\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFIND: INterface for Foundation models’ embeDDings\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEAGLE: Extrapolation Algorithm for Greater Language-model Efficiency\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMambaByte: Token-free Selective State Space Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInstruction-Tune Llama2 with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTowards Conversational Diagnostic AI\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChatQA: Building GPT-4 Level Conversational QA Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Fine-Tune LLMs with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMerge Model using Mergekit\n\n\n\n\n\n\ntools\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTuning Language Models by Proxy\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Evaluation Improves Selective Generation in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReciprocal Rank Fusion (RRF) with LambdaMART: Context Tuning for Retrieval Augmented Generation (RAG)\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChain of Thought (CoT): The Impact of Reasoning Step Length on Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInfinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSoaring from 4K to 400K: Extending LLM’s Context with Activation Beacon\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDOCLLM: A Layout Aware Generative Language Models for Multi model document understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Play Fine-Tuning (SPIN): Converts Weak Language Models to Strong Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nA Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMamba-Chat: A Chat LLM based on State Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nKwaiAgents: Generalized Information-seeking Agent System with LLMs - 2 Open-source models fine tuned for agent systems! Better than GPT-3.5 turbo as an agent!\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "title": "Merge Model using Mergekit",
    "section": "",
    "text": "Model merging is a technique that combines two or more LLMs into a single model. It’s a relatively new and experimental method to create new models for cheap (no GPU required). Model merging works surprisingly well and produced many state-of-the-art models on the Open LLM Leaderboard.\nIn this tutorial, we will implement it using the mergekit library. More specifically, we will review four merge methods and provide examples of configurations. Then, we will use mergekit to create our own model"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "title": "Merge Model using Mergekit",
    "section": "Merge models",
    "text": "Merge models\n\n!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle\n\nFetching 8 files: 100% 8/8 [00:00&lt;00:00, 18275.83it/s]\nFetching 11 files: 100% 11/11 [00:00&lt;00:00, 21670.90it/s]\n  0% 0/291 [00:00&lt;?, ?it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for model.embed_tokens.weight\n 70% 203/291 [02:58&lt;00:44,  1.98it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for lm_head.weight\n100% 291/291 [04:22&lt;00:00,  1.11it/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n!pip install -qU huggingface_hub\n\nfrom huggingface_hub import ModelCard, ModelCardData\nfrom jinja2 import Template\n\nusername = \"santoshsawant\"\n\ntemplate_text = \"\"\"\n---\nlicense: apache-2.0\ntags:\n- merge\n- mergekit\n- lazymergekit\n{%- for model in models %}\n- {{ model }}\n{%- endfor %}\n---\n\n# {{ model_name }}\n\n{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n\n{%- for model in models %}\n* [{{ model }}](https://huggingface.co/{{ model }})\n{%- endfor %}\n\n## Configuration\n\n```yaml\n{{- yaml_config -}}\n```\n\"\"\"\n\n# Create a Jinja template object\njinja_template = Template(template_text.strip())\n\n# Get list of models from config\ndata = yaml.safe_load(yaml_config)\nif \"models\" in data:\n    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\nelif \"parameters\" in data:\n    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\nelif \"slices\" in data:\n    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\nelse:\n    raise Exception(\"No models or slices found in yaml config\")\n\n# Fill the template\ncontent = jinja_template.render(\n    model_name=MODEL_NAME,\n    models=models,\n    yaml_config=yaml_config,\n    username=username,\n)\n\n# Save the model card\ncard = ModelCard(content)\ncard.save('merge/README.md')\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/330.1 kB ? eta -:--:--     ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.4/330.1 kB 2.2 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 327.7/330.1 kB 5.3 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.1/330.1 kB 4.6 MB/s eta 0:00:00\n\n\n\nfrom google.colab import userdata\nfrom huggingface_hub import HfApi\n\nusername = \"santoshsawant\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=userdata.get(\"huggingface\"))\n\napi.create_repo(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    repo_type=\"model\"\n)\napi.upload_folder(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    folder_path=\"merge\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/santoshsawant/NeuralHermes-7B-slerp/commit/57dae104809557372cabe688eb608448d32fa485', commit_message='Upload folder using huggingface_hub', commit_description='', oid='57dae104809557372cabe688eb608448d32fa485', pr_url=None, pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "1. Define our use case",
    "text": "1. Define our use case\nWhen fine-tuning LLMs, it is important you know your use case and the task you want to solve. This will help you to choose the right model or help you to create a dataset to fine-tune your model. If you haven’t defined your use case yet. You might want to go back to the drawing board. I want to mention that not all use cases require fine-tuning and it is always recommended to evaluate and try out already fine-tuned models or API-based models before fine-tuning your own model.\nAs an example, we are going to use the following use case:\n\nWe want to fine-tune a model, which can generate SQL queries based on a natural language instruction, which can then be integrated into our BI tool. The goal is to reduce the time it takes to create a SQL query and make it easier for non-technical users to create SQL queries.\n\nText to SQL can be a good use case for fine-tuning LLMs, as it is a complex task that requires a lot of (internal) knowledge about the data and the SQL language."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "2. Setup development environment",
    "text": "2. Setup development environment\nOur first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven’t heard of trl yet, don’t worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n\n# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  # \"trl==0.7.10\" # \\\n  # \"peft==0.7.1\" \\\n\n# install peft & trl from github\n!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f--upgrade\n\nIf you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention.\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\n\nimport torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\n# install flash-attn\n!pip install ninja packaging\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nWe will use the Hugging Face Hub as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the Hugging Face for this. After you have an account, we will use the login util from the huggingface_hub package to log into our account and store our token (access key) on the disk.\n\nfrom huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "3. Create and prepare the dataset",
    "text": "3. Create and prepare the dataset\nOnce you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. The dataset should be a diverse set of demonstrations of the task you want to solve. There are several ways to create such a dataset, including: * Using existing open-source datasets, e.g., Spider * Using LLMs to create synthetically datasets, e.g., Alpaca * Using Humans to create datasets, e.g., Dolly. * Using a combination of the above methods, e.g., Orca\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nIn our example we will use an already existing dataset called sql-create-context, which contains samples of natural language instructions, schema definitions and the corresponding SQL query.\nWith the latest release of trl we now support popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and trl will take care of the rest. Those formats include: * conversational format\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n\ninstruction format\n\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\nIn our example we are going to load our open-source dataset using the 🤗 Datasets library and then convert it into the the conversational format, where we include the schema definition in the system message for our assistant. We’ll then save the dataset as jsonl file, which we can then use to fine-tune our model. We are randomly downsampling the dataset to only 10,000 samples.\nNote: This step can be different for your use case. For example, if you have already a dataset from, e.g. working with OpenAI, you can skip this step and go directly to the fine-tuning step.\n\nfrom datasets import load_dataset\n\n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\nSCHEMA:\n{schema}\"\"\"\n\ndef create_conversation(sample):\n  return {\n    \"messages\": [\n      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n      {\"role\": \"user\", \"content\": sample[\"question\"]},\n      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n    ]\n  }  \n\n# Load dataset from the hub\ndataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\n\n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=2500/12500)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk \ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Fine-tune LLM using trl and the SFTTrainer",
    "text": "4. Fine-tune LLM using trl and the SFTTrainer\nWe are now ready to fine-tune our model. We will use the SFTTrainer from trl to fine-tune our model. The SFTTrainer makes it straightfoward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including: * Dataset formatting, including conversational and instruction format * Training on completions only, ignoring prompts * Packing datasets for more efficient training * PEFT (parameter-efficient fine-tuning) support including Q-LoRA * Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\nWe will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA blog post.\nNow, lets get started! 🚀\nFirst, we need to load our dataset from disk.\n\nfrom datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n\nNext, we will load our LLM. For our use case we are going to use CodeLlama 7B. CodeLlama is a Llama model trained for general code synthesis and understanding. But we can easily swap out the model for another model, e.g. Mistral or Mixtral models, TII Falcon, or any other LLMs by changing our model_id variable. We will use bitsandbytes to quantize our model to 4-bit.\nNote: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU.\nCorrectly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In trl we have a convinient method called setup_chat_format, which: * Adds special tokens to the tokenizer, e.g. &lt;|im_start|&gt; and &lt;|im_end|&gt;, to indicate the start and end of a conversation. * Resizes the model’s embedding layer to accommodate the new tokens. * Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import setup_chat_format\n\n# Hugging Face model id\nmodel_id = \"codellama/CodeLlama-7b-hf\" # or `mistralai/Mistral-7B-v0.1`\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'right' # to prevent warnings\n\n# # set chat template to OAI chatML, remove if you start from a fine-tuned model\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\nThe SFTTrainer  supports a native integration with peft, which makes it super easy to efficiently tune LLMs using, e.g. QLoRA. We only need to create our LoraConfig and provide it to the trainer. Our LoraConfig parameters are defined based on the qlora paper and sebastian’s blog post.\n\nfrom peft import LoraConfig\n\n# LoRA config based on QLoRA paper & Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=128,\n        lora_dropout=0.05,\n        r=256,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\", \n)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n    num_train_epochs=3,                     # number of training epochs\n    per_device_train_batch_size=3,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=10,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to hub\n    report_to=\"tensorboard\",                # report metrics to tensorboard\n)\n\nWe now have every building block we need to create our SFTTrainer to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 3072 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n\nStart training our model by calling the train() method on our Trainer instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n\n# start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model \ntrainer.save_model()\n\nThe training with Flash Attention for 3 epochs with a dataset of 10k samples took 01:29:58 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of only 1.8$.\n\n# free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n\n\nOptional: Merge LoRA adapter in to the original model\nWhen using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. This will save a default model, which can be used for inference.\nNote: You might require &gt; 30GB CPU Memory.\n\n\n#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import PeftModel, PeftConfig\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import AutoPeftModelForCausalLM\n\n# # Load PEFT model on CPU\n# config = PeftConfig.from_pretrained(args.output_dir)\n# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\n# tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n# model.resize_token_embeddings(len(tokenizer))\n# model = PeftModel.from_pretrained(model, args.output_dir)\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     args.output_dir,\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )  \n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\nNote: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out Evaluate LLMs and RAG a practical example using Langchain and Hugging Face blog post.\n\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline \n\npeft_model_id = \"./code-llama-7b-text-to-sql\"\n# peft_model_id = args.output_dir\n\n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nLet’s load our test dataset try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randint\n\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Test on sample \nprompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n\nNice! Our model was able to generate a SQL query based on the natural language instruction. Lets evaluate our model on the full 2,500 samples of our test dataset. Note: As mentioned above, evaluating generative models is not a trivial task. In our example we used the accuracy of the generated SQL based on the ground truth SQL query as our metric. An alternative way could be to automatically execute the generated SQL query and compare the results with the ground truth. This would be a more accurate metric but requires more work to setup.\n\nfrom tqdm import tqdm\n\n\ndef evaluate(sample):\n    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n        return 1 \n    else:\n        return 0\n\nsuccess_rate = []\nnumber_of_eval_samples = 1000\n# iterate over eval dataset and predict\nfor s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n    success_rate.append(evaluate(s))\n\n# compute accuracy\naccuracy = sum(success_rate)/len(success_rate)\n\nprint(f\"Accuracy: {accuracy*100:.2f}%\")  \n        \n\nWe evaluated our model on 1000 samples from the evaluation dataset and got an accuracy of 79.50%, which took ~25 minutes. This is quite good, but as mentioned you need to take this metric with a grain of salt. It would be better if we could evaluate our model by running the qureies against a real database and compare the results. Since there might be different “correct” SQL queries for the same instruction. There are also several ways on how we could improve the performance by using few-shot learning, using RAG, Self-healing to generate the SQL query."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "6. Deploy the LLM for Production",
    "text": "6. Deploy the LLM for Production\nYou can now deploy your model to production. For deploying open LLMs into production we recommend using Text Generation Inference (TGI). TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching for the most popular open LLMs, including Llama, Mistral, Mixtral, StarCoder, T5 and more. Text Generation Inference is used by companies as IBM, Grammarly, Uber, Deutsche Telekom, and many more. There are several ways to deploy your model, including:\n\nDeploy LLMs with Hugging Face Inference Endpoints\nHugging Face LLM Inference Container for Amazon SageMaker\nDIY\n\nIf you have docker installed you can use the following command to start the inference server.\nNote: Make sure that you have enough GPU memory to run the container. Restart kernel to remove all allocated GPU memory from the notebook.\n\n%%bash \n# model=$PWD/{args.output_dir} # path to model\nmodel=$(pwd)/code-llama-7b-text-to-sql # path to model\nnum_shard=1             # number of shards\nmax_input_length=1024   # max input length\nmax_total_tokens=2048   # max total tokens\n\ndocker run -d --name tgi --gpus all -ti -p 8080:80 \\\n  -e MODEL_ID=/workspace \\\n  -e NUM_SHARD=$num_shard \\\n  -e MAX_INPUT_LENGTH=$max_input_length \\\n  -e MAX_TOTAL_TOKENS=$max_total_tokens \\\n  -v $model:/workspace \\\n  ghcr.io/huggingface/text-generation-inference:latest\n\nOnce your container is running you can send requests.\n\nimport requests as r \nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom random import randint\n\n# Load our test dataset and Tokenizer again\ntokenizer = AutoTokenizer.from_pretrained(\"code-llama-7b-text-to-sql\")\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# generate the same prompt as for the first local test\nprompt = tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\nrequest= {\"inputs\":prompt,\"parameters\":{\"temperature\":0.2, \"top_p\": 0.95, \"max_new_tokens\": 256}}\n\n# send request to inference server\nresp = r.post(\"http://127.0.0.1:8080/generate\", json=request)\n\noutput = resp.json()[\"generated_text\"].strip()\ntime_per_token = resp.headers.get(\"x-time-per-token\")\ntime_prompt_tokens = resp.headers.get(\"x-prompt-tokens\")\n\n# Print results\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{output}\")\nprint(f\"Latency per token: {time_per_token}ms\")\nprint(f\"Latency prompt encoding: {time_prompt_tokens}ms\")\n\nAwesome, Don’t forget to stop your container once you are done.\n\n!docker stop tgi"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "1. Define our use case in detail and create a template for our instructions",
    "text": "1. Define our use case in detail and create a template for our instructions\nBefore we describe our use case, we need to better understand what even is an instruction.\n\nAn instruction is a piece of text or prompt that is provided to an LLM, like Llama, GPT-4, or Claude, to guide it to generate a response. Instructions allow humans to steer the conversation and constrain the language model’s output to be more natural, useful, and aligned with the user’s goals. Crafting clear, well-formulated instructions is key to productive conversations.\n\nExamples of instructions are listed below in the table.\n\n\n\n\n\n\n\nCapability\nExample Instruction\n\n\n\n\nBrainstorming\nProvide a diverse set of creative ideas for new flavors of ice cream.\n\n\nClassification\nCategorize these movies as either comedy, drama, or horror based on the plot summary.\n\n\nClosed QA\nAnswer the question ‘What is the capital of France?’ with a single word.\n\n\nGeneration\nWrite a poem in the style of Robert Frost about nature and the changing seasons.\n\n\nInformation Extraction\nExtract the names of the main characters from this short story.\n\n\nOpen QA\nWhy do leaves change color in autumn? Explain the scientific reasons.\n\n\nSummarization\nSummarize this article on recent advancements in renewable energy in 2-3 sentences.\n\n\n\nAs described in the beginning, we want to fine-tune a model to be able to generate instructions based on input. (output). We want to use this as a way to create synthetic datasets to personalize LLMs and Agents.\nConverting the idea into a basic prompt template following the Alpaca format we get.\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nDear [boss name],\n\nI'm writing to request next week, August 1st through August 4th,\noff as paid time off.\n\nI have some personal matters to attend to that week that require \nme to be out of the office. I wanted to give you as much advance \nnotice as possible so you can plan accordingly while I am away.\n\nPlease let me know if you need any additional information from me \nor have any concerns with me taking next week off. I appreciate you \nconsidering this request.\n\nThank you, [Your name]\n\n### Response:\nWrite an email to my boss that I need next week 08/01 - 08/04 off."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "2. Create an instruction dataset",
    "text": "2. Create an instruction dataset\nAfter we defined our use case and prompt template, we need to create our instruction dataset. Creating a high-quality instruction dataset is key for a good-performing model. Research shows that “Less Is More for Alignment” shows that creating a high-quality, low-quantity (~1000 samples) dataset can achieve the same performance as less-quality and high-quantity datasets.\nThere are several ways to create an instruction dataset, including:\n\nUsing an existing dataset and converting it into an instruction dataset, e.g., FLAN\nUse existing LLMs to create synthetically instruction datasets, e.g., Alpaca\nUse Humans to create instructions datasets, e.g., Dolly.\n\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nTo keep it simple, we are going to use Dolly an open-source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nLet’s start coding, but first, let’s install our dependencies.\n\n!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors&gt;=0.3.1\" --upgrade\n\nTo load the databricks/databricks-dolly-15k dataset, we use the load_dataset() method from the 🤗 Datasets library.\n\nfrom datasets import load_dataset\nfrom random import randrange\n\n# Load dataset from the hub\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\nprint(f\"dataset size: {len(dataset)}\")\nprint(dataset[randrange(len(dataset))])\n# dataset size: 15011\n\nFound cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n\n\ndataset size: 15011\n{'instruction': 'On what month and day was Antwan Deon Odom born?', 'context': 'Antwan Deon Odom (born September 24, 1981) is a former American football defensive end. He was drafted by the Tennessee Titans in the second round of the 2004 NFL Draft. He played college football at Alabama. He has also played for the Cincinnati Bengals.', 'response': 'September 24', 'category': 'closed_qa'}\n\n\nTo instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a formatting_function that takes a sample and returns a string with our format instruction.\n\ndef format_instruction(sample):\n    return f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n{sample['instruction']}\n\"\"\"\n\nLet’s test our formatting function on a random example.\n\nfrom random import randrange\n\nprint(format_instruction(dataset[randrange(len(dataset))]))\n\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nSir Dorabji Tata and Allied Trusts and Sir Ratan Tata Trust\n\n### Response:\nWhat are the names of Tata trusts which Ratan Tata heads?"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "3. Instruction-tune Llama 2 using trl and the SFTTrainer",
    "text": "3. Instruction-tune Llama 2 using trl and the SFTTrainer\nWe will use the recently introduced method in the paper “QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation” by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:\n\nQuantize the pre-trained model to 4 bits and freeze it.\nAttach small, trainable adapter layers. (LoRA)\nFinetune only the adapter layers while using the frozen quantized model for context.\n\nIf you want to learn more about QLoRA and how it works, I recommend you to read the Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA blog post.\n\nFlash Attention\nFlash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention. Flash Attention is currently only available for Ampere (A10, A40, A100, …) & Hopper (H100, …) GPUs. You can check if your GPU is supported and install it using the following command:\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\npython -c \"import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\"\npip install ninja packaging\nMAX_JOBS=4 pip install flash-attn --no-build-isolation\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nThe example supports the use of Flash Attention for all Llama checkpoints, but is not enabled by default. To use Flash Attention change the value of use_flash_attentin to True\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nuse_flash_attention = False\n\n# Hugging Face model id\nmodel_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    use_flash_attention_2=use_flash_attention,\n    device_map=\"auto\",\n)\nmodel.config.pretraining_tp = 1\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nThe SFTTrainer  supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer.\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# LoRA config based on QLoRA paper\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\", \n)\n\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"llama-7-int4-dolly\",\n    num_train_epochs=3,\n    per_device_train_batch_size=6 if use_flash_attention else 4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    bf16=True,\n    fp16=False,\n    tf32=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    disable_tqdm=False,  # disable tqdm since with packing values are in correct\n)\n\n\n# Upcast layer for flash attnetion\nif use_flash_attention:\n    from utils.llama_patch import upcast_layer_for_flash_attention\n    torch_dtype = torch.bfloat16 if args.bf16 else torch.float16 if args.fp16 else torch.float32\n    model = upcast_layer_for_flash_attention(model, torch_dtype)\n\nmodel = get_peft_model(model, peft_config)\n\nWe now have every building block we need to create our SFTTrainer to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=format_instruction, \n    args=args,\n)\n\nStart training our model by calling the train() method on our Trainer instance.\n\n# train\ntrainer.train() # there will not be a progress bar since tqdm is disabled\n\n# save model\ntrainer.save_model()\n\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\nThe training without Flash Attention enabled took 03:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 3.7$. The training with Flash Attention enabled took 02:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 2.6$.\nThe results using Flash Attention are mind blowing and impressive, 1.5x faster and 30% cheaper."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model.\n\nif use_flash_attention:\n    # unpatch flash attention\n    from utils.llama_patch import unplace_flash_attn_with_attn\n    unplace_flash_attn_with_attn()\n    \nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\nargs.output_dir = \"llama-7-int4-dolly\"\n\n# load base LLM model and tokenizer\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n) \ntokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n\nLet’s load the dataset again with a random sample to try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\nsample = dataset[randrange(len(dataset))]\n\nprompt = f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n\nprint(f\"Prompt:\\n{sample['response']}\\n\")\nprint(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\nprint(f\"Ground truth:\\n{sample['instruction']}\")\n\nNice! our model works! If want to accelerate our model we can deploy it with Text Generation Inference. Therefore we would need to merge our adapter weights into the base model.\n\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n) \n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\n\n# push merged model to the hub\n# merged_model.push_to_hub(\"user/repo\")\n# tokenizer.push_to_hub(\"user/repo\")"
  },
  {
    "objectID": "posts/EAGLE/EAGLE.html",
    "href": "posts/EAGLE/EAGLE.html",
    "title": "EAGLE: Extrapolation Algorithm for Greater Language-model Efficiency",
    "section": "",
    "text": "Auto-regressive decoding has become the de facto standard for large language models (LLMs). This process generates output tokens one at a time, which makes the generation by LLMs both costly and slow. Speculative sampling based methods offer a solution to this challenge. They divide the generation process of LLMs into two stages: the draft stage, where potential tokens are conjectured at a low cost, and the verification stage, where these tokens are validated in parallel through a single forward pass of the LLM.\nSpeculative sampling aims to accelerate generation by minimizing time overhead and increasing the acceptance rate of drafts generated by the original Large Language Model (LLM). Popular methods like Lookahead and Medusa achieve this by reducing overhead and enhancing acceptance rates. Nonetheless, their full potential is limited by the lower accuracy of the drafts they generate.\nEAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), is a simple framework for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding.\nCompared with existing speculative sampling-based techniques, the advantages of EAGLE include:\n\nSimplicity: EAGLE adds only a lightweight plug-in (a single transformer decoder layer) to the LLM, which can be easily deployed in a production.\nReliability: EAGLE does not involve any fine-tuning of the original LLM, and the preservation of the output distribution by EAGLE is theoretically guaranteed for both the greedy and non-greedy settings. This is in sharp contrast to Lookahead and Medusa which focuses on greedy settings only.\nSpeed: EAGLE stands out as the fastest framework within the family of speculative sampling. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface’s implementations.\n\nPaper : https://arxiv.org/pdf/2401.15077.pdf\nCode : https://github.com/SafeAILab/EAGLE"
  },
  {
    "objectID": "posts/MambaByte/MambaByte.html",
    "href": "posts/MambaByte/MambaByte.html",
    "title": "MambaByte: Token-free Selective State Space Model",
    "section": "",
    "text": "In December 2023, “Mamba : Linear-Time Sequence Modeling with Selective State Spaces” paper was release and with it the whole discussion about Mamba (SSM) been a viable replacement for Transformer base model had started as Mamba achieved 4-5x higher throughput than a Transformer of a similar size. To capitalize on this, there is a growing trend of re-implementing various transformer based LLMs on Mamba (SSM) architecture such as MoE-Mamba and VMamba.\nLooks like Token free LLM is also going in this direction. Typically, Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. Look no further, we have MambaByte to save.\nMambaByte, a token-free adaptation of the Mamba state space model, trained autoregressive on byte sequences. MambaByte eliminates the need for patching and achieves better performance and computational efficiency compared to bite level Transformers. MambaByte being a straightforward adaptation of the Mamba architecture, utilizes a linear time approach for sequence modeling by incorporating a selection mechanism that is more effective for discrete data like text section parallel scans for linear recurrences\nMambaByte outperforms other byte-level models over several datasets and shows competitive results with subword Transformers, thus serving as a promising tokenization alternative. SSMs also enable significantly fast text generation due to their recurrent nature, making byte models practical.\nPaper : https://arxiv.org/pdf/2401.13660.pdf"
  },
  {
    "objectID": "posts/Towards Conversational Diagnostic AI/Towards Conversational Diagnostic AI.html",
    "href": "posts/Towards Conversational Diagnostic AI/Towards Conversational Diagnostic AI.html",
    "title": "Towards Conversational Diagnostic AI",
    "section": "",
    "text": "With the Med-PaLM series of LLMs Google is one of the few companies you can claim expertise in building medical domain specific LLMs. The latest addition has been AMIE (Articulate Medical Intelligence Explorer).\nAMIE is a conversational medical AI optimized for diagnostic dialogue. AMIE is instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical reasoning, question answering, and summarization datasets.\nAMIE has a self-play based simulated dialogue environment with automated feedback mechanisms to scale its capabilities across various medical contexts and specialities. There are two types of self-play loops in place : 1. An “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; 2. An “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations.\nDuring online inference, AMIE used a chain-of-reasoning (COR) strategy to progressively refine its response conditioned on the current conversation to arrive at an accurate and grounded reply to the patient in each dialogue turn\nAcross multiple axes corresponding to both specialist physician (28 out of 32) and patient actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest. However, the results should be interpreted with appropriate caution. Translating from this limited scope of experimental, towards real-world tools, requires significant additional research and development.\nPaper : https://arxiv.org/pdf/2401.05654.pdf"
  },
  {
    "objectID": "posts/ChatQA/ChatQA.html",
    "href": "posts/ChatQA/ChatQA.html",
    "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
    "section": "",
    "text": "With all open source LLM models trying to outperform GPT-4 one may wonder, which one has truly been successful in Conversational QA - one of the elementary use cases of LLMs.\nIntroducing ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. It proposes a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, it fine-tunes a dense retriever on a multiturn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models\nIn addition, it demonstrates that fine-tuning a single-turn query retriever using its own curated conversational QA data performs comparable to the state-of-the-art LLM-based query rewriting model, without the need of extra computational time and potential API cost from rewriting.\nPaper : https://arxiv.org/pdf/2401.10225.pdf\n  arxiv:2401.10225"
  },
  {
    "objectID": "posts/Medusa/Medusa.html",
    "href": "posts/Medusa/Medusa.html",
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
    "section": "",
    "text": "@article{cai2024medusa,\n  title   = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},\n  author  = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},\n  year    = {2024},\n  journal = {arXiv preprint arXiv: 2401.10774}\n}\n \nWhy is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge\n\nLarge memory footprint. Both model parameters and intermediate states are needed in memory at inference time. For example, The KV cache should be stored in memory during decoding time; E.g. For a batch size of 512 and context length of 2048, the KV cache totals 3TB, that is 3x the model size. Inference cost from the attention mechanism scales quadratically with input sequence length.\nLow parallelizability. Inference generation is executed in an autoregressive fashion, making the decoding process hard to parallel.\n\nThis paper introduces MEDUSA, a method for improving inference in Large Language Models (LLMs) by adding extra decoding heads to predict multiple tokens in parallel. MEDUSA achieves significant speedup without compromising generation quality.\nMedusa adds extra “heads” to LLMs to predict multiple future tokens simultaneously. When augmenting a model with Medusa, the original model stays untouched, and only the new heads are fine-tuned during training. During generation, these heads each produce multiple likely words for the corresponding position. These options are then combined and processed using a tree-based attention mechanism. Finally, a typical acceptance scheme is employed to pick the longest plausible prefix from the candidates for further decoding.\nSo how does Medusa solve the challenges associated with speculative decoding ?\n\nInstead of introducing a new model, we train multiple decoding heads on the same model.\nThe training is parameter-efficient so that even the “GPU-Poor” can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.\nRelaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.\n\nDuring experimentation, Medusa delivers approximately a 2x speed (1.94x) increase across a range of Vicuna models. Will be interesting to see Medusa’s performance with other open source foundational models.\nPaper : https://arxiv.org/pdf/2401.10774.pdf"
  },
  {
    "objectID": "posts/Tuning Language Models by Proxy/Tuning Language Models by Proxy.html",
    "href": "posts/Tuning Language Models by Proxy/Tuning Language Models by Proxy.html",
    "title": "Tuning Language Models by Proxy",
    "section": "",
    "text": "These days capabilities of large pretrained LLMs can be significantly enhanced for specific domains of interest or task using additional fine tuning. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private e.g. OpenAI GPT-4.\nPaper has introduced the Proxy-Tuning method, a lightweight decoding-time algorithm that can be used to customize large pretrained language models without accessing their weights. It achieves similar results to direct tuning and can be applied for domain adaptation and task-specific finetuning.\nIn the experiments, it apply proxy-tuning to steer a large pretrained (base) model (LLAMA2-13B or 70B) using small, cheaper-to-tune (anti-)experts (based on LLAMA2-7B) for instruction-following, domain adaptation, and task fine tuning. When it applies proxy-tuning to LLAMA2-70B using proxies of only 7B size, it can close 88% of the gap between LLAMA2-70B and it’s truly-tuned CHAT version, when evaluated across knowledge, reasoning, and safety benchmarks.\nPaper : https://arxiv.org/pdf/2401.08565.pdf"
  },
  {
    "objectID": "posts/MoE-LLaVA/MoE-LLaVA.html",
    "href": "posts/MoE-LLaVA/MoE-LLaVA.html",
    "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
    "section": "",
    "text": "For Large Vision-Language Models (LVLMs), scaling the model can effectively improve performance. However, expanding model parameters significantly increases the training and inferring costs, as all model parameters are activated for each token in the calculation.\nIn contrast, sparse Mixtures of Experts (MoE) effectively scale model capacity by using fixed activated parameters to process data, which has thrived in the field of NLP . Recently, Mistral LLM equipped with the MoE layers has gained popularity in LLMs. Mixtral-MoE8×7B achieves performance comparable to LLaMA 2-70B with fewer computational resources.\nHowever, directly applying MoE to train sparse LVLMs is challenging as it leads to significant performance degradation. Proper initialization is crucial for sparsifying the LVLM, and that’s exactly what MoE-tuning does. MoW-tuning - a novel three-stage training strategy for adapting MoE to LVLMs and preventing the model degradation caused by sparsity.\nMoE-LLaVA model operates by using multiple sparse paths, where each token is directed to different experts through a router. These activated experts collaboratively process the tokens, while inactive paths remain dormant. By stacking MoE encoder layers iteratively, the model creates a sparse pathway to a larger and more potent Large Vocabulary Language Model (LVLM). This approach allows for efficient and effective processing of input data by dynamically routing tokens to appropriate experts for processing.\nDuring experimentation MoELLaVA model demonstrates great potential for multi-modal understanding and hallucination inhibition. MoELLaVA achieves comparable performance to state-of-the-art 7B models with only 3B sparse activated parameters on multiple visual understanding datasets, and outperforms LLaVA-1.5-13B by 1.1% on the POPE hallucination benchmark with 2.2B activated parameters.\nPaper : https://arxiv.org/pdf/2401.15947.pdf\n \n@article{lin2023video,\n  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},\n  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},\n  journal={arXiv preprint arXiv:2311.10122},\n  year={2023}\n}"
  },
  {
    "objectID": "posts/DeepSpeed-FastGen/DeepSpeed-FastGen.html",
    "href": "posts/DeepSpeed-FastGen/DeepSpeed-FastGen.html",
    "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference",
    "section": "",
    "text": "Recently Microsoft DeepSpeed launched DeepSpeed-FastGen LLM serving framework, which offers up to 2.3x higher effective throughput compared to state-of-the-art systems like vLLM. DeepSpeed-FastGen leverages the combination of DeepSpeed-MII and DeepSpeed-Inference to provide an easy-to-use serving system.\nDeepSpeed-FastGen is built to leverage continuous batching and non-contiguous KV caches to enable increased occupancy and higher responsivity for serving LLMs in the data center, similar to existing frameworks such as TRT-LLM, TGI, and vLLM. In order to achieve a new level of performance, DeepSpeed-FastGen introduces SplitFuse which leverages dynamic prompt and generation decomposition and unification to further improve continuous batching and system throughput.\nDuring experiment, DeepSpeed-FastGen outperforms vLLM in both throughput and latency. On Llama-2 70B with 4 A100x80GB, DeepSpeed-FastGen demonstrates up to 2x higher throughput (1.36 rps vs. 0.67 rps) at identical latency (9 seconds) or up to 50% latency reduction (7 seconds vs. 14 seconds) while achieving the same throughput (1.2 rps).\nSupported models : LLaMA and LLaMA-2, Mistral, OPT, Falcon, Mixtral, Phi-2, Qwen\nPaper : https://arxiv.org/pdf/2401.08671.pdf"
  },
  {
    "objectID": "posts/Self-Evaluation Improves Selective Generation in Large Language Models/Self-Evaluation Improves Selective Generation in Large Language Models.html",
    "href": "posts/Self-Evaluation Improves Selective Generation in Large Language Models/Self-Evaluation Improves Selective Generation in Large Language Models.html",
    "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
    "section": "",
    "text": "Trustworthiness of LLMs output is one of the important considerations for safe deployment of LLMs in production.Once of the straightforward way to do so is by measuring quality of selected outputs of LLMs. Reinforcement Learning from Human Feedback (RLHF) is one of the widely used method for better quality-calibrated models.\nSince human feedback data is expensive to obtain, the paper has explored the use of token-level self-evaluation to improve the accuracy and quality of generated content by large language models. Experimental results show that self-evaluation based scores are effective in selective generation. and thereby improving the self-evaluation ability of LLMs to improve quality-calibration.\nThe paper proposes methods to convert open-ended generation into token-level evaluation tasks that the LLM can self-evaluate, such as multi-choice question answering or true/false evaluation. Two main methods are proposed: Sample & Select (multi-choice) and Sample & Eval (true/false).\nExperiments on TRUTHFULQA and TL;DR datasets show the self-evaluation scores significantly improve calibration for selective generation compared to sequence likelihood scores. The hybrid method with a “none of the above ’’ option performs the best overall on accuracy, calibration AUC, and selective AUC metrics. Self-evaluation provides a way to improve calibration of LLMs for selective text generation, without needing extra training data.\nOne of the pitfalls I can see in this paper is that all experiments are carried out with PALM-2 and GPT-3 rather than GPT-3.5 or GPT-4 models as OpenAI API does not provide output log-probabilities for them. It will be interesting to see how Self-Evaluation holds up against GPT-3.5 + models.\nPaper : https://arxiv.org/pdf/2312.09300.pdf"
  },
  {
    "objectID": "posts/Self-RAG/Self-RAG.html",
    "href": "posts/Self-RAG/Self-RAG.html",
    "title": "Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections",
    "section": "",
    "text": "Self-RAG is a new framework to train an arbitrary LM to learn to retrieve, generate, and critique to enhance the factuality and quality of generations, without hurting the versatility of LLMs. It outperformed ChatGPT and retrieval-augmented LLama2 Chat on six tasks.\nUnlike a widely-adopted Retrieval-Augmented Generation (RAG; Figure left) approach, Self-RAG retrieves on demand (e.g., can retrieve multiple times or completely skip retrieval) given diverse queries, and criticize its own generation from multiple fine-grained aspects by predicting reflection tokens as an integral part of generation. It conducts a segment-wise beam search to select the output that maximizes the utility for diverse preferences.\nEagerly waiting for Self-RAG SciPhi-Self-RAG-Mistral-7B-32k on top of Mistral-7B.\nPaper : https://arxiv.org/pdf/2310.11511.pdf\nModel : https://huggingface.co/selfrag/selfrag_llama2_7b"
  },
  {
    "objectID": "posts/Reciprocal Rank Fusion /Reciprocal Rank Fusion.html",
    "href": "posts/Reciprocal Rank Fusion /Reciprocal Rank Fusion.html",
    "title": "Reciprocal Rank Fusion (RRF) with LambdaMART: Context Tuning for Retrieval Augmented Generation (RAG)",
    "section": "",
    "text": "RAG typically consists of three primary components: Tool Retrieval, Plan Generation, and Execution. Existing RAG methodologies rely heavily on semantic search for tool retrieval, but this approach has limitations, especially when queries lack specificity or context. Context Tuning, can be looked at as a viable solution, a component in RAG that precedes tool retrieval, to provide contextual understanding and context seeking abilities to improve tool retrieval and plan generation.\nPaper proposes a new lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART. Results indicate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accuracy. The lightweight model outperforms other methods and helps reduce hallucinations during planning. However, limitations include the absence of conversation history for multi-turn tasks, constraints on planner context window size affecting performance, and the use of synthetic personas instead of real-world data due to privacy concerns.\nIn summary, Context Tuning enhances RAG showcasing improvements in retrieval, planning accuracy, and hallucination reduction compared to baseline methods.\nPaper : https://arxiv.org/pdf/2312.05708.pdf"
  },
  {
    "objectID": "posts/Infinite-LLM/Infinite-LLM.html",
    "href": "posts/Infinite-LLM/Infinite-LLM.html",
    "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
    "section": "",
    "text": "Introducing DistAttention, a distributed attention algorithm, and DistKV-LLM, a distributed LLM serving system, to improve the performance and resource management of cloud-based LLM services. The system achieved significant throughput improvements and supported longer context lengths compared to existing systems\nTraditionally, serving LLMs with long context lengths poses challenges due to the dynamic and growing memory requirements of the attention layer’s key-value (KV) cache. This makes efficient resource management difficult. Introducing DistAttention, a novel distributed attention algorithm that partitions the KV cache into smaller blocks (“rBlocks”) to enable distributed processing and storage. The paper also introduces DistKV-LLM, a distributed LLM serving engine that coordinates memory usage across GPUs and CPUs in a data center. It manages the distributed KV cache through two components the rManager and gManager.\nThe rManager virtualizes memory for each instance and handles local and remote memory requests. The gManager maintains a global view of memory usage and facilitates allocation between instances. Techniques like overlapping communication and computation, a memory optimization algorithm (DGFM), and a coordination protocol are proposed to improve performance.\nEvaluation on a 32 GPU cluster shows the system supports context lengths 2-19x longer than prior work, with 1.03-2.4x higher throughput. It achieves efficient resource utilization for long-context LLM serving in distributed environments. In summary, the key novelty lies in DistAttention’s distributed approach to processing the attention layer, and DistKV-LLM’s coordinated management of the distributed KV cache memory across\nPaper : https://arxiv.org/pdf/2401.02669.pdf"
  },
  {
    "objectID": "posts/CoT/CoT.html",
    "href": "posts/CoT/CoT.html",
    "title": "Chain of Thought (CoT): The Impact of Reasoning Step Length on Large Language Models",
    "section": "",
    "text": "If you are doing prompt engineering for LLMs then you might have come across Chain of Thought (CoT) prompting, which is significant in improving the reasoning abilities of LLMS. However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown.\nLength of reasoning steps/chains in prompting impacts the performance of large language models (LLMs) on tasks requiring reasoning abilities. Experiments show that increasing the number of reasoning steps in prompts, even without adding new information, significantly improves LLM performance across multiple datasets. Shortening steps diminishes performance. Surprisingly, incorrect rationales can still yield good results if they maintain sufficient step length, suggesting step length is more important than factual accuracy. The benefits of longer steps scale with task complexity: simpler tasks require fewer steps while complex tasks gain more from longer chains. Zero-shot prompting can also be improved by lengthening initial prompts to encourage more reasoning (e.g. “Think step-by-step, think more steps”). Compressing step lengths undermines few-shot CoT (chain-of-thought) performance, regressing it to zero-shot levels. Bigger LLMs require fewer steps to reach peak performance compared to smaller models, showing a relationship between model size and optimal step count. Altering questions within prompts has minimal impact, suggesting step length rather than question details primarily drives reasoning.\nPaper : https://arxiv.org/pdf/2401.04925.pdf"
  },
  {
    "objectID": "posts/Improving Text Embeddings with Large Language Models/Improving Text Embeddings with Large Language Models.html",
    "href": "posts/Improving Text Embeddings with Large Language Models/Improving Text Embeddings with Large Language Models.html",
    "title": "Improving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM",
    "section": "",
    "text": "Check out a groundbreaking paper on improving text embeddings with large language models (LLMs) like GPT-4! The authors propose generating synthetic training data for text embedding tasks using LLMs, instead of relying on human-labeled datasets.\nTheir two-step prompt method generates diverse synthetic data for hundreds of thousands of embedding tasks across 93 languages, covering semantic textual similarity, bitext retrieval, and more. The Mistral-7B LLM is fine-tuned on the synthetic data and achieves state-of-the-art results on the MTEB benchmark, outperforming previous models by 2.4 points on average across task categories.\nIn summary, this paper presents an effective and efficient method for improving text embeddings by leveraging data generation with LLMs.\nPaper : https://arxiv.org/pdf/2401.00368.pdf"
  },
  {
    "objectID": "posts/DocLLM/DocLLM.html",
    "href": "posts/DocLLM/DocLLM.html",
    "title": "DOCLLM: A Layout Aware Generative Language Models for Multi model document understanding",
    "section": "",
    "text": "Introducing DocLLM, a groundbreaking generative language model that can understand visually rich documents without the need for expensive image encoders. DocLLM uses a disentangled attention mechanism that captures the interdependencies between text and layout, making it possible to handle irregular layouts and heterogeneous content in visual documents.\nDocLLM’s pre-training objective focuses on infilling missing text segments, and the pre-trained model is fine-tuned using instructions from various datasets, including visual question answering, natural language inference, key information extraction, and document classification.\nEvaluated against comparable models, DocLLM outperforms on 14 out of 16 datasets and generalizes to 4 out of 5 unseen datasets. Its awareness of multi-page documents and page breaks enhances its ability to understand long documents.\nDocLLM can enable the use of more types of data for pre-training language models, allowing documents with complex layouts to be used without much preprocessing. Its cohesive text blocks for pre-training enable meaningful infilling.\nPaper : https://arxiv.org/pdf/2401.00908.pdf"
  },
  {
    "objectID": "posts/Soaring from 4K to 400K/Soaring from 4K to 400K.html",
    "href": "posts/Soaring from 4K to 400K/Soaring from 4K to 400K.html",
    "title": "Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon",
    "section": "",
    "text": "Activation Beacon is a plug-and-play module for large language models that allows them to process longer contexts with a limited context window, while preserving their original capabilities. It achieves competitive memory and time efficiency and can be trained efficiently with short-sequence data. When dealing with long-sequence data, it resorts to sliding windows for streaming processing, which leads to a superior working efficiency at both inference and training time. With the diversely sampled condensing ratios, it can be effectively learned to support the extensions for a wide scope of context lengths based on short-sequence training data. The experimental study verifies Activation Beacon as an effective, efficient, compatible, low-cost (training) method to extend the context length of LLM.\nPaper : https://arxiv.org/pdf/2401.03462.pdf"
  },
  {
    "objectID": "posts/Comprehensive Survey of Hallucination Mitigation Techniques /Comprehensive Survey of Hallucination Mitigation Techniques .html",
    "href": "posts/Comprehensive Survey of Hallucination Mitigation Techniques /Comprehensive Survey of Hallucination Mitigation Techniques .html",
    "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
    "section": "",
    "text": "The paper provides a comprehensive taxonomy categorizing over 32 techniques for mitigating hallucinations in large language models (LLMs). It groups the techniques into categories such as prompt engineering, self-refinement through feedback and reasoning, prompt tuning, and model development. Key mitigation techniques highlighted include:\n\nRetrieval Augmented Generation (RAG) which enhances LLM responses by retrieving information from authoritative external knowledge bases. This helps ground the responses in facts.\nMethods leveraging iterative feedback loops and self-contradiction detection to refine LLM outputs. For example, the Self-Reflection Methodology employs knowledge acquisition and answer generation over multiple cycles.\nPrompt tuning techniques like UPRISE which tune lightweight retrievers to automatically provide task-specific prompts that reduce hallucinations.\nNovel model decoding strategies such as Context-Aware Decoding that override an LLM’s biases by amplifying differences between outputs with and without context.\nUtilizing knowledge graphs and adding faithfulness based loss function\nSupervised Fine-tuning\n\nPaper : https://arxiv.org/pdf/2401.01313.pdf"
  },
  {
    "objectID": "posts/KwaiAgents/KwaiAgents.html",
    "href": "posts/KwaiAgents/KwaiAgents.html",
    "title": "KwaiAgents: Generalized Information-seeking Agent System with LLMs - 2 Open-source models fine tuned for agent systems! Better than GPT-3.5 turbo as an agent!",
    "section": "",
    "text": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user’s query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system’s performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\nPaper : https://arxiv.org/pdf/2312.04889v1.pdf"
  },
  {
    "objectID": "posts/SPIN/SPIN.html",
    "href": "posts/SPIN/SPIN.html",
    "title": "Self-Play Fine-Tuning (SPIN): Converts Weak Language Models to Strong Language Models",
    "section": "",
    "text": "Self-Play Fine-Tuning (SPIN) is a new fine-tuning method to improve large language models (LLMs) without needing additional human-annotated data.\nThe key idea is to use a self-play mechanism where the LLM plays against itself. Specifically, the LLM from the previous iteration generates synthetic responses. The new LLM being trained tries to discern between the synthetic responses and real human responses. This iterates, with the new LLM becoming the synthetic data generator for the next iteration.\nThe method is shown to significantly enhance LLMs’ performance on a variety of benchmarks: - On the HuggingFace Open LLM Leaderboard, SPIN improves the average score from 58.14 to 63.16 with over 10% gains on some datasets. - On the MT-Bench benchmark, the score improves from 5.94 to 6.78. - The gains match or exceed models trained with additional human preference data.\nTheoretically, it is proven that SPIN converges when the LLM distribution aligns perfectly with the real data distribution.\nOverall, the self-play approach enables iterative improvement of LLMs without needing extra human feedback or data, converting weak models to strong models by unleashing the full potential of existing labeled data.\nPaper : https://arxiv.org/pdf/2401.01335.pdf"
  },
  {
    "objectID": "posts/LLM Maybe LongLM/LLM Maybe LongLM.html",
    "href": "posts/LLM Maybe LongLM/LLM Maybe LongLM.html",
    "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
    "section": "",
    "text": "With only four lines of code modification, the proposed method can effortlessly extend existing LLMs’ context window without any fine-tuning. This work elicits LLMs’ inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.\nIn this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs’ context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs’ long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model’s self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs’ context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs’ context window’s length.\nPaper : https://arxiv.org/pdf/2401.01325.pdf"
  },
  {
    "objectID": "posts/Mamba-Chat/Mamba-Chat.html",
    "href": "posts/Mamba-Chat/Mamba-Chat.html",
    "title": "Mamba-Chat: A Chat LLM based on State Space Models",
    "section": "",
    "text": "Mamba-Chat is the first chat language model based on a state-space model architecture, not a transformer.\nThe model is based on Albert Gu’s and Tri Dao’s work Mamba: Linear-Time Sequence Modeling with Selective State Spaces (paper) as well as their model implementation. This repository provides training / fine-tuning code for the model based on some modifications of the Huggingface Trainer class. Mamba-Chat is based on Mamba-2.8B and was fine-tuned on 16,000 samples of the HuggingFaceH4/ultrachat_200k dataset.\nMamba: Linear-Time Sequence Modeling with Selective State Spaces\nPaper : https://arxiv.org/pdf/2312.00752.pdf"
  },
  {
    "objectID": "posts/BLIVA/BLIVA.html",
    "href": "posts/BLIVA/BLIVA.html",
    "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
    "section": "",
    "text": "Vision Language Models (VLMs), such as OpenAI’s GPT-4, Flamingo, BLIP-2 and LLaVA have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios.\nStandard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context.\nTo improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process.\nBLIVA uses a Q-Former to draw out instruction-aware visual features from the patch embeddings generated by a frozen image encoder. These learned query embeddings are then fed as soft prompt inputs into the frozen Language-Learning Model (LLM). Additionally, the system repurposes the originally encoded patch embeddings through a fully connected projection layer, serving as a supplementary source of visual information for the frozen LLM.\nDuring experiment, BLIVA significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence.\nPaper : https://arxiv.org/pdf/2308.09936.pdf"
  },
  {
    "objectID": "posts/FIND/FIND.html",
    "href": "posts/FIND/FIND.html",
    "title": "FIND: INterface for Foundation models’ embeDDings",
    "section": "",
    "text": "Foundation models across the vision and language domains, such as GPT4, DALLE-3, SAM and LLaMA etc., have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) .\nHowever, the process of training individual foundation models has become remarkably costly. Furthermore, the full potential of these models remains untapped due to limitations in their fixed output modalities (i.e. text output for Q&A and visual output for image generation). Although techniques such as prompt engineering and adaptive tuning have shown promising results, these approaches struggle with integrating different foundation models off the shelf, expanding the output types and task objectives.\nPaper proposes FIND - a generalized interface for aligning foundation models’ embeddings. The interface enables task-adaptive prototyping, which means we only need to change the configure file instead of the model architecture when adapting to the new tasks. Because all the vision-language tasks are trained in a unified way, this creates an interleaved shared embedding space where vision and language references are replaceable and addable. The proposed interface has the following favorable attributes: 1. Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights. 2. Prototypable. Different tasks are able to be implemented through prototyping attention masks and embedding types. 3. Extendable. The proposed interface is adaptive to new tasks, and new models. 4. Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space.\nFurthermore, FIND has achieved SoTA performance on interleaved image retrieval and segmentation and shows better or comparable performance on generic/interactive/grounded segmentation and image-text retrieval.\nPaper : https://arxiv.org/pdf/2312.07532.pdf"
  },
  {
    "objectID": "posts/Re3val/Re3val.html",
    "href": "posts/Re3val/Re3val.html",
    "title": "Re3val: Reinforced and Reranked Generative Retrieval",
    "section": "",
    "text": "The primary objective of retrieval models is to enhance the accuracy of answers by selecting the most relevant documents retrieved for a given query, ensuring models have sufficient information to help the downstream reasoning process. However, there are two major limitations: First, the generative retrieval does not account for contextual information. Secondly, the retrieval can’t be tuned for the downstream readers as decoding the page title is a non-differentiable operation.\nPaper introduces Re3val - Reinforced and Reranked Generative Retrieval, a novel framework specifically designed to address the challenges in neural information retrieval. Re3val uses Dense Passage Retrieval (DPR) contexts for reranking retrieved page titles, leading to improved RPrecision. Re3val enhances performance by integrating generated questions in pre-training and utilizing REINFORCE during distant supervision. Moreover, Re3val achieves more accurate answers by reading reranked contexts retrieved with the reranked page titles. These advancements enable Re3val to achieve state-of-the-art performance while also offering cost savings by reducing training time and minimizing the need for extensive data labeling.\nTypical Re3val Training Pipeline consists of the following. Generated questions after filtering are integrated into pre-training (1), followed by few-shot training (3) with REINFORCE (2, 4). Retrieved DPR contexts (5), perturbed page titles (6), and queries are concatenated for reranker training (7). Gold and negative passages retrieved with BM-25 are employed (8) for context reranker training (9). Contexts are retrieved using the top 5 reranked titles from KILT (10), where missing titles are imputed with BM-25 (11). DPR contexts are imputed (12) if lacking five gold contexts during FiD model pre-training (13). FiD model is fine-tuned using five reranked contexts (14).\nDuring inference Reranker concatenates retrieved DPR contexts (1), page titles (2), and query to rerank page titles (3). Contexts retrieved with the top five reranked page titles (4), including BM-25 imputed titles (5), are reranked (6). The top-5 reranked contexts are used to generate an answer (7).\nExperimental results demonstrate Re3val’s superiority over the CorpusBrain zero-shot baseline, with an average 8% R-Precision improvement across five tasks using reduced pretraining data. Re3val also achieves an average 1.9% R-Precision increase compared to other generative models via page title reranking with limited taskspecific data. Moreover, by employing a context reranker before grounding, Re3val achieves top-1 KILT scores among generative retrieval models, showing an average 2.1% improvement across five datasets.\nPaper : https://arxiv.org/pdf/2401.16979.pdf"
  },
  {
    "objectID": "posts/Repeat After Me/Repeat After Me.html",
    "href": "posts/Repeat After Me/Repeat After Me.html",
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "section": "",
    "text": "Transformers are the workhorse of modern sequence modeling, achieving remarkable performance on a variety of tasks, but they have unavoidable inefficiencies. Specifically, when it comes to memory and compute requirements to predict the next token of a sequence of length. Recently, there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as “generalized state space models” (GSSMs).\nGSSMs have demonstrated impressive performance, but it is not yet clear what these models sacrifice for their improved efficiency, if anything. Well that’s what exactly this paper has found out. It seems that GSSMs are promising in terms of inference-time efficiency but are limited compared to transformer models on tasks that require copying from the input context.\nTo understand this gap in capabilities experimentation was carried out to copy strings of length that are exponential in the number of heads of the transformer under the following scenarios.\n\nCopying: training efficiency. Here the train models copy strings of length ≤ 300 and evaluate string-level accuracy on strings of length 300. Transformers train much faster than GSSMs.\nCopying: length generalization. Here train models copy strings of length ≤ 50 until all models are perfect in-distribution and evaluate string-level accuracy. Evaluating on longer inputs, the transformer models dramatically outperform the GSSMs.\nLookup with pretrained models. Here the task requires looking up and retrieving a number from a “phone book” of varying length that is entirely in context. Pythia (a transformer model) substantially outperforms Mamba (a GSSM) across model sizes.\nCopy: natural language strings. It compares pretrained models on their ability to copy natural language strings sampled from C4 of varying lengths and report string-level accuracy. The transformer models substantially outperform the GSSMs.\nCopy: shuffled strings. To test whether it mattered that the strings were in natural language, randomly shuffle the word order of the strings from the previous experiment. it was found that this degrades performance, especially for the Mamba models.\n\nOverall transformers are better than GSSMs at copying from their input context. However, SSMs have many advantages over transformers when it comes to memory and computational complexity as well as generating long consistent text. Future work should focus on building hybrid architectures that endow state space models with an attention-like mechanism, allowing them to retrieve relevant pieces of text from their input. What do you think ?\nSo why size of input context is so much important in LLMs. In order to understand this let’s look at GPU level. Modern GPUs have a “problem”: they’re too fast. GPUs have become very fast at performing calculations, insomuch that the speed of computation (FLOPs) is much higher than the memory bandwidth (GB/s) or speed of data transfer between memory areas. For example, an NVIDIA A100 can perform 19.5 TFLOPs while having a memory bandwidth of 2TB/s, which is 40 times slower considering each operation is 32 bit.\nThis means that sometimes the bottleneck is not how many operations we perform, but how much data transfer our operations need, and that depends on the size and the quantity of the tensors involved in our calculations. For example, computing the same operation on the same tensor N time may be faster than computing the same operation on N different tensors, even if they have the same size, this is because the GPU may need to move the tensors around. That what happens during memory-intensive tasks such as copying long strings, retrieval and few-shot question answering.\nSo the goal should not only be to optimize the number of operations we do, but also minimize the memory access/transfers that we perform."
  },
  {
    "objectID": "posts/BlackMamba/BlackMamba.html",
    "href": "posts/BlackMamba/BlackMamba.html",
    "title": "BlackMamba: Mixture of Experts for State-Space Models",
    "section": "",
    "text": "State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint.\nSo why not combine Mamba with MoE. Well that is what this paper has explored with BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. This Mamba-MoE architecture have the following improvements over a dense transformer:\n\nMamba: Linear computational complexity with respect to input sequence length for both training and inference. Autoregressive generation in constant time and memory.\nMoE: Inference latency and training FLOPs of the equivalent smaller dense base model, while preserving model quality close to an equi-parameter dense model.\n\nBlackMamba architecture simply replaces both the MLP layer in a transformer with an expert layer, and the attention layer with a mamba SSM layer. Further, it used the SwiGLU activation function for the expert MLPs. For the expert router, it used top-1 routing with a Sinkhorn routing function to load-balance between experts. It utilized a novel custom version of the Sinkhorn algorithm which converges substantially faster than vanilla Sinkhorn. Model was trained using Megatron-LM distributed training framework and was trained in bf16 precision on 300B tokens on custom dataset both 340M/1.5B and 630M/2.8B models .\n\n\nBlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. BlackMamba inherits and combines both of the benefits of SSM and MoE architectures, combining linear-complexity generation from SSM with cheap and fast inference from MoE. Moreover, BlackMamba shows that it is capable of rapid generation with both linear time and memory cost.\nPaper: https://arxiv.org/pdf/2402.01771.pdf"
  },
  {
    "objectID": "posts/MambaFormer/MambaFormer.html",
    "href": "posts/MambaFormer/MambaFormer.html",
    "title": "MambaFormer: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks",
    "section": "",
    "text": "State-space models (SSMs), such as Mamba, have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, against Transformer models in standard regression in-context learning (ICL) tasks, it falls short in more complex ICL tasks like Vector-valued MQAR. To address these limitations, the paper has introduced a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.\nBefore jumping to MambaFormer lets us have a deepdive at in-context learning (ICL). In-context learning (ICL) has emerged as one of the most remarkable capabilities of LLMs like GPT-3 and GPT-4. With just a few demonstration examples (few-short learning), these models can rapidly adapt to new tasks and make accurate predictions without any parameter updates. Numerous research studies have been dedicated to understanding the mechanics of Attention in transformer models that enable such meta-in-context-learning capabilities, either through constructive arguments or extensive experimental investigation. Transformer language models are currently the only large models that have been reported to be capable of ICL in practice.\nSo Can attention-free models perform ICL?\nWell that’s what this paper tries to address. Series of experiment against Transformer, Mumba and MumbaFormer LLMs was carried out for various ICL task such as Linear regression, Sparse linear regression, 2NN regression, Decision Tree, Orthogonal-outlier regression, Many-outlier regression , Sparse parity, Chain-of-Thought I/O and Vector-valued MQAR. MumbaFormer has been the winner in all these tasks. So what is this MambaFormer ?\nMambaFormer is a hybrid architecture that replaces MLP blocks within the transformer with Mamba blocks. Importantly, the architecture also starts with a Mamba block and does not use positional encoding. During ICL evaluations, it was found that MambaFormer consistently achieves a best-of-both-worlds performance compared to Transformer and Mamba.\nPaper: https://arxiv.org/pdf/2402.04248.pdf"
  },
  {
    "objectID": "posts/Hydragen/Hydragen.html",
    "href": "posts/Hydragen/Hydragen.html",
    "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
    "section": "",
    "text": "Transformer-based large language models (LLMs) such as OpenAI GPT3.5 and GPT4 are now deployed to hundreds of millions of users. LLM inference in such scenarios commonly performed on batches of sequences that share a prefix (the system prompt), such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. Even with FlashAttention and PagedAttention models redundantly read the prefix’s keys and values from GPU memory when computing attention, regardless of whether the prefix is redundantly stored.\nIn order to eliminate these redundant reads paper introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications.\nLet’s take an example of LLM chat model inference, which processes many sequences that share a large shared prefix (the system prompt). With Hydragen overall attention is decomposed into attention over the shared prefix (batched across all queries in a batch) and attention over the remaining suffixes (independent across sequences, as is normally done). Hydragen’s attention decomposition allows many matrix vector products to be replaced with fewer matrix-matrix products. Using matrix-matrix products is particularly important as GPUs dedicate an increasingly large ratio of their total FLOPs to tensor cores that are specialized in matrix multiplication\nIn end-to-end benchmarks, Hydragen increases the throughput of CodeLlama-13b by up to 32x over vLLM, a high-performance inference framework that avoids redundant prefix storage but not redundant prefix reads. Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.\nPaper: https://arxiv.org/pdf/2402.05099.pdf"
  },
  {
    "objectID": "posts/Tag-LLM/Tag-LLM.html",
    "href": "posts/Tag-LLM/Tag-LLM.html",
    "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
    "section": "",
    "text": "General-purpose LLMs like LLaMA and GPT-4 have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains such as processing amino acid sequences for proteins (e.g., MTVPDRSEIAG) or SMILES strings for chemical compounds, hampering their adoption for a wide range of scientific problems. Further, Training domain specific LLM requires a significant amount of compute and in-domain data. Fine-tune existing LLMs or perform in-context learning might be another way to go around, but the former is prone to catastrophic forgetting and can hurt the model’s reasoning abilities\nSo can we effectively repurpose general-purpose LLMs for specialized tasks without compromising their linguistic and reasoning capabilities?\nTo address this issue, the paper has introduced Tag-LLM. A novel model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM’s embedding layer to condition the LLM. Two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compressing function-solving instructions. Further, a three-stage protocol is also been introduce to learn these tags using auxiliary data and domain knowledge.\nLet’s consider the task of protein-drug binding affinity prediction. In Tag-LLM method domain tags ⟨Protein⟩, ⟨SMILES⟩ and a function tag ⟨Binding Affinity⟩ is been injects to the input, which are mapped to specially trained embeddings. The model’s last hidden state is passed to a task-specific head to generate predictions of the desired type (e.g., a scalar binding affinity value in this case).\nWhile experimenting with the LLaMA-7B model, on a diverse set of ten domains, encompassing eight languages and two specialized scientific domains (protein sequences and SMILES molecule representations). Results demonstrate that the domain tags can act as effective context switchers, and a function tag can be applied to multiple domains to solve different tasks, achieving zero-shot generalization to unseen problems.\nPaper: https://arxiv.org/pdf/2402.05140.pdf\nGithub: https://github.com/sjunhongshen/Tag-LLM"
  },
  {
    "objectID": "posts/PHATGOOSE/PHATGOOSE.html",
    "href": "posts/PHATGOOSE/PHATGOOSE.html",
    "title": "PHATGOOSE: Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "section": "",
    "text": "The availability of Huggingface PEFT modules has made it cheap and easy to modularly adapt a given pre-trained model to a specific task or domain. In the meantime, extremely large-scale language models (LLMs) are now being treated as “general-purpose” and often exhibit strong zero-shot generalization. Relying on zero-shot generalization stands in stark contrast to the aforementioned approach of training specialized models for each task such as PEFT.\nSo can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model ?\nThat’s what this paper has tried to address with PHATGOOSE : Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts, a post-hoc method that enables zero-shot generalization among specialized models. PHATGOOSE recycles PEFT modules by introducing an additional computationally inexpensive step after training the PEFT-based model itself. Specifically, the entire model (including the newly introduced PEFT modules) is frozen and a per-module gate is trained. This gate (whose parameters are shared across sequence positions) comprises a linear layer followed by a sigmoid nonlinearity that determines whether the activation at a given sequence position should be fed into the module or not. Training this gate only requires a small amount of additional compute compared to performing PEFT. The gates for every module across specialized models are then combined to determine how to route different tokens to different modules during inference using a standard “top-k” routing strategy.\nTo test the effectiveness of PHATGOOSE, T5 family models were used to improve zero-shot generalization on standard benchmarks. Notably, it was found that PHATGOOSE not only outperforms prior methods involving merging experts or retrieving a single expert but can also outperform explicit multitask training in some cases. In qualitative analysis, it was found that PHATGOOSE uses a diverse set of modules to perform a given task, thereby combining abilities from multiple specialized models and, in some cases, producing better performance than the single best-performing expert model. Overall, this work sets the groundwork for a promising new framework for the decentralized development of generalist AI systems.\nPaper : https://arxiv.org/pdf/2402.05859.pdf\nGithub : https://github.com/r-three/phatgoose"
  },
  {
    "objectID": "posts/Fiddler/Fiddler.html",
    "href": "posts/Fiddler/Fiddler.html",
    "title": "Fiddler: CPU-GPU Orchestration for Fast Local Inference of MoE Models",
    "section": "",
    "text": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architectures are showing remarkable performance on various tasks. By activating a subset of experts inside feed-forward layers with a gating mechanism, such models scale up model size and improve model performance with a small computation overhead. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes.\nTo address this paper proposes Fiddler, a fast inference system for LLMs based on Mixture-of-Experts (MoE) architecture at local devices. It allows you to run an unquantized Mixtral-8x7B model (&gt;90GB of parameters) with &gt;3 token/s on a single 24GB GPU. The key idea behind Fiddler is to use the CPU’s computation power. Existing offloading systems primarily utilize the memory resources available on the CPU, while the computation mainly occurs on the GPU. The typical process involves: (1) When some expert weights are missing from the GPU memory, (2) they are copied from the CPU memory to the GPU memory, then (3) GPU executes the expert layer. Although GPU execution is faster, the data movement introduces significant overhead.\nOn the other hand, Fiddler uses CPU computation resources in addition to memory resources. The process is as follows: (1) when some expert weights are missing on the GPU memory, (2) it copies the activation values from the GPU memory to the CPU memory, instead of copying the weights. (3) The computation of the expert layer then happens on the CPU, and (4) the output activation after the expert is copied back to the GPU.\nThis approach significantly reduces the latency of CPU-GPU communication, especially since the size of activations is considerably smaller than the weight size (batch_size x 4096 versus 3 x 4096 x 14336 per expert for the Mixtral-8x7B) for a small batch size. Despite slower computation speeds on the CPU compared to the GPU, avoiding the weight copying process makes this approach more efficient.\nCompared with DeepSpeed-MII and Mixtral offloading, Fiddler is on average faster by 19.4 and 8.2 times for Environment 1, and by 22.5 and 10.1 times for Environment 2.\nPaper : https://arxiv.org/pdf/2402.07033.pdf\nGithub : https://github.com/efeslab/fiddler"
  },
  {
    "objectID": "posts/GraphMamba/Graph Mamba.html",
    "href": "posts/GraphMamba/Graph Mamba.html",
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "section": "",
    "text": "Graph Transformers (GTs) has shown promising potential in graph representation learning. GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). Recently, Mamba’s outstanding performance in language modeling, outperforming Transformers of the same size and matching Transformers twice its size, motivates several recent studies to adapt its architecture for different data modalities. Mamba architecture is specifically designed for sequence data and the complex non-causal nature of graphs makes directly applying Mamba on graphs challenging.\nTo address all the above mentioned limitations, the paper presents Graph Mamba Networks (GMNs), a new class of machine learning on graphs based on state space models. Recipe for Graph Mamba Networks is simple : (1) Tokenization: the graph is mapped into a sequence of tokens (m ≥ 1: subgraph and m = 0: node tokenization) (2) (Optional Step) PE/SE: inductive bias is added to the architecture using information about the position of nodes and the structure of the graph. (3) Local Encoding: local structures around each node are encoded using a subgraph vectorization mechanism. (4) Token Ordering: the sequence of tokens are ordered based on the context. (Subgraph tokenization (m ≥ 1) has implicit order and does not need this step). (5) (Stack of) Bidirectional Mamba: it scans and selects relevant nodes or subgraphs to flow into the hidden states.\nExperimental evaluations demonstrate that GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets, while consuming less GPU memory. These results show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary.\nPaper : https://arxiv.org/pdf/2402.08678.pdf\nCodes and models will be available soon (Feb 20)."
  },
  {
    "objectID": "posts/Aespa/Aespa.html",
    "href": "posts/Aespa/Aespa.html",
    "title": "Aespa: Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers",
    "section": "",
    "text": "With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers.\nTo address this paper propose a novel post-training quantization algorithm, called attention-centric efficient and scalable post-training quantization algorithm (Aespa), that pursues both accuracy and efficiency. The key idea of Aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency by refining quantization loss functions to preserve the attention score. To accelerate the quantization process, a refined quantization objectives for the attention module is been propose. Through a complexity analysis, it was demonstrate that it is about 10 times faster quantization than existing block-wise approaches can be achieved by exploiting the proposed objectives.\nExperimentation on various LLMs including OPT, BLOOM, and LLaMA demonstrates that Aespa approach outperforms conventional quantization schemes such as RTN, QPTQ and Z-Fold by a significant margin, particularly for low-bit precision (INT2).\nPaper : https://arxiv.org/pdf/2402.08958.pdf"
  },
  {
    "objectID": "posts/GRIT/GRIT.html",
    "href": "posts/GRIT/GRIT.html",
    "title": "GRIT : Generative Representational Instruction Tuning",
    "section": "",
    "text": "All text-based language problems can be reduced to either generation or embedding. Creating a single general model that performs such a wide range of tasks has been a long-standing goal. Recently, large language models (LLMs) have emerged as a promising direction for a single multi-task model.\nSo can we train an unified LLM which is equally good both in generation and embedding tasks ?\nIntroducing generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. GRIT combines this two previously disjoint training paradigms by: (1) Generative instruction tuning, whereby the model is trained to respond to instructions by generating an answer and (2) Representational instruction tuning, whereby the model is trained to represent a provided input according to an instruction Via the instructions and separate loss functions the model learns to differentiate the two streams.\nCompared to other open models, resulting LLM trained with GRIT - GRITLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GRITLM 8X7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, it was found that GRIT matches training on only generative or embedding data, thus unifying both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, by no longer requiring separate retrieval and generation models.\nPaper: https://arxiv.org/pdf/2402.09906.pdf\nCode : https://github.com/ContextualAI/gritlm"
  },
  {
    "objectID": "posts/FinBen/FinBen.html",
    "href": "posts/FinBen/FinBen.html",
    "title": "The FinBen: An Holistic Financial Benchmark for Large Language Models",
    "section": "",
    "text": "Recent studies have shown the great potential of advanced LLMs such as GPT-4 on financial text analysis and prediction tasks in the financial domain. While their potential is evident, a comprehensive understanding of their capabilities and limitations for finance, remains largely unexplored.\nExisting financial domain evaluation benchmarks including FLUE, BBTCFLEB, and PIXIU, have a limited scope and are solely focused on financial NLP tasks, primarily targeting language understanding abilities where LLMs have already been extensively evaluated. These benchmarks fail to capture other crucial facets of the financial domain, such as comprehending and extracting domain-specific financial knowledge and resolving realistic financial tasks. As such, their efficacy in evaluating and understanding LLM performance is limited.\nTo bridge this gap, the paper proposes FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the CattellHorn-Carroll theory, to evaluate LLMs’ cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more.\nEvaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations within the financial domain. The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities.\nPaper : https://arxiv.org/pdf/2402.12659.pdf\nCode : https://github.com/The-FinAI/PIXIU"
  },
  {
    "objectID": "posts/TinyLLaVA/TinyLLaVA.html",
    "href": "posts/TinyLLaVA/TinyLLaVA.html",
    "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models",
    "section": "",
    "text": "Large language models (LLMs) with large model size can greatly improve task performance but demand expensive computational resources for training. To address this, the LLM communities started releasing smaller-scale models like 7-B and sub-3B versions, maintaining performance parity with larger predecessors like OpenFlamingo and LLaVA series.\nNow more efforts on exploring various ways for efficient training, applying sparse MoE, freezing or lora tuning backbones and deploying in terms of using such tiny LLMs have started and TinyLLaVA is one such effort. TinyLLaVA framework that provides a unified perspective in designing and analyzing small-scale Large Multimodal Models (LMMs). Study shows the effects of different vision encoders, connection modules, language models, training data and training recipes.\nExperiments show that with better training recipes and quality of data, smaller LMMs can achieve on-par performance with larger counterparts, setting new baselines for the research field. Finally it presents a family of small scale LMMs, encompassing three language models: Phi2 , StableLM-2, and TinyLlama, and two vision encoders: CLIP, and SigLIP. Best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. Hope these findings can serve as baselines for future research in terms of data scaling, training setups and model selections.\nPaper : https://arxiv.org/pdf/2402.14289.pdf\nCode : https://github.com/DLCV-BUAA/TinyLLaVABench"
  },
  {
    "objectID": "posts/ChunkAttention/ChunkAttention.html",
    "href": "posts/ChunkAttention/ChunkAttention.html",
    "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition",
    "section": "",
    "text": "Self-attention, one of the critical components in LLM, has a poor performance during inference since it performs intensive memory operations on key/value tensors of context tokens (KV cache) and is memory-bound. Due to which it becomes a significant source of inference latency for long sequences. To negate this it’s now common to use prompt prefixes caching during multi-tenant LLMs serving scenarios. But there are some limitations to this (1) predefined system prompts are static and inflexible in frequent refreshes for large scale deployments (2) there is memory waste in case of long system prompts and low hit rate.\nTo address this paper has introduced ChunkAttention, a prefix aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts.\nExperiments show that ChunkLlama (ChunkAttention on Llama) can achieve comparable throughput with SOTA PagedAttention kernel without shared system prompts and can outperform it by 3.2-4.8× with a shared system prompt of 1024 to 4096 tokens on A100 (80G) by applying prefix-aware KV cache and two-phase partition. ChunkLlama can achieve 1.6× (2.9 against 1.8) and 2.3× (2.3 against 1.0) higher throughput compared to vLLM when 1024 and 2048 prefix tokens are shared while maintaining a normalized latency of less than 40 ms/token. The KV cache memory usage is reduced by 70%-90% with long shared prefixes. The peak batch size is also reduced by 20%-40% since ChunkLlama can decode faster.\nPaper : https://arxiv.org/pdf/2402.15220.pdf"
  },
  {
    "objectID": "posts/MobiLlama/MobiLlama.html",
    "href": "posts/MobiLlama/MobiLlama.html",
    "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
    "section": "",
    "text": "MobiLlama, another Small Language Models (SLMs) for resource constrained devices. MobileLlama is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost. MobileLlama was trained using the Amber data sources.\nMobiLlama, with an aim to develop accurate SLMs by alleviating the redundancy in the transformer blocks. Different from the conventional SLM design where dedicated feed forward layers (FFN) are typically allocated to each transformer block, it employs a shared FFN design for all the transformer blocks within SLM.\nMobiLlama 0.5B and 0.8B models outperform OLMo-1.17B and TinyLlama-1.1B in terms of pre-training tokens, pre-training time and memory, model parameters, overall accuracy across nine benchmarks and on-device efficiency. MobiLlama achieves comparable accuracy while requiring significantly fewer pre-training data (1.2T tokens vs. 3T tokens), lesser pre-training time and GPU memory along with being efficient in terms of deployment on a resource constrained device\nPaper : https://lnkd.in/gR2RzMiF\nCode : https://lnkd.in/gg_-ArZt\nModel : https://lnkd.in/gXegFw-8"
  },
  {
    "objectID": "posts/ChunkLlama/ChunkLlama.html",
    "href": "posts/ChunkLlama/ChunkLlama.html",
    "title": "ChunkLlama : Training-Free Long-Context Scaling of Large Language Models",
    "section": "",
    "text": "The ability to comprehend and process long-context information is essential for large language models (LLMs) to cater to a wide range of applications effectively. Finetuning is one way to improve LLM long-context capability but due to the high cost of continual pretraining on longer sequences, previously released long-context models are typically limited to scales of 7B/13B.\nTo address this, researchers have introduced Dual Chunk Attention (DCA), a new training-free framework to extrapolate the context window of LLMs. Inspired by efficient chunk-based attention patterns, DCA segments self-attention computations for a long sequence into small chunks, each chunk being smaller than the size of the pretraining window. DCA consists of three components: (1) intra-chunk attention, tailored for processing tokens within the same chunk; (2) inter-chunk attention, for processing tokens between distinct chunks; and (3) successive chunk attention, for processing tokens in successive, distinct chunks. These respective treatments help the model effectively capture both long-range and short-range dependencies in a sequence. In addition to that, the chunk-based attention calculation can be seamlessly integrated with Flash Attention 2, a key element for long-context scaling in the open-source community.\nDual chunk attention provides a training-free and effective method for extending the context window of large language models (LLMs) to more than 8x times their original pre-training length. DCA can be seamlessly integrated with (1) popular extrapolation methods such as Positional Interpolation (PI) and NTK-Aware RoPE; and (2) widely-used libraries for memory-efficient inference like FlashAttention and vLLM.\nDCA achieves performance on practical long-context tasks that is comparable to or even better than that of fine tuned models. When compared with proprietary models, ChunkLlama2 training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative\nPaper : https://arxiv.org/pdf/2402.17463.pdf\nCode : https://github.com/HKUNLP/ChunkLlama"
  },
  {
    "objectID": "posts/BitNet/BitNet.html",
    "href": "posts/BitNet/BitNet.html",
    "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
    "section": "",
    "text": "Large Language Models (LLMs) have demonstrated remarkable performance in a wide range of natural language processing tasks, but their increasing size has posed challenges for deployment and overall cost. One approach to address these challenges is to use post-training quantization to create low-bit models for inference moving from 16 bits to lower bits, such as 4-bit variants. However, post-training quantization is sub-optimal, even though it is widely used in industry LLMs.\nRecent work on 1-bit model architectures, such as BitNet, presents a promising direction for reducing the cost of LLMs while maintaining their performance. BitNext advantages include (1) matrix multiplication of BitNet only involves integer addition (2) lower memory footprint thereby reducing the cost and time of loading weights from DRAM, leading to faster and more efficient inference.\nContinuing on this work, researchers developed a new 1-bit LLM variant called BitNet b1.58, where every parameter is ternary, taking on values of {-1, 0, 1}. An additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original 1-bit BitNet. Furthermore, BitNet b1.58 offers two additional advantages. (1) its modeling capability is stronger due to its explicit support for feature filtering, made possible by the inclusion of 0 in the model weights, which can significantly improve the performance of 1-bit LLMs.(2) BitNet b1.58 can match full precision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a 3B size, when using the same configuration (e.g., model size, training tokens, etc.).\nDuring experimental comparison between BitNet b1.58 and LLaMA LLM. It shows that BitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity, while being 2.71 times faster and using 3.55 times less GPU memory. In particular, BitNet b1.58 with a 3.9B model size is 2.4 times faster, consumes 3.32 times less memory, but performs significantly better than LLaMA LLM 3B.\nSo what next from here ? Yes 1-bit Mixture-of-Experts (MoE) LLMs.\nPaper : https://arxiv.org/abs/2402.17764"
  },
  {
    "objectID": "posts/bGPT/bGPT.html",
    "href": "posts/bGPT/bGPT.html",
    "title": "Beyond Language Models: Byte Models are Digital World Simulators",
    "section": "",
    "text": "Bytes are the foundation of all digital data, devices, and software, from computer processors to operating systems in everyday electronics. Therefore, training models for next byte prediction can potentially lead to a paradigm shift in deep learning, allowing them to truly understand and simulate all activities in the digital world. This has practical benefits not only in conventional areas, but also in some underexplored areas such as boosting cybersecurity, improving computer diagnostics, optimizing data compression, and even advancing complex tasks like reverse-engineering the source code of that software from its binary representation.\nPaper has introduced bGPT, a model designed for binary data processing and digital world modeling by next byte prediction. bGPT segments byte sequences into patches, predicts next patch features with a patch-level decoder, and reconstructs bytes within patches using these features with a byte-level decoder. Its advantages are twofold: 1) Interpreting Digital System: By training on byte sequences, bGPT can learn the patterns of digital systems, enabling it to predict, simulate, and diagnose algorithm or hardware behavior. This ability allows for the reconstruction of complex systems from binary data. 2) Unified Modeling: bGPT integrates various data types into a single framework, treating everything as a byte sequence. This simplifies modeling and allows for easy integration of various data sources.\nExperimentation include two main areas: 1) well-studied tasks like generative modeling and classification on digital media data (e.g., text, audio, and images); and 2) relatively underexplored tasks intrinsic to binary-native operations, including data conversion and CPU state modeling, which represent algorithm and hardware simulation, respectively. bGPT models were pre-trained on IrishMAN for data conversion, CPU states for CPU state modeling, Wikipedia for text (achieved a score of 1.0639 BPB and an accuracy of 92.49%), ImageNet for images (achieved a score of 3.12 BPB and an accuracy of 88.69%), and LibriSpeech for audio (achieved a score of 1.48 BPB and an accuracy of 93.63%). All showcased generative samples from bGPT are produced using the same data preprocessing, model architecture, hyperparameters, and training objectives, without any modality-specific customizations.\nPaper : https://arxiv.org/abs/2402.19155\nModel : https://huggingface.co/sander-wood/bgpt/tree/main\nCode : https://github.com/sanderwood/bgpt"
  },
  {
    "objectID": "posts/VisionLLaMA/VisionLLaMA.html",
    "href": "posts/VisionLLaMA/VisionLLaMA.html",
    "title": "VisionLLaMA : A Unified LLaMA Interface for Vision Tasks",
    "section": "",
    "text": "Large language models, especially the LLaMA family of models, aroused great interest in the research community for multimodal models application, where many methods heavily rely on LLaMA for text processing and CLIP-fashioned vision transformers for visual perception.\nCan the same transformer be used to process both text and 2D images?\nThat is what researchers try to address by unveiling a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA. VisionLLaMA follows the pipeline of ViT and retains the architecture design of LLaMA as closely as possible. For an image of H × W, it’s firstly transformed and flattened into N = H×W P 2 non-overlapped patches X ∈ R N×C. Then a class token is prepended at the beginning of the sequence and the whole sequence is processed by L VisionLLaMA blocks. The basic block differs from the standard ViT block by two components: self-attention with positional encoding (RoPE) and SwiGLU activation. Researchers also introduce AS2DRoPE (i.e. auto-scaled 2D RoPE), which expands rotated positional encoding from 1D to 2D and utilizes interpolation scaling to accommodate arbitrary resolutions. For the Pyramid Transformer, VisionLLaMA is applied to windows based transformers such as Twin that utilize additive relative position encoding Swin.\nDuring experimentation VisionLLaMA was trained either in supervised or self-supervised schemes to validate the power in a myriad of downstream vision tasks like image classification, detection, and segmentation. Particularly VisionLLaMA image generation capacity was explored under the diffusion framework DiT and SiT to confirm its potency. VisionLLaMA converges much faster than ViT across all models. SiT-LLaMA with 300k training iterations even outperforms the baseline with 400k steps. Further, VisionLLaMA converges faster than DeiT3-L. In conclusion, VisionLLaMA has strong potential to serve as a new vision backbone to facilitate a large realm of downstream applications.\nPaper : https://arxiv.org/pdf/2403.00522.pdf"
  },
  {
    "objectID": "posts/DiffuseKronA/DiffuseKronA.html",
    "href": "posts/DiffuseKronA/DiffuseKronA.html",
    "title": "DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model",
    "section": "",
    "text": "In recent years, text-to-image (T2I) generation models such as DreamBooth and BLIP-Diffusion have rapidly evolved, generating intricate and highly detailed images that often defy discernment from real-world photographs. Yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis.\nTo address these constraints, researcher has introduce DiffuseKronA, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by up to 35% and 99.947% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis.\nThe main idea of DiffuseKronA is to leverage the Kronecker product to decompose the weight matrices of the attention layers in the UNet model. Kronecker Product is a matrix multiplication method that captures structured relationships and pairwise interactions between elements of two matrices. In contrast to the low-rank decomposition in LoRA, the Kronecker Adapter in DiffuseKronA offers a higher-rank approximation with less parameter count and greater flexibility.\nDuring experimentation performance of DiffuseKronA was compared LoRA-DreamBooth under the following criteria:\nEnhanced Stability: DiffusekronA is more stable compared to LoRA-DreamBooth. Stability refers to variations in images generated across different learning rates and Kronecker factor/ranks, which makes LoRA-DreamBooth harder to fine-tune.\nText Alignment and Fidelity: On average, DiffusekronA captures better subject semantics and large contextual prompts.\nInterpretability: Leverages the advantages of the Kronecker product to capture structured relationships in attention-weight matrices. More controllable decomposition makes DiffusekronA more interpretable.\nAll in all, DiffusekronA outperforms LoRA-DreamBooth in terms of visual quality, text alignment, fidelity, parameter efficiency, and stability.\nPaper : https://arxiv.org/pdf/2402.17412.pdf"
  },
  {
    "objectID": "posts/Design2Code/Design2Code.html",
    "href": "posts/Design2Code/Design2Code.html",
    "title": "Design2Code: How Far Are We From Automating Front-End Engineering?",
    "section": "",
    "text": "Recent releases of advanced multimodal LLMs such as GPT-4V and Gemini version pro have led to breakthroughs in visual and code generation understanding. This has opened up new possibilities in front-end development, where such multimodal large language models (LLMs) have the potential to translate visual designs into code directly, streamlining the front-end engineering process.\nSo can we take a screenshot of the website design and give this image to LLMs to obtain the full code implementation that can render into the desired web page in a fully end-to-end manner ?\nThat’s what researchers tried to answer with Design2Code task, which provides the first systematic study on this visual design to code implementation task. Researcher introduce Design2Code benchmark, a curated list of 484 real-world webpages as benchmark test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. Further, a suite of multimodal prompting methods such as Direct Prompting, Text-Augmented Prompting and Self-Revision Prompting were also developed, which show their effectiveness on GPT-4V and Gemini Vision Pro.\nFinally, researchers finetune an open-source Design2Code-18B model, with CogAgent-18B as base model, that successfully matches the performance of Gemini Pro Vision. Both human evaluation and automatic metrics show that GPT-4V is the clear winner on this task, where annotators think GPT-4V generated webpages can replace the original reference webpages in 49% cases in terms of visual appearance and content; and perhaps surprisingly, in 64% cases GPT-4V generated webpages are considered better than even the original reference webpages. fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning.\nPaper : https://arxiv.org/pdf/2403.03163.pdf"
  },
  {
    "objectID": "posts/InfiMM-HD/InfiMM-HD.html",
    "href": "posts/InfiMM-HD/InfiMM-HD.html",
    "title": "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding",
    "section": "",
    "text": "Multimodal Large Language Models (MLLMs) have experienced significant advancements recently, largely driven by the integration of pretrained vision encoders with Large Language Models (LLMs). This trend is exemplified by developments in Flamingo, BLIP-2, LLaVA, and MiniGPT-4. However, challenges persist in accurately recognizing and comprehending intricate details within high-resolution images. This is attributed to pretrained Vision Transformer (ViT) encoders used by MLLMs, where low resolution suffices for basic image-level semantic understanding but is inadequate for detailed, region-level analysis.\nTo address these challenges, researchers introduce InfiMM-HD, a novel architecture specifically designed for processing images of high resolutions with low computational overhead. InfiMM-HD consists of three components: a Vision Transformer Encoder, a Gated Cross Attention Module, and a Large Language Model. InfiMM-HD employs a cross attention mechanism to seamlessly integrate visual information with language models in a low-dimensional space. To address the formidable computational demands associated with high-resolution images, it partitioned the input high resolution image into smaller sub-images, each subjected to individual processing using a shared Vision Transformer (ViT) specifically tailored for relatively lower resolutions.\nFurther, researcher also introduced a four-stage training pipeline that effectively achieves a high-resolution Multimodal Large Language Model with reduced training cost, from initial low-resolution pretraining stage, to continue pretraining stage for knowledge injection and alignment, to dynamic resolution adaption stage for high resolution adoption and finally go through visual instruction fine-tuning stage.\nDuring evaluation under Visual Question Answering (VQA) and text oriented VQA tasks InfiMM-HD outperformed its closest competitor by an average margin of 3.88%. InfiMM-HD was also evaluated on recently proposed MLLMs evaluation benchmarks, including MMMU, MMVet, InfiMM-Eval, MMB, MME, and POPE. Overall InfiMM-HD demonstrates commendable overall performance, highlighting its adaptability and competence across diverse disciplines.\nPaper : https://arxiv.org/pdf/2403.01487.pdf\nCode : https://github.com/InfiMM/infimm-hd/\nModel : https://huggingface.co/Infi-MM/infimm-hd"
  },
  {
    "objectID": "posts/GaLore/GaLore.html",
    "href": "posts/GaLore/GaLore.html",
    "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
    "section": "",
    "text": "Training Large Language Models (LLMs) is challenging due to memory constraints from weight and optimizer size. Low-rank adaptation (LoRA) addresses this by adding trainable low-rank matrices to frozen pre-trained weights, reducing parameters and memory usage. However for fine-tuning, LoRA is not shown to reach a comparable performance as fullrank fine-tuning. For pre-training from scratch, it is shown to require a full-rank model training as a warmup, before optimizing in the low-rank subspace. There are two possible reasons: (1) the optimal weight matrices may not be low-rank, and (2) the reparameterization changes the gradient training dynamic.\nTo address this researchers have introduced Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory efficient than common low-rank adaptation methods such as LoRA. GaLore takes a novel approach compared to methods like LoRA and ReLoRA. Instead of doing low-rank projection in weight space (W = W0 + BA) and baking this into the weights every T steps, GaLore performs low-rank projection in gradient space: G = P @ G' @ Q^T. i.e, GaLore recomputes the projection matrices P and Q every T steps using the SVD of the full gradient. This allows the low-rank subspace to maximally adapt to the changing gradient distribution during training.\nSince GaLore computes full gradients first and uses them to update the projection matrices every T steps. This means that it has only one matrix for the gradients (G) instead of two like in LoRA. Well, P and Q must be stored, but they aren’t optimized.\nThe memory savings in GaLore come from the reduction in optimizer state and the lack of newly introduced weights. In their implementation, they only use P for further savings. This is different from LoRA where the savings only come from the low-rank approx of the weights.\nOur approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, it demonstrates, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.\nPaper : https://arxiv.org/pdf/2403.03507.pdf\nCode : https://github.com/jiaweizzhao/galore"
  },
  {
    "objectID": "posts/MoA/MoA.html",
    "href": "posts/MoA/MoA.html",
    "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
    "section": "",
    "text": "Adapter-based fine-tuning methods, such as LoRA, are key to making large language models disruptive in various domain specific applications. LoRA introduces a limited number of domain-specific parameters to retain domain related knowledge and does not need to fine-tuning all parameters of the pre-trained model, which can effectively reduce the training cost of LLMs.\nSo is it possible to obtain multiple customized domain capabilities from a single LLM model with LoRA?\nSimple approach would be to directly mix data from multiple domains together and only add one LoRA module for instruction fine-tuning. However, achieving the right balance of data is crucial in such a case in order to prevent catastrophic forgetting and interference between tasks.\nTo address these limitations and enhance training flexibility, researchers propose the Mixture-of-LoRAs (MoA) architecture – a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. MoA introduces a routing mechanism within a decoder-only model architecture to automatically select LoRA experts. This mechanism is applicable to mainstream Large Language Models (LLMs) and can deploy LoRA modules for multiple tasks using the same LLM while managing limited computing resources. Additionally, To enhance training and inference efficiency, MoA employs parallel processing of different domain samples within a batch during training and a LoRA expert selection approach during inference. This approach harnesses the strengths of different expert models and the base LLM while leveraging the complementary nature of knowledge across different domains.\nExperiments on diverse domain specific tasks such in Finance, Medicine, Stackoverflow and leetcode have demonstrate that MoA outperform both single LoRA and single-LoRA mix in teams of PPL, BLUE and ROUGE-L (Perplexity: 4.0128→3.9450, BLUE: 28.5348→30.5912, ROUGE-L: 37.7877→39.0163), which will also further promote the application of domain specific LLMs.\nPaper : https://arxiv.org/pdf/2403.03432.pdf"
  },
  {
    "objectID": "posts/VideoMamba/VideoMamba.html",
    "href": "posts/VideoMamba/VideoMamba.html",
    "title": "VideoMamba: State Space Model for Efficient Video Understanding",
    "section": "",
    "text": "Mastering spatiotemporal representation is one of the key areas in any video understanding task. However there usually are two challenges associated with it: (1) the large spatiotemporal redundancy within short video clips, and (2) the complex spatiotemporal dependencies among long contexts. Models such as 3D-CNN + Video transformer, S4, RMKV and RetNet tried to resolve above challenges associated with spatio-temporal but none has been successful so far.\nSo can Mamba work well for video understanding?\nThat’s what researchers have tried to address with VideoMamba, a purely SSM-based model tailored for video understanding. VideoMamba harmoniously merges the strengths of convolution and attention in vanilla ViT style. It offers a linear-complexity method for dynamic spatiotemporal context modeling, ideal for high-resolution long videos.\nFramework of VideoMamba strictly follow the architecture of vanilla ViT and adapt the bidirectional mamba block (B-Mamba) for 3D video sequences. Bidirectional Mamba (B-Mamba) block, adapts bidirectional sequence modeling for vision-specific applications. This block processes flattened visual sequences through simultaneous forward and backward SSMs, enhancing its capacity for spatially-aware processing. To apply the B-Mamba layer for spatiotemporal input, VideoMamba extends the original 2D scan into different Spatial-First bidirectional 3D scan, organizing spatial tokens by location then stacking them frame by frame.\nExtensive evaluations reveal VideoMamba’s four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding.\nPaper : https://lnkd.in/g8quHTqR\nCode : https://lnkd.in/gFN3sbZ5\nModel : https://lnkd.in/gH85xRkz"
  },
  {
    "objectID": "posts/MoAI/MoAI.html",
    "href": "posts/MoAI/MoAI.html",
    "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
    "section": "",
    "text": "Following the success of the instruction-tuned LLMs, several visual instruction tuning datasets have been meticulously curated to enhance zero-shot vision language (VL) performances in large language and vision models (LLVMs). Due to this several open-source LLVMs such as InstructBLIP, Owen-VL and LLaVA1.5 have been closing the gap in zero-shot VL performances compared to closed-source LLVMs such as GPT-4V, Gemini-Pro, and Qwen-VL-Plus.\nHowever, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR). Instead, the existing LLVMs rely mainly on the large capacity and emergent capabilities of their LLM backbones.\nIn light of this, researcher propose a new LLVM, Mixture of All Intelligence ( MoAI), which leverages auxiliary visual information obtained from various sources: (1) panoptic segmentation , (2) open-world object detection, (3) SGG, and (4) OCR models. To effectively leverage this information, two new modules are introduce: MoAI-Compressor and MoAI-Mixer. The MoAI-Compressor aligns and condenses the verbalized outputs of the external CV models into auxiliary visual information, enabling the efficient use of relevant information for VL tasks. Subsequently, MoAI-Mixer blends three types of intelligence—(1) visual features, (2) auxiliary features from external CV models, and (3) language features—into a cohesive whole.\nDuring experimentation, MoAI-7B surpasses the zero-shot performances, despite being relatively small compared to the considerably larger open-source (InstructBLIP, Owen-VL and LLaVA1.5) and closed source models (GPT-4V, Gemini-Pro, and Qwen-VL-Plus). Notably, those related to real-world scene understanding such as object existence, positions, relations, and OCR without enlarging the model size or curating extra visual instruction tuning datasets. MoAI performs well even on hallucination zero-shot datasets: POPE and HallusionBench, which suggests that accurately recognizing objects and their relationships can help prevent LLVMs from making mistakes.\nPaper : https://arxiv.org/pdf/2403.07508.pdf\nCode : https://github.com/ByungKwanLee/MoAI\nModel : https://huggingface.co/BK-Lee/MoAI-7B"
  },
  {
    "objectID": "posts/RAT/RAT.html",
    "href": "posts/RAT/RAT.html",
    "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
    "section": "",
    "text": "Factual correctness has been one of the growing concerns around LLMs reasoning capabilities. This issue becomes more significant when it comes to zero-shot CoT (Chain of Thought) prompting, aka. “let’s think step-by-step” and long-horizon generation tasks that require multi-step and context-aware reasoning, including code generation, task planning, mathematical reasoning, etc.\nSeveral prompting techniques have been proposed to mitigate this issue, one promising direction, Retrieval Augmented Generation (RAG), which utilizes retrieved information to facilitate more factually grounded reasoning.\nSo can we synergize RAG with prompting techniques such as CoT for sophisticated long-horizon reasoning ?\nWell that is what researchers have done with retrieval-augmented thoughts (RAT) prompting strategy. RAT comprises two key steps: (1) First initial a zero-shot CoT produced by LLMs along with the original task prompt and used it as queries to retrieve the information that could help revise the possibly flawed CoT. (2) Now instead of retrieving and revising with the full CoT and producing the final response at once, used the LLMs to produce the response step-by-step following the CoT (a series of subtasks), and revised only the current thought step based on the information retrieved with task prompt, the current and the past CoTs.\nDuring experimentation several LLMs of varied scales: GPT-3.5 , GPT-4 and CodeLLaMA7b were used with RAT. The results indicate that combining RAT with these LLMs elicits strong advantages over vanilla CoT prompting and RAG approaches. In particular, performances across following selection of tasks was measured : (1) code generation: HumanEval (+20.94%), HumanEval+ (+18.89%), MBPP (+14.83%), MBPP+ (+1.86%); (2) mathematical reasoning problems: GSM8K (+8.36%), and GSMHard (+31.37%); (3) Minecraft task planning (2.96 times on executability and +51.94% on plausibility);( 4) creative writing (+19.19% on human score). Nevertheless, RAT reveals how LLMs revise their reasoning process in a zero-shot fashion with the help of outside knowledge, just as what humans do.\nPaper : https://arxiv.org/pdf/2403.05313.pdf"
  },
  {
    "objectID": "posts/USER-LLM/USER-LLM.html",
    "href": "posts/USER-LLM/USER-LLM.html",
    "title": "USER-LLM: Efficient LLM Contextualization with User Embeddings",
    "section": "",
    "text": "Large language models (LLMs) have revolutionized the field of user modeling and personalization due to its ability to learn and adapt from massive amounts of textual data. By analyzing user interactions and understanding user preferences, LLMs can be leveraged to power recommendations, language generation, summarization, and question answering in ways that are highly relevant and engaging to users.\nHowever, user interaction data is often complex, spanning multiple journeys with sparse data points, various interaction types (multimodal), and potential noise or inconsistencies. This complexity can hinder an LLM’s ability to identify and focus on the most relevant patterns.\nTo address these inherent complexities and limitations of leveraging raw user interaction data with LLMs, researchers have proposed USER-LLM, a novel approach centered around user embeddings. USER-LLM dynamically incorporates user preferences and behaviors from various interaction modalities (e.g., video watch history, ratings, location visits), enhancing LLM understanding and personalization capabilities while supporting various encoder architectures and multimodal fusion mechanisms.\nThe USER-LLM approach consists of two key phases: generating high-quality user embeddings and contextualizing LLMs with these user embeddings. In phase one, a Transformer-based encoder is pretrain on user interaction data, utilizing self-supervised learning to capture behavioral patterns across multiple interaction modalities. Then a multifeature autoregressive Transformer is used to generate embeddings that capture long-range dependencies and contextual relationships within sequential data while handling multimodal user data effectively. In phase two, user embeddings is integrated with an LLM during fine tuning using cross attention, where the LLM’s intermediate text representations attend to the output embeddings from the pretrained user encoder, enabling dynamic context injection (similar to Flamingo).\nDuring experimentation, USER-LLM v.s. DualEnc & Bert4Rec baselines for next item prediction. USER-LLM outperforms the two nonLLM baselines on MovieLens and Google Local review datasets. Overall, USER-LLM showed competitive performance compared with non-LLM baselines and text-prompt-based LLM personalization techniques, particularly in handling long sequences and understanding users deeply.\nPaper : https://arxiv.org/pdf/2402.13598.pdf"
  },
  {
    "objectID": "posts/RAFT/RAFT.html",
    "href": "posts/RAFT/RAFT.html",
    "title": "RAFT: Adapting Language Model to Domain Specific RAG",
    "section": "",
    "text": "Adapting LLMs to the specialized domains, which is essential to many emerging applications, usually takes two paths: in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning. RAG-based methods allow the Language Model (LLM) to use documents for answering questions but miss out on learning opportunities in fixed domain settings. On other hand, Supervised fine-tuning offers better learning of general document patterns but current approaches don’t effectively use documents during testing or consider retrieval imperfections during training.\nSo can we adapt pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains?\nThat’s what researchers have tried to address with Retrieval Augmented Fine Tuning (RAFT). RAFT is a training recipe that improves the model’s ability to answer questions in an “openbook” in-domain setting. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT’s chain-of-thought (COT) style response helps improve the model’s ability to reason.\nCompared with the base Llama2 instruction-tuned model, RAFT train model with RAG does much better in terms of extracting information as well as being robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on Torch Hub evaluation. Compared with DSF (domain specific finetuning) on the specific dataset, RAFT does much better on tasks like HotpotQA and HuggingFace datasets (30.87% on HotpotQA and 31.41% on HuggingFace). Overall, RAFT presents a novel, yet simple technique to improve pretrained LLMs for in-domain RAG.\nPaper : https://arxiv.org/pdf/2403.03432.pdf"
  },
  {
    "objectID": "posts/PERL/PERL.html",
    "href": "posts/PERL/PERL.html",
    "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
    "section": "",
    "text": "Reinforcement Learning from Human Feedback (RLHF) is one of the most popular methods to align Pretrained Large Language Models (LLMs) with human preferences. It involves fitting a reward model (RM) on human preference data, and then uses this RM to tune the parameters of the LLM using Reinforcement Learning (RL). Typically, there are 2 model training processes as part of the RLHF process: reward model training, and reinforcement learning. However, the complexity and computational cost of the RLHF training process has hindered its adoption.\nSo is there any way to make RLHF more efficient and accessible ?\nThat is what researchers from Google have solved by using Parameter Efficient Reinforcement Learning (PERL), in which LoRA is used to perform reward model training and reinforcement learning.\nDuring reward model training in PERL, LoRA adapters are attached to each attention projection matrix and trained while keeping the language model backbone frozen. These trained adapters are saved and combined with the projection matrices during inference, resulting in a reward model equivalent to a non-LoRA one.\nIn the PERL reinforcement learning loop, language models with LoRA adapters serve as policy models. LoRA adapters are attached to each attention projection matrix, with only these adapters being trained while the language model backbone remains frozen. Training involves policy gradient computation on reward scores, along with KL regularization with the anchor policy. The memory required for training, primarily due to modern optimizers like Adam or Adafactor, is reduced significantly by PERL’s reduction in trainable parameters.\nDuring extensive experiments on various datasets (Reddit TL;DR, BOLT English SMS/Chat dataset, Anthropic’s Helpfulness and Harmlessness dataset and Stanford Human Preferences Dataset), PERL achieves comparable results to conventional RLHF, for which all the model parameters are tuned, while reducing memory usage by approx 50%, and speeding up the training by up to 90% for the Reward Model training, and more modest memory savings of 20%, and speed-up of 10% in the RL loop.\nPaper : https://arxiv.org/pdf/2403.10704.pdf"
  },
  {
    "objectID": "posts/EvoLLM/EvoLLM.html",
    "href": "posts/EvoLLM/EvoLLM.html",
    "title": "Evolutionary Optimization of Model Merging Recipes",
    "section": "",
    "text": "Model merging offers a novel approach to leverage the strengths of multiple pre-trained models. It allows us to combine task-specific models, each potentially fine-tuned for a particular downstream task, into a single unified model. Model merging works surprisingly well and produced many state-of-the-art models on the Open LLM Leaderboard (https://lnkd.in/g3b7eZpm).\nThere are multiple widely used model merge algorithm such as Spherical Linear Interpolation (SLERP), TIES-Merging and DARE each one has its own advantage over one another. Recently, SakanaAI has released a new model merge method called Evolutionary Model Merge, a general method that uses evolutionary techniques to efficiently discover the best ways to combine different models from the vast ocean of different open-source models with diverse capabilities. Evolutionary Model Merge approach encompasses (1) evolving the weights for mixing parameters at each layer in the parameter space (PS); (2) evolving layer permutations in the data flow space (DFS); and (3) an integrated strategy that combines both methods for merging in both PS and DFS. Notice that merging in the PS is not simply copying and stitching of the layers parameters, but also mixes the weights.\nDuring experimentation, the Evolutionary Model merge method was used to automatically evolve for a Japanese Large Language Model (LLM) capable of Math reasoning, and a Japanese Vision-Language Model (VLM). Surprisingly, both models achieve state-of-the-art results on several LLM and Vision benchmarks, while not being explicitly optimized to be good at these benchmarks! EvoLLM-JP-A-v1-7B achieved 52.0%, outperforming individual models Shisa Gamma 7B v1 (9.6%) Abel 7B 002 (30.0%) where as EvoVLM-JP-v1-7B achieved 51.25, outperforming LLaVA-1.6-Mistral-7B (41.10).\nPaper : https://arxiv.org/pdf/2403.13187.pdf\nModel : https://huggingface.co/SakanaAI\nDemo : https://huggingface.co/spaces/SakanaAI/EvoVLM-JP"
  },
  {
    "objectID": "posts/mPLUG/mPLUG.html",
    "href": "posts/mPLUG/mPLUG.html",
    "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
    "section": "",
    "text": "Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Most of Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding consist of a pre-trained visual encoder (e.g. the ViT from CLIP ) and the LLM with a Vision-toText (V2T) module presenting a promising performance on text recognition ability but lack general structure understanding abilities for text-rich document images.\nFor better Visual Document Understanding with MLLMs researchers have proposed Unified Structure Learning on text-rich images for MLLMs and design both structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image.\nTo better encode structure information, a simple and effective SOTA model DocOwl 1.5 was built. DocOwl 1.5 follows the typical architecture of MLLMs, which consists of a visual encoder, a vision-to-text module, and a large language model as the decoder. To better keep the textual and layout information in text-rich images of high resolution, vision-to-text module H-Reducer was design, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently.\nFinally, to enhance the text recognition and structure understanding abilities, Unified Structure Learning is performed with structure-aware parsing and multi-grained text localization tasks for all types of images. Then, the model is jointly tuned on multiple downstream tasks of Visual Document understanding.\nDocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, covering documents (DocVQA, InfoVQA, DeepForm, KLC ), tables (WTQ, TabFact), charts (ChartQA), natural images (TextVQA, TextCaps), and webpage screenshots (VisualMRC). improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks.\nPaper : https://arxiv.org/pdf/2403.12895.pdf"
  },
  {
    "objectID": "posts/Cobra/Cobra.html",
    "href": "posts/Cobra/Cobra.html",
    "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
    "section": "",
    "text": "Vision language models (VLMs) like GPT-4, LLaMAadapter, and LLaVA have been instrumental in augmenting LLMs with visual understanding capabilities. VLMs serve as foundational models in tackling a wide array of tasks including Visual Question Answering (VQA), captioning, and visual content generation. However, there has not been much progress to improve VLMs performance mainly due to LLMs transformer architecture, which has a less efficient quadratic computation complexity.\nSo can VLMs based on non transformer architecture like Mamba perform better than transformer one ?\nWell that’s what researchers from Westlake University have addressed with Cobar, an efficient Mamba language model with integrated visual modality. Cobar consists of three components: a vision encoder, a projector, and a Mamba backbone. For vision encoder DINOv2 and SigLIP are fused and used as vision backbone. The intuition is that combining the visual representations, which capture low-level spatial properties from DINOv2 and the semantic properties provided by SigLIP further improves the performance on downstream tasks. The projector is a simple learnable MLP that aligns the features of vision and text. Finally, the LLM backbone is a Mamba language model with 2.8B parameters. During training, the parameters of vision encoders are frozen and the parameters of the projector and Mamba LLM backbone are fine-tuned.\nDuring experimentation several ablation studies on projectors (MLP or Lightweight Downsample Projector), vision backbones (DINOv2 + SigLIP or SigLIP only), and LLM backbones (base model or instruction-tuned chat model) was carried out. Compared with Baselines, Cobra achieves comparable performance to LLaVA v1.5 7B with about 43% of the number of parameters. Inference Speed, Cobra performs 3× ∼ 4× faster than MobileVLM v2 3B and TinyLLaVA 3B on a single NVIDIA A100 80G GPU. Overall, Cobra is competitive in the field of Visual Large Language Models (VLLM), especially in processing visual information and generating natural language descriptions.\nPaper : https://arxiv.org/pdf/2403.14520.pdf"
  },
  {
    "objectID": "posts/SiMBA/SiMBA.html",
    "href": "posts/SiMBA/SiMBA.html",
    "title": "SiMBA: Simplified Mamba-based Architecture for Vision and Multivariate Time series",
    "section": "",
    "text": "Recently, Structured State Space models (SSM) such as Mumba have been pitched as an for Transformer based models especially when it comes to increase efficiency and performance for processing longer input sequences. Mamba, while being the state-of-the-art SSM, is good for longer input sequences but has a stability issue when scaled to large networks for computer vision datasets. This is evident in Mamba when all eigenvalues of matrix A are negative real numbers, which leads to the problem of vanishing/exploding gradients.\nTo address this, researchers from Microsoft have proposed SiMBA (Simplified Mamba-based Architecture). SiMBA introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. EinFFT is specifically designed for complex number representations of frequency components, enabling the effective capture of key patterns in Image patch data with a global view and energy compaction.\nSiMBA illustrates an important trade-off between performance and scalability. Mamba by itself may have stability issues for large networks. Mamba combined with MLP for channel mixing bridges the performance gap for small-scale networks, but may have the same stability issues for large networks. Mamba combined with EinFFT solves stability issues for both small-scale and large networks.\nExtensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art transformers. Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and transfer learning benchmarks such as Stanford Car and Flower as well as task learning benchmarks as well as seven time series benchmark datasets.\nPaper : https://arxiv.org/pdf/2403.15360.pdf"
  },
  {
    "objectID": "posts/DRAGIN/DRAGIN.html",
    "href": "posts/DRAGIN/DRAGIN.html",
    "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
    "section": "",
    "text": "Traditional methods of RAG typically rely on single-round retrieval, using the LLM’s initial input to retrieve relevant information from external corpora. While this method is effective for straightforward tasks, it tends to fall short for complex multi-step tasks and long-form generation tasks. In contrast, Dynamic RAG performs multiple times of retrieval during the generation process of LLMs. It includes two steps: identifying the optimal moment to activate the retrieval module (deciding when to retrieve), and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods such as IR CoT, RETRO or IC-RALM fall short in one or both aspects.\nTo overcome these limitations, a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation has been introduced based on the real-time Information Needs of LLMs. For the timing of retrieval (deciding when to retrieve), RIND: Real-time Information Needs Detection is used. This method refines the retrieval activation process by evaluating not only the uncertainty of each token, but also its semantic contribution and the impact on the following context. RIND begins by quantifying the uncertainty of each token generated during the LLM’s inference process. This is accomplished by recording the entropy of the token’s probability distribution across the vocabulary. For the formulation of retrieval queries, QFS: Query Formulation based on Self-attention is used, which innovates query formulation by leveraging the LLM’s self-attention across the entire context.\nDuring evaluating the performance of DRAGIN against various baselines across four benchmark datasets: 2WikiMultihopQA, HotpotQA, StrategyQA, and IIRC. DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of underline methods. In conclusion, DRAGIN provide a lightweight RAG framework that can be incorporated into any Transformer-based LLMs without further training, fine-tuning, or prompt engineering.\nPaper : https://arxiv.org/pdf/2403.10081.pdf"
  },
  {
    "objectID": "posts/RigorLLM/RigorLLM.html",
    "href": "posts/RigorLLM/RigorLLM.html",
    "title": "RigorLLM: Resilient Guardrails for large language models against undesired content",
    "section": "",
    "text": "Large language models (LLMs) have demonstrated impressive capabilities in NLG and different downstream tasks. However, the potential of LLMs to produce biased or harmful outputs, especially when exposed to malicious prompts, remains a significant concern.\nExisting mitigation strategies such as OpenAI content moderation API, Perspective API, Nemo Guardrails and LlamaGuard, which directly moderates both the inputs and outputs of LLMs, presents an effective and efficient solution. However, these solutions primarily rely on LLMs for detecting harmful contents, leaving them susceptible to jailbreaking attacks.\nTo overcome this challenges researchers have proposed RigorLLM (Resilient Guardrails for large language models), a novel and multi-faceted framework for input/output content moderation for LLMs based on different levels of constrained optimizations on corresponding components, such as data generation and safe suffix optimization.\nRigorLLM consists of a training stage and a testing stage. During the training stage, real-world harmful and benign data are embedded with a pre-trained text encoder. Next, embedding space is augmented by generating instances belonging to harmful categories leveraging Langevin dynamics. During the testing stage, first a safe suffix is optimized for the input to alleviate the vulnerability against jailbreak attacks. Then the input is augmented by generating text-level transformations such as paraphrases or summaries using LLMs. Finally, predictions for all augmented texts and the original text is obtained by (1) performing probabilistic KNN in the embedding space and (2) querying a pre-trained LLM. Lastly, we aggregate the predictions from KNN and LLM to derive the final prediction, yielding a comprehensive and reliable harmful content detection mechanism.\nDuring benchmark RigorLLM against state-of-the-art solutions such as OpenAI content moderation API, Perspective API, NeMo Guardrails, and LlamaGuard. RigorLLM not only surpasses these baselines in harmful content detection on various datasets but also exhibits superior resilience to jailbreaking attacks. For example, on the ToxicChat dataset, RigorLLM achieves an improvement of 23% in F1 score compared to the best baseline model. Under jailbreaking attacks, RigorLLM maintains a 100% detection rate on harmful content with different adversarial strings, while other baselines exhibit significantly lower performance.\nPaper : https://arxiv.org/pdf/2403.13031.pdf"
  },
  {
    "objectID": "posts/Mini-Gemini/Mini-Gemini.html",
    "href": "posts/Mini-Gemini/Mini-Gemini.html",
    "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
    "section": "",
    "text": "LLM empowered multi-modality inputs are becoming an essential part of Vision Language Models (VLMs) such as LLaVA and Otter. However, despite these advancements, a significant gap remains between academic initiatives and the prowess of well-established models like GPT-4 and Gemini, which are trained with huge amounts of data and resources.\nSo how to push forward the VLMs approaching well-developed models with acceptable cost in an academic setting?\nIntroducing Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Mini-Gemini employs an any-to-any paradigm, which is adept at handling both image and text as input and output. In particular, Mini-Gemini introduced an efficient visual token enhancement pipeline for input images, featuring a dual-encoder system. It comprises twin encoders, one for high-resolution images and the other for low-resolution visual embedding, mirroring the cooperative functionality of the Gemini constellation. During inference, they work in an attention mechanism, where the low-resolution one generates visual queries, and the high-resolution counterpart provides candidate keys and values for reference.\nTo augment the data quality, data based on public resources, including high-quality responses, task-oriented instructions, and generation-related data are being used. The increased amount and quality improve the overall performance and extend the capability of the model.\nIn general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. For instance Mini-Gemini is on par with Gwen-VL-Plus on the MathVista and MMMU benchmark and even surpasses Gemini Pro and GPT-4V on the widely-adopted MMB benchmark\nPaper: https://arxiv.org/pdf/2403.18814\nCode : https://github.com/dvlab-research/MiniGemini"
  },
  {
    "objectID": "posts/Jamba/Jamba.html",
    "href": "posts/Jamba/Jamba.html",
    "title": "Jamba: A Hybrid Transformer-Mamba Language Model",
    "section": "",
    "text": "Finally, the first production-grade commercially available Mamba-based model delivering best-in-class quality and performance is here. Introducing Jamba, a novel architecture which combines Attention and Mamba layers, with MoE modules. Here are some key features of Jamba\n\n3X throughput on long contexts compared to Mixtral 8x7B\nDemocratizes access to a massive 256K context window\nThe only model in its size class that fits up to 140K context on a single GPU\n\nSo how does Jamba provide flexibility for balancing performance and memory requirements, while the previous Mamba-Transformer based models were not able to do so ?\nIt all comes down to Jamba architecture choice, which aims to provide not only a small number of active parameters but also an 8x smaller KV cache compared to a vanilla Transformer. The basic component is a Jamba block, which may be repeated in sequence. Each Jamba block is a combination of Mamba or Attention layers. Each such layer contains either an attention or a Mamba module, followed by a multi-layer perceptron (MLP). Further, In Jamba some of the MLPs may be replaced by MoE layers, which helps increase the model capacity while keeping the active number of parameters, and thus the compute, small\nIn summary, the different degrees of freedom in the Jamba architecture are:\n* l: The number of layers. * a : m : ratio of attention-to-Mamba layers.\n* e: how often to use MoE instead of a single MLP. * n: total number of experts per layer. * K: number of top experts used at each token.\nGiven this design space, Jamba provides flexibility in preferring certain properties over others. For example, increasing m and decreasing a, i.e, increasing the ratio of Mamba layers at the expense of attention layers, reduces the required memory for storing the key-value cache thus reducing overall memory space. Increasing the ratio of Mamba layers also improves throughput, especially at long sequences. However, decreasing a might lower the model’s capabilities. Other architecture details are standard, including grouped-query attention (GQA), SwiGLU activation function, and load balancing for the MoE. The vocabulary size is 64K. The tokenizer is trained with BPE and each digit is a separate token.\nPaper : https://arxiv.org/pdf/2403.19887.pdf"
  },
  {
    "objectID": "posts/Gecko/Gecko.html",
    "href": "posts/Gecko/Gecko.html",
    "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models",
    "section": "",
    "text": "Recent advancement in the Text Embedding model has been instrumental for various downstream tasks including document retrieval, sentence similarity, classification, and clustering. However, there present challenges in developing general-purpose text embedding models as such models require large amounts of training data to comprehensively cover desired domains and skills. Large language models (LLMs) offer a powerful alternative in such scenarios.\nSo to what extent can we leverage LLMs directly to improve text embedding models?\nIntroduced Gecko, a versatile text embedding model distilled from large language models. Gecko utilizes a two-step distillation process that begins with generating diverse, synthetic paired data using an LLM. Next, data quality is further refined by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM.\nTo train Gecko, a 1.2B parameter pre-trained transformer base language model is being used that undergoes two additional training stages: pre-finetuning and fine-tuning. During pre-finetuning stage model is been trained on large self-supervised text corpus such as question-answer pairs from online forums and QA websites.\nFor fine-tuning, Gecko uses a novel fine-tuning dataset FRet, the Few-shot Prompted Retrieval dataset. Given a sampled passage from the web, FRet first utilizes LLMs to generate a relevant task and a query for the passage. Then, each query and task is fed into a pre-trained embedding model to obtain nearest neighbor passages, which are then scored by the LLM to mine positive and negative passages.\nBy combining this LLM-generated and LLM-ranked data with human-annotated data, Gecko-1B with 768-dimensional embeddings achieves the best performance on the popular MTEB benchmark, Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings. Moreover, Gecko often outperforms other systems that use either larger base models (7B) or higher dimensional embeddings (1k to 4k).\nPaper : https://arxiv.org/pdf/2403.20327.pdf"
  },
  {
    "objectID": "posts/sDPO/sDPO.html",
    "href": "posts/sDPO/sDPO.html",
    "title": "sDPO: Don’t Use Your Data All at Once",
    "section": "",
    "text": "As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important to ensure safety and usefulness of the model. Thus, reinforcement learning techniques such as proximal policy optimization (PPO) are key in this alignment phase, despite their complexity.\nTo resolve this complexity during LLM training direct preference optimization (DPO) has been widely used mainly for its simplicity and effectiveness. DPO involves curating preference datasets using human or strong AI (e.g., GPT4 ) judge to select chosen and rejected responses to questions. These datasets are then used to train LLMs by comparing log probabilities of chosen versus rejected answers. However, due to unavailability of log probabilities for proprietary models like GPT-4 the reference model is simply set as the base SFT model, which is a much weaker alternative with potentially misaligned preferences.\nTo address this researchers have proposed ‘stepwise DPO’, named sDPO, where preference datasets are divided to be used in multiple steps. The aligned model from the previous step is used as the reference and target models for the current step. The reference model is used to calculate the log probabilities and the target model is trained using the preference loss of DPO at each step.\nEvaluating results of applying sDPO to the SFT base model such as pretrained-only ‘SOLAR 10.7B’ to the instruction-tuned ‘SOLAR 10.7B + SFT’, seen an increase of +5.24 in terms of H4. Applying sDPO on SOLAR 10.7B + SFT further increases the H4 score upto 74.31, an improvement of +4.80. Notably, ‘SOLAR 10.7B + SFT + sDPO’ outperforms other larger models such as Mixtral 8x7BInstruct-v0.1, despite the smaller number of parameters. This highlights that effective alignment tuning could be the key to unlocking next level performance for smaller LLMs.\nPaper : https://arxiv.org/pdf/2403.19270.pdf"
  },
  {
    "objectID": "posts/Mixture-of-Depths/Mixture-of-Depths.html",
    "href": "posts/Mixture-of-Depths/Mixture-of-Depths.html",
    "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
    "section": "",
    "text": "Transformer FLOPs Equation or FLOPs-per-token is one of the key attributes in determining computation budget for any transformer base LLM models. Usually in language models not all tokens and sequences require the same time or effort to accurately make a prediction. And yet, Transformer-based language models spread FLOPs uniformly across input sequences.\nSo can transformers learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimizing the allocation along the sequence for different layers across the model depth ?\nThis is what researchers from Google are trying to address with the Mixture-of-Depths (MoD) Transformer. MoD is similar to mixture-of-experts (MoE) transformers where in a router is used to choose among potential computational paths. But unlike in MoE transformers the possible choices are a standard block’s computation (i.e., self-attention and MLP) or a residual connection.\nIn general, MoD method enforces a total compute budget by capping the number of tokens (𝑘) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-𝑘 routing mechanism. Since 𝑘 is defined a priori, this simple procedure uses a static computation graph with known tensor sizes. Nevertheless, since the identities of the 𝑘 tokens are fluid, MoD method can expend FLOPs non-uniformly across the time and model depth dimensions. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently.\nDuring evaluation Mixture-of-Depths (MoD) Transformer models not only match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.\nPaper : https://arxiv.org/pdf/2404.02258.pdf"
  },
  {
    "objectID": "posts/ReFT/ReFT.html",
    "href": "posts/ReFT/ReFT.html",
    "title": "ReFT: Representation Finetuning for Language Models",
    "section": "",
    "text": "Parameter-efficient finetuning (PEFT) methods have been instrumental in rapid adoption of fine tuned domain specific LLMs. PEFTs not only reduced memory usage and time during LLMs training but also shown to achieve similar performance to full finetuning LLMs in many practical settings. Recent PEFT adapters such as LoRA and QLoRA have further fuelled this growth.\nA hallmark of current state-of-the-art PEFTs is that they modify weights rather than representations. It has been shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative.\nSo instead of weight updates can we learn interventions to modify a small fraction of model representations ?\nIntroducing Representation Finetuning (ReFT), a family of intervention-based representation finetuning methods. Typically, an intervention I is a tuple ⟨Φ, P, L⟩ that encapsulates a single inference-time modification of the representations computed by a Transformer-based LM. The three components of an intervention are * The intervention function Φ ∶ R d → R d with learned parameters ϕ. * A set of input positions P ⊆ {1, . . . , n} that the intervention is applied to. * The layer L ∈ {1, . . . , m} at which the intervention is applied. A ReFT method is a constrained set of f non-overlapping interventions I = {I1, . . . , If }\nFurther, researchers have also introduced a strong instance of the ReFT family called Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a parameterization of ReFT that intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix, building directly on the distributed alignment search (DAS) method.\nDuring evaluation of LoReFT on LLaMA-family models against existing PEFTs on standard benchmarks from four domains: commonsense reasoning, arithmetic reasoning, instruction following, and NLU it was found that LoReFT uses 10×–50× times fewer parameters while achieving state-of-the-art performance. These findings indicate that ReFT methods may emerge as more efficient and effective alternatives to weight-based PEFTs.\nPaper : https://arxiv.org/pdf/2404.03592.pdf"
  },
  {
    "objectID": "posts/SoS/SoS.html",
    "href": "posts/SoS/SoS.html",
    "title": "Stream of Search (SoS): Learning to Search in Language",
    "section": "",
    "text": "Transformer-based auto-regressive models such as GPT have shown remarkable performance in generative tasks but struggle when it comes to complex decision-making and reasoning tasks requiring search.\nThere are two main factor attributes for this performance: (1) the snowballing of errors, where a single mistake can compound and lead to increasingly poor performance in subsequent steps, and (2) a difficulty in ‘lookahead tasks’, where the model must predict the consequences of its actions several steps ahead. Both of these issues can be attributed to limited ability to search and backtrack. Even supplementing language models with search ability during inference has not shown much improvement.\nWhat if language models can learn to search during training itself ? thereby improving its ability to discover more flexible search strategies through self-improvement and better equipped itself to handle error compounding and lookahead tasks.\nIntroducing Stream of Search (SoS) framework, a unified language for search that captures an array of different symbolic search strategies such as exploration, backtracking, and pruning. To understand SoS let’s instantiate a simple countdown problem with input numbers and a target number. The goal here is to combine input numbers with simple arithmetic operations to get to the given target number. To start with, the Stream of Search (SoS) dataset is created containing search trajectories generated by diverse search strategies, including exploration and backtracking. Then a transformer-based language model is pretrain on this SoS dataset. Finally, the model is finetuned with two policy improvement methods: Advantage-Induced Policy Alignment (APA) optimize for correctness and Self-Taught Reasoner (STaR) for expert iteration.\nDuring evaluation, SoS models were able to solve approximately 36% of the previously unsolved problems and about 4% of the difficult problems including problems that cannot be solved by any of the heuristic solvers. Finally, SoS + APA and SoS + STaR models also have better models of the environment, making fewer errors while searching, and finding the solution more quickly.\nPaper : https://arxiv.org/pdf/2404.03683.pdf"
  },
  {
    "objectID": "posts/DNO/DNO.html",
    "href": "posts/DNO/DNO.html",
    "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences",
    "section": "",
    "text": "Fine-tuning LLMs using Reinforcement Learning from Human Feedback (RLHF) has alway been a preferred way for making LLMs more useful by aligning them with human values or preferences. RLHF optimization toward the preference is usually done using a two-step procedure: reward learning, and policy optimization (through RL) to maximize the learned reward. RLHF fundamentally relies on the reward maximization framework, wherein reward-based preferences are governed by, e.g., the BT model.\nHowever, this reward maximization framing poses following major limitations: (1) Reward functions, used to score responses to inputs, can’t capture complex preferences like intransitive or cyclic preferences between multiple outputs. (2) Reward functions in practice can quickly become “stale” as the distribution of the policy shifts under training leaving them vulnerable to “reward hacking (3) Even when preferences can be perfectly expressed in reward-based Behavioral Transfer (BT) models, optimizing towards rewards can lead to problematic behaviors.\nTo address these weaknesses, researchers from Microsoft have introduced Direct Nash Optimization (DNO), a provable and scalable RLHF algorithm that marries the simplicity and stability of contrastive learning with theoretical generality from optimizing general preferences.\nDirect Nash Optimization, addresses these challenges by approximating soft policy iteration updates with a regression-based contrastive objective in a batched manner, which is a much more stable and forgiving learning objective, and establish a concentration bound of Oe( 1/N) on the squared total variation error between the learned policy and its target of the soft policy iteration update at any given iteration t. Theoretically, DNO converges to the Nash equilibrium on-average, but in practice enjoys monotonic improvement across iterations. Because DNO is a batched on-policy algorithm using a regression-based objective, its implementation is straightforward and efficient.\nDuring experimentation, a 7B parameter Orca-2.5 model is aligned by DNO and achieves the state-of-the-art win-rate against GPT-4-Turbo of 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% → 33%) over the initializing model. It outperforms models with far more parameters, including Mistral Large, Self-Rewarding LM (70B parameters), and older versions of GPT-4. Moreover, DNO enjoys monotonic improvement across iterations which helps it improve even over a strong teacher (such as GPT-4).\nPaper : https://arxiv.org/pdf/2404.03715.pdf"
  },
  {
    "objectID": "posts/RHO-1/RHO-1.html",
    "href": "posts/RHO-1/RHO-1.html",
    "title": "RHO-1: Not All Tokens Are What You Need",
    "section": "",
    "text": "High quality training data sets are crucial to boost LLMs performance. Various data filtering techniques such as heuristics and classifiers are being utilized to select such training dataset. However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training.\nFurther, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can result in wasted computation on non-beneficial tokens, possibly limiting LLM’s potential to merely mediocre intelligence.\nSo are all tokens in a corpus equally important for language model training ?\nApparently not, researchers have found two groups of tokens exist during training : “easy tokens” that are already learned, and “hard tokens” that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates. To explain this researchers have introduced a new language model called RHO-1. Unlike traditional LMs that learn to predict every next token in a corpus, RHO-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that are aligned with the desired distribution. The approach involved\n\nTrain a reference language model on high-quality corpora.\nUse above reference model to score each token in a corpus using its loss\nFinally, train a language model only on those tokens that exhibit a high excess loss between the reference and the training model.\n\nDuring evaluation, RHO-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively — matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, RHO-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.\nPaper : https://arxiv.org/pdf/2404.07965.pdf"
  },
  {
    "objectID": "posts/TR-DPO/TR-DPO.html",
    "href": "posts/TR-DPO/TR-DPO.html",
    "title": "Trust Region Direct Preference Optimization (TR-DPO) : Learn Your Reference Model for Real Good Alignment",
    "section": "",
    "text": "Aligning large language models with human preferences (RLHF) has become increasingly important to ensure safety and overall usefulness of the model. Traditionally, the alignment of language models hinges upon the training objective defined as\n\nwhere D is the collection of training data, πθ is the policy being optimized, πref is the reference model that is usually a supervised fine-tuned LM (SFT policy), and rϕ(x, y) denotes the Reward Model (RM) that is trained in line with human preferences.\nRecent methods such as The Direct Preference Optimization (DPO) reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy.\n\nwith the dataset D consisting of tuples (x, yw, yl) in which x represents a text prompt, while yw and yl stand for the human annotator’s preferred and less preferred continuations, respectively\nThis practice prompts us to question: Why does the reference model remain static during training ?\nTo address this researchers have introduced a novel conceptualization of the training process for alignment algorithms, dubbed Trust Region Direct Preference Optimization (TR-DPO). TR-DPO features updating the reference policy during training—either by softly integrating πθ into πref using a weighted approach or by outright replacing the reference policy with πθ after a predetermined number of steps.\nTraditionally, vanilla DPO uses a fixed reference policy during the training, where as TR-DPO update it either with soft-update for which parameters of πθ are merged into parameters of πref with some weight α, or with hard-update (right) for which we copy parameters of πref into reference policy once in a predetermined number of training steps.\nDuring evaluation of TR-DPO against DPO on the Anthropic HH and TLDR datasets. TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.\nPaper : https://arxiv.org/pdf/2404.09656.pdf"
  },
  {
    "objectID": "posts/MEGALODON/MEGALODON.html",
    "href": "posts/MEGALODON/MEGALODON.html",
    "title": "MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length",
    "section": "",
    "text": "The Transformer architecture is backbone of any production LLMs, but despite its remarkable capabilities, it faces challenges with quadratic computational complexity and limited inductive bias for length generalization, making it inefficient for long sequence modeling.\nTechniques like efficient attention mechanisms and structured state space models (SSM) have been introduced to overcome these limitations, aiming to enhance scalability and performance. However, the practical application of these methods empirically underperform Transformers in pretraining efficiency and downstream task accuracy.\nTo address this researchers have introduced MEGALODON, an improved MEGA architecture (exponential moving average with gated attention), which harnesses the gated attention mechanism with the classical exponential moving average (EMA). To further improve the capability and efficiency of MEGALODON on large-scale long context pre-training, multiple novel technical components have been introduced.\nFirst, MEGALODON introduces the complex exponential moving average (CEMA) component, which extends the multi-dimensional damped EMA in MEGA to the complex domain. Then, MEGALODON proposes the timestep normalization layer, which generalizes the group normalization layer to autoregressive sequence modeling tasks to allow normalization along the sequential dimension. To improve large-scale pretraining stability, MEGALODON further proposes normalized attention, together with pre-norm with two-hop residual configuration by modifying the widely-adopted pre and post-normalization methods. By simply chunking input sequences into fixed blocks, as is done in MEGA-chunk, MEGALODON achieves linear computational and memory complexity in both model training and inference.\nIn a controlled head-to-head comparison with LLAMA2, MEGALODON achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. MEGALODON reaches a training loss of 1.70, landing mid-way between LLAMA2- 7B (1.75) and 13B (1.67). The improvements of MEGALODON over Transformers are robust throughout a range of benchmarks across different tasks and modalities\nPaper : https://arxiv.org/pdf/2404.08801.pdf"
  },
  {
    "objectID": "posts/RecurrentGemma/RecurrentGemma.html",
    "href": "posts/RecurrentGemma/RecurrentGemma.html",
    "title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language Models",
    "section": "",
    "text": "Recently Google has released RecurrentGemma, an open language model which uses Google’s novel Griffin architecture. Griffin combines linear RNN with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences.\nTypically, transformer architecture KV cache grows linearly with sequence length. Although there are various techniques such as local attention to reduce the cache size but it comes at the expense of reduced performance. In contrast, RecurrentGemma-2B compresses input sequences into a fixed-size state without sacrificing performance. This reduces memory use and enables efficient inference on long sequences.\nDuring evaluating RecurrentGemma-2B across a broad range of domains, using a combination of automated benchmarks and human evaluation. RecurrentGemma-2B achieves comparable performance to Gemma-2B, even though Gemma-2B was trained on 50% more tokens. In creative writing and coding tasks, RecurrentGemma-2B-IT achieves a 43.7% win rate against the larger Mistral 7B model. A key advantage of RecurrentGemma is its inference speed, which is roughly 40k tokens per second, considerably higher than your average transformer architecture based models In conclusion, RecurrentGemma-2B offers the performance of Gemma, while achieving higher throughput during inference, especially on long sequences.\nPaper : https://lnkd.in/g3YS_su9"
  },
  {
    "objectID": "posts/TransformerFAM/TransformerFAM.html",
    "href": "posts/TransformerFAM/TransformerFAM.html",
    "title": "TransformerFAM: Feedback attention is working memory",
    "section": "",
    "text": "While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. One of the widely used approaches to handle long context inputs is Sliding Window Attention or Block Sliding Window Attention (BSWA).\nDuring inference, LLMs allocates KV cache twice the window length at the beginning, and then using a ring buffer to update the necessary components at each step, in order to avoid memory allocation and copying operations every step, which are computationally expensive. With SWA or BSWA only a fixed ring buffer (block size + memory segment) needs to cache, which keeps memory usage constant regardless of token length enabling LLMs to generate tokens of infinite length. However, TransformerBSWA has a limited receptive field, approximately equal to the model depth × window size. This means later generated tokens may not be related to tokens outside this receptive field, such as the prompt.\nTo address this limitation, researchers have proposed a novel Transformer architecture called Feedback Attention Memory (FAM) or TransformerFAM in short that enables attention to both homogeneous sequence data and latent representations via a feedback loop. This architecture change fosters the natural emergence of working memory within Transformers allowing it to process indefinitely long sequences.\nTo understand it better we require to see attention patterns in the Transformer layer. In TransformerBSWA, input query attends to the current block and two memory segments, providing past context whereas in TransformerFAM input query attends to the current block, memory segments, and past FAM. FAM query (copied from previous FAM) compresses the current block to update FAM. This feedback loop enables information compression and propagation over indefinite horizon, which is working memory.\nDuring evaluation TransformerFAM outperformed TransformerBSWA on all the long context tasks (LCT), and on various model sizes (1B, 8B, and 24B) regardless of the number of memory segments in BSWA. It shows a significant performance improvement on Scrolls Qasper and NarrativeQA, where it has to understand 5k to 500k tokens of context before answering a question. The LCT results demonstrate that TransformerFAM can effectively compress and retain important contextual information within extremely long contexts. Further, TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models.\nPaper: https://arxiv.org/pdf/2404.09173.pdf"
  },
  {
    "objectID": "posts/LLM-R2/LLM-R2.html",
    "href": "posts/LLM-R2/LLM-R2.html",
    "title": "LLM-R2 : A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency",
    "section": "",
    "text": "Recently, DB query rewrite using LLMs has been one of the sort out use cases. The aim of query rewrite is to output a new query equivalent to the original SQL query, while having a shorter execution time. However, most of these methods utilize the sequence-to-sequence generation ability of a language model to directly output a new rewritten query given an input query, without considering any rewrite rules or DBMS information. Relying solely on LLM’s output query may lead to hallucination problems or a syntax or reference error during generation, especially for long and complicated queries.\nTo address this researchers have proposed a novel method of query rewrite named LLM-enhanced rule-based rewrite system (LLM-R2), adopting a large language model (LLM) to propose possible rewrite rules for a database rewrite system. To overcome hallucination, LLM-R2 collects a pool of demonstrations consisting of effective query rewrites using existing methods and designed baselines. Then a contrastive query is learned using a representation model to select the most useful in-context demonstration for the given query to prompt the system, optimizing the LLM’s rewrite rule selection. In addition, to address the challenge of limited training data, the learning curriculum technique utilizes to schedule the training data from easy to hard.\nDuring experimentation LLM-R2 method was applied on three different datasets, namely TPC-H, IMDB, and DSB. A significant decrease in query execution time was observed, taking only 52.5%, 56.0%, 39.8% of the querying time of the original query and 94.5%, 63.1%, 40.7% of the time of the state-of-the-art baseline method on average on the three datasets.\nPaper : https://lnkd.in/gd3SVd6H"
  },
  {
    "objectID": "posts/CodecLM/CodecLM.html",
    "href": "posts/CodecLM/CodecLM.html",
    "title": "CodecLM: Aligning Language Models with Tailored Synthetic Data",
    "section": "",
    "text": "Recent progress in instruction tuned LLM highlights the critical role of high-quality data in enhancing LLMs’ instruction-following capabilities. However, acquiring such data through human annotation remains cost-prohibitive and difficult to scale, hindering further progress. Using LLMs generated instruction aligned synthetic data have often shown to neglect downstream use cases.\nSo how can we tailor synthetic data to align LLMs for different instruction-following tasks?\nTo address this researchers have introduced CodecLM, a general framework for adaptively generating high quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles CodecLM works as follows: First, the strong LLM (fs) encodes the seed instruction into instruction metadata, specifying its use case and skills required for responses. Next, fs decodes metadata into basic instructions. Meanwhile, Self-Rubrics leverages fs to generate rubrics and actions to improve the basic instruction, tailoring them for the downstream task. Finally, Contrastive Filtering uses a scoring function S to compare fs and ft’s responses. The most effective pairs are selected for aligning the LLM, while less effective instructions are sent for further improvement.\nCodecLM was evaluated on various benchmarks including EvolInstruct, Vicuna, Self-Instruct and Koala and against various methods Self-Instruct, Alpagasus, Tree-Instruct, WizardLM and WizardLM+. All methods were trained on LLaMA-7B or 13B as the target LLM and compared against Gemini-Pro, the strong LLM that generates the data. CodecLM outperforms against all methods consistently on all benchmarks, with both the target LLMs.\nPaper: https://arxiv.org/pdf/2404.05875"
  },
  {
    "objectID": "posts/IN2/IN2.html",
    "href": "posts/IN2/IN2.html",
    "title": "Make Your LLM Fully Utilize the Context",
    "section": "",
    "text": "These days the training context windows of many contemporary LLMs have been expanded to tens of thousands of tokens, thereby enabling these models to process extensive context as input. However, recent studies have revealed that these long-context LLMs struggle to effectively and robustly utilize all the information provided in the context, known as the lost-in-the-middle challenge. This mainly stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.\nSo how can we make long-context LLMs fully utilize the information in the long context?\nTo address this researchers have introduced INformation-INtensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. The IN2 training aims to explicitly teach the model that any position in a long context can contain crucial information. To achieve this goal, a long-context question-answer training dataset D = {Li , qi , ai}, is constructed based on a general natural language corpus C where the answer ai to the question qi requires the information contained in some short segments (∼128 tokens) that are randomly placed in the whole long context Li (ranging from 4K to 32K tokens). Two types of question-answer pairs are generated which require (1) the awareness of fine-grained information in the long context, and (2) the integration and reasoning of information appearing at different positions in the long context.\nDuring experimentation IN2 training method was applied on Mistral-7B model with realnewslike subset from the C4 corpus as training corpus, and GPT-4-Turbo as the stronger LLM to generate QA pairs and a new model was introduce called FILM-7B (FILlin-the-Middle). Further, FILM-7B was evaluated on three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5→26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3→59.2 accuracy on MMLU).\nPaper : https://arxiv.org/pdf/2404.16811"
  },
  {
    "objectID": "posts/PoLL/PoLL.html",
    "href": "posts/PoLL/PoLL.html",
    "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
    "section": "",
    "text": "Evaluating language models is a challenging task: not only is it difficult to find meaningful data to test the models, but evaluating the correctness of a generated response is itself a challenge. You will find that LLM evaluation benchmark tests such as MMLU or automatic evaluation metrics such as BLEU score for machine translation and ROUGE for summarization commonly fail to analyze the intended property of interest.\nRecently, model-based scoring or LLM “as Judge” found to be performing better than above heuristic metrics. However, using LLMs like GPT4 for evaluation tend to have their own biases; often recognizing and preferring their own outputs over those of other models and also much more expensive.\nSo is there any better way to automatically evaluate LLMs?\nCohere has come up with a new approach called Panel of LLm evaluators (PoLL) that replaces a single LLM “as Judge” with multiple LLMs “Juries” and uses vote to get the best result. PoLL consists of a larger number of smaller models called Juries and a voting function to aggregate the score across these Juries and finally Cohen’s Kappa correlation is used to compare results with human preferences.\nDuring experimentation, a PoLL was constructed from three models drawn from three disparate model families (Command R, Haiku, and GP 3.5). Further, two different voting functions for aggregating scores across the judges were used. For QA datasets, max voting was used as all judgements are binary [correct, incorrect]. For Chatbot Arena average pooling methods were used because judgements are scores ranging from 1-5 and a three judge panel often does not produce a clear majority decision. Finally, Cohen’s Kappa correlations were used to compare results with human preferences. Cohen’s kappa Correlation measures inter-rater reliability, which quantifies the level of agreement between two or more raters or judges.\nPoLL outperforms single judges (GPT-4) across multiple datasets, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive (7-8x cheaper than GPT-4).\nPaper : https://arxiv.org/pdf/2404.18796"
  },
  {
    "objectID": "posts/Octopusv4/Octopusv4.html",
    "href": "posts/Octopusv4/Octopusv4.html",
    "title": "Octopus v4: Graph of language models",
    "section": "",
    "text": "LLMs have been effective in a wide range of applications, yet the most sophisticated models are often proprietary (GPT 4, Gemini) and considerably costly than open source ones. However, recently niche-specific smaller language models, such as those tailored for legal, medical or financial tasks, have outperformed their proprietary counterparts considerably.\nIs there any way to integrate all these LLMs in order to manage multiple downstream tasks without compromising performance and staying within budget?\nThat is what researcher from nexa4ai has done with Octopus v4, a novel approach that employs functional tokens to integrate multiple open-source models, each optimized for particular tasks. Octopus v4 model leverages functional tokens to intelligently direct user queries to the most appropriate vertical model and reformat the query to achieve the best performance.\nThe architecture consists of two abstraction layers. The first layer utilizes functional tokens to represent the actions executable by the Octopus v2 model. This layer encompasses three distinct Octopus v2 models, each identified by different functional tokens, effectively differentiating them as separate AI agents. The second layer of abstraction pertains to the Octopus v4 model, where internal functional tokens are mapped to various v2 models.\nFramework features a graph of language models with a master node deployed on a central device and worker nodes distributed across various devices. Kubernetes (k8s) is employed for serverless deployment of each individual worker language model. For efficient data sharing, a distributed cache mechanism supported by Redis is used. Note that for each worker node, a small Octopus v4 Lora is attached to guide it to the next neighbor node for the case of multi-Agent use cases. During evaluation it was found that the model achieved a SOTA MMLU score of 74.8 among the same 10B level models.\nPaper : https://arxiv.org/pdf/2404.19296"
  },
  {
    "objectID": "posts/PROMETHEUS2/PROMETHEUS2.html",
    "href": "posts/PROMETHEUS2/PROMETHEUS2.html",
    "title": "PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    "section": "",
    "text": "Proprietary LMs such as GPT-4 model-based evaluation have emerged as a scalable solution for assessing LM-generated text. However, concerns related to transparency, controllability, and affordability these proprietary LMs have led to the development of open source LMs specialized in evaluations. But, these open source evaluator language models fall short in two key areas: they often diverge from human scores and lack flexibility to perform common evaluation methods like direct assessment and pairwise ranking. They also lack the ability to evaluate based on custom criteria, focusing on general attributes instead.\nTo address these issues, researchers have introduced Prometheus 2, a more powerful evaluator LM that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. To achieve this, researchers have merged the weights of two evaluator LMs trained separately on direct assessment and pairwise ranking formats and this weight merging have yield an evaluator LM that not only works in both formats, but also outperforms evaluator LMs that are jointly trained or only trained on a single format.\nFor Prometheus 2 researchers use Mistral-7B and Mixtral8x7B as base models, and merge the weights of evaluator LMs separately trained on the FEEDBACK COLLECTION, a direct assessment feedback dataset, and the PREFERENCE COLLECTION dataset, a new fine-grained pairwise ranking feedback dataset that builds on the FEEDBACK COLLECTION, to obtain resulting models, PROMETHEUS 2 (7B & 8x7B).\nOn four direct assessment benchmarks (Vicuna Bench, MT Bench, FLASK, Feedback Bench), the PROMETHEUS 2 models demonstrate the highest correlation with both human evaluators and proprietary LM-based judges compared to existing open evaluator LMs, with the Pearson correlation surpassing other baselines by 0.2 units across all datasets. Similarly, on four pairwise ranking benchmarks (HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench), the PROMETHEUS 2 models show the highest agreement with human evaluators among all the open evaluator LMs we tested, reducing the performance gap with GPT-4 in half.\nPaper : https://arxiv.org/pdf/2405.01535"
  },
  {
    "objectID": "posts/NeMo-Aligner/NeMo-Aligner.html",
    "href": "posts/NeMo-Aligner/NeMo-Aligner.html",
    "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
    "section": "",
    "text": "Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters.\nIn order to simplify LLM alignment issues, Nvidia has launched NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, our toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort.\nNeMo-Aligner addresses scalability challenges by (1) building upon Megatron-LM with 3D (data, tensor, and pipeline)-parallelism training, (2) having a distributed approach to Proximal Policy Optimization (PPO) training in RLHF and (3) integrating PPO inference optimizations based on TensorRT-LLM during rollout stage. Combined, these optimizations allow users to efficiently train the largest models over hundreds of GPUs reducing research iteration time.\nPaper : https://lnkd.in/gMh2vqqc\nGithub : https://lnkd.in/gvqsWZQq"
  },
  {
    "objectID": "posts/Multi-token Prediction/Multi-token Prediction.html",
    "href": "posts/Multi-token Prediction/Multi-token Prediction.html",
    "title": "Better & Faster Large Language Models via Multi-token Prediction",
    "section": "",
    "text": "All Large language models such as GPT and Llama are trained with a next-token prediction loss. However, despite the recent wave of impressive achievements in LLMs, next-token prediction remains an inefficient way of acquiring language, world knowledge and reasoning capabilities. More precisely, LLM training are more focus on next-token prediction based on local patterns and overlooks “hard” decisions.\nSo is there any way we can improve next-token prediction capabilities of LLMs without any additional training ?\nIt looks like AI @ Meta has a solution for it by moving away from traditional next-token prediction to a multi-token prediction strategy, demonstrating that LLMs can achieve better outcomes without additional training time. In order to do so Meta has introduced multi-token prediction architecture with no train time or memory overhead. During training, the model predicts 4 future tokens at once, by means of a shared trunk and 4 dedicated output heads. During inference, it employs only the next-token output head. Optionally, the other three heads may be used to speed-up inference time.\nOne big challenge in training multi-token predictors is reducing their GPU memory utilization. To overcome this Meta multi-tone prediction architecture carefully adapt the sequence of forward and backward operations. By performing the forward/backward on an n-token prediction model with n = 2 heads in sequential order, it avoids materializing all unembedding layer gradients in memory simultaneously and reduces peak GPU memory usage. During inference multiple independent output heads of the architecture, each predicting a future token, allowing parallel token prediction there by speed up decoding from the next-token prediction head with self-speculative decoding methods such as blockwise parallel decoding.\nDuring evaluation, 13B models trained on multi-token prediction architecture solved 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3× faster at inference, even with large batch sizes.\nPaper : https://arxiv.org/pdf/2404.19737"
  },
  {
    "objectID": "posts/LayerSkip/LayerSkip.html",
    "href": "posts/LayerSkip/LayerSkip.html",
    "title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding",
    "section": "",
    "text": "Optimizing LLMs operational cost and computation requirement is one of the sortout topics for researchers. Accelerated solutions deploy on mobile, edge devices or commodity GPUs such as laptops do exist but they suffer from significant drop in accuracy since a large portion of these LLM acceleration approaches focus on reducing the number of non-zero weights, number of bits per weight or number of heads per layer.\nSo is there any way we can deploy accelerated LLMs solutions economically without sacrificing accuracy ?\nTo address this AI @ Meta has introduced the LayerSkip method, an end-to-end solution to speed-up inference of large language models (LLMs), which reduces the number of layers required for each token by exiting early during inference. Unlike quantization or sparsity, acceleration by reducing the number of layers does not require specialized hardware or software kernels. LayerSkip solution work as follow\nFirst, during training layer dropout is applied, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit.\nSecond, during inference, the training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model.\nThird, a self-speculative decoding novel solution is utilize where it exists at early layers and verifies and corrects with remaining layers of the model.This self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages.\nDuring evaluation experiments were run on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. LayerSkip inference solution have show speedups of up to 2.16× on summarization for CNN/DM documents, 1.82× on coding, and 2.0× on TOPv2 semantic parsing tasks.\nPaper : https://lnkd.in/dWNjW52i"
  },
  {
    "objectID": "posts/Flash Attention Stable/Flash Attention Stable.html",
    "href": "posts/Flash Attention Stable/Flash Attention Stable.html",
    "title": "Is Flash Attention Stable?",
    "section": "",
    "text": "Given the size and complexity of workloads, training Large Language Models (LLMs) often takes months together, across hundreds or thousands of GPUs. For example, LLaMA2’s 70-B parameter model, took 1,720,320 GPU hours to train. With such long training jobs, training instability has become increasingly problematic. As reported in works such as Google’s PaLM model, training instability often manifests itself in the form of loss spikes occurring up to 20 times throughout training. These loss spikes are costly, as they often cause interrupts in the training process, requiring training to stop and restart\nOne under-explored potential cause of training instability is numeric deviation. Numeric deviation between an optimization and its corresponding baseline can lead to the gradual accumulation of errors, which over the course of training have the potential to culminate in loss spikes that require a resetting of the model state.\nTo understand this numeric deviation in training optimizations researchers from Meta developed a principled quantitative approach consists of two phases (1) a numerical microbenchmark of the Flash Attention operation was developed, which allows for the experimentation of different numerical precisions, as well as the testing of various optimizations throughout the algorithm. this framework allows for the direct comparison of the Attention Matrix output between Baseline Attention, Flash Attention, and numeric re-implementation. (2) a data-driven analysis based on Wasserstein distance were used to contextualize this numeric difference via examining model weight changes over the course of training.\nAfter applying the above framework on widely-adopted Flash Attention optimization it was found that flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass. Further, data-driven analysis based on the Wasserstein Distance have provided upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training.\nIn conclusion, investigations underscore the importance of developing a principled approach to not only quantify, but contextualize, the impact of training optimizations on numeric deviation. By constructing proxies to put this numeric deviation in context, this paper aims to reason about the likelihood of downstream model effects (i.e training instability) that are traditionally difficult to measure.\nPaper: https://lnkd.in/gpSdZu99"
  },
  {
    "objectID": "posts/GraphRAG /GraphRAG.html",
    "href": "posts/GraphRAG /GraphRAG.html",
    "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
    "section": "",
    "text": "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. RAG works great for explicit retrieval tasks but fails during query focused summarization (QFS) tasks such as global questions directed at an entire text corpus.\nTo address this, Microsoft Research has come up with a new approach, GraphRAG, which uses the LLM to create a knowledge graph based on the private dataset. GraphRAG approach uses an LLM to build a graph-based text index in two stages: (1) a knowledge graph from source documents is created, (2) then generating summaries for groups of closely-related entities. When a question is posed, these summaries are used to create partial responses, which are then combined into a final answer for the user.\nA typical Graph RAG pipeline uses an LLM-derived graph index of source document text. This index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset. Community detection is used to partition the graph index into groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries reporting relevance to that query.\nUnlike previous methods that utilize the structured retrieval and traversal capabilities of graph indexes, Graph RAG prioritizes an untapped aspect of graphs: their built-in modularity. It emphasizes the capacity of community detection algorithms to divide graphs into cohesive clusters of closely-connected nodes. LLM-generated summaries of these community descriptions provide complete coverage of the underlying graph index and the input documents it represents. Query-focused summarization of an entire corpus is then made possible using a map-reduce approach: first using each community summary to answer the query independently and in parallel, then summarizing all relevant partial answers into a final global answer.\nPaper : https://arxiv.org/pdf/2404.16130v1"
  },
  {
    "objectID": "posts/SUPRA/SUPRA.html",
    "href": "posts/SUPRA/SUPRA.html",
    "title": "Linearizing Large Language Models",
    "section": "",
    "text": "Over the last few years, Transformers have displaced Recurrent Neural Networks (RNNs) in sequence modeling tasks, owing to their highly parallel training efficiency and unmatched scaling performance. However, this training efficiency comes at the cost of inference cost that scales linearly with the number of tokens, compared to the fixed-cost inference of RNNs.\nTo overcome this limitation researchers have proper various novel methods such as Linear Transformer which combine good of both worlds i.e, train models with sequence parallelism (i.e. as transformers), but operate as RNNs at inference time. State-space models (SSMs) such as Mumba show impressive performance at smaller scales, matching or exceeding the performance of softmax transformers. However, a gap remains for long-context NLU tasks, showing a persistent advantage of softmax attention.\nRather than pre-training linear models, can we convert an existing transformer into an RNN ?\nWell that is what researchers have done with Scalable UPtraining for Recurrent Attention (SUPRA), a linearization strategy to up train state-of-the-art LLMs into performant RNNs, enabling the study of the strengths and limitations of recurrent models at scale with minimal compute cost. SUPRA replaced the softmax normalization with GroupNorm (GN) and introduced a small MLP to project the queries and keys, converting a pre-trained attention block (left) to a linear attention (right). The model can be trained in parallel as a transformer and used recurrently at inference time with a mathematically equivalent reformulation. This allows it to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost.\nCompared to pre-training linear models from scratch, the SUPRA strategy produces competitive models comparable to the best available recurrent LLMs (RWKV and Mamba) at the 7B scale. Not only the strengths of linear models on standard NLU benchmarks was identify but also the enduring limitations on in-context (i.e. MMLU) and long-context (NarrativeQA, Qasper) tasks, showing that linearized models do not inherit these capabilities from the base softmax transformers.\nPaper : https://arxiv.org/pdf/2405.06640\nCode : https://github.com/TRI-ML/linear_open_lm"
  },
  {
    "objectID": "posts/SUTRA/SUTRA.html",
    "href": "posts/SUTRA/SUTRA.html",
    "title": "SUTRA: Scalable Multilingual language model architecture",
    "section": "",
    "text": "Recent advancements in Large Language Models (LLMs) have predominantly focused on a limited set of data-rich languages, with training datasets being notably skewed towards English. Most of the existing multilingual LLMs models often suffer from significant trade-offs between performance, efficiency, and scalability, particularly when extending support across a broader spectrum of languages. Models such as BLOOM and Llama2, typically underperform in languages that are less represented in the training data due to the difficulty of balancing language-specific nuances whereas language-specific LLMs like HyperClova in Korean or OpenHaathi in Hindi are bit challenging due to the exponential data and training requirements.\nTo overcome this challenge researchers have introduced SUTRA (Sanskrit for “thread”), a transformative approach in the architecture of multilingual LLMs. SUTRA is a novel multilingual large language model architecture that is trained by decoupling concept learning from language learning. The input is processed through a multilingual concept encoder, followed by the concept model and finally through a multilingual concept decoder to generate the output response.This architecture enables the core model to focus on universal language agnostic concepts while leveraging specialized neural machine translation (NMT) mechanisms for language-specific processing, thus preserving linguistic nuances without compromising the model’s scalability or performance.\nFurther, SUTRA employs a Mixture of Experts (MoE) strategy, enhancing the model’s efficiency by engaging only the relevant experts based on the linguistic task at hand. MoE Layer is configured in such a way that the Input vectors are routed to a subset of the available experts, specifically 2 out of 8, by a specialized router. The aggregate output of this layer is the sum of the individual outputs, each weighted accordingly. Each expert comprises a feedforward module similar to those found in conventional transformer models.\nIn conclusion, a combination of multilingual skills, online connectivity, and efficiency in language generation incorporated by SUTRA models promises to redefine the landscape of multilingual language modeling.\nPaper : https://arxiv.org/pdf/2405.06694"
  },
  {
    "objectID": "posts/Layer-Condensed KV Cache/Layer-Condensed KV Cache.html",
    "href": "posts/Layer-Condensed KV Cache/Layer-Condensed KV Cache.html",
    "title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models",
    "section": "",
    "text": "Key-value (KV) cache is one of the most significant parts of any transformer based LLM model and takes over 30% of the GPU memory during deployment. Hence KV cache plays a critical role in deciding overall LLMs throughput and latency. Recently, various work has been focused on improving KV cache performance either by prompt compression or caching strategies such as cache evacuation and sequence caching.\nNow researchers have proposed a novel perspective that is orthogonal to previous efforts: Layer-Condensed KV Cache, a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput.\nLayer-Condensed KV Cache, a new variant of transformer decoders in which queries of all layers are paired with keys and values of just the top layer so that model do not have to cache or even compute KVs for layers other than the top layer, saving both memory consumption and computation. Furthermore, since models no longer need to compute KVs for these layers, nor do they need to keep the weights WK, WV that map hidden representations to KVs for these layers, thus also saving model parameters.\nDuring experiments on Llama show that this model achieves up to 32× larger batch sizes and up to 26× higher throughput than standard transformers for LLMs of 1B–30B parameters; at the same time, the model has competitive performance to standard transformers in language modeling and downstream tasks. In addition, this method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency.\nPaper : https://lnkd.in/d3XcDA4Z"
  },
  {
    "objectID": "posts/Xmodel-VLM/Xmodel-VLM.html",
    "href": "posts/Xmodel-VLM/Xmodel-VLM.html",
    "title": "Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model",
    "section": "",
    "text": "Recently small-scale visual language models performance have come in par with its larger-scale counterparts. Models such as LLaVAPhi [47], which combines the open source multi-modal model LLaVA-1.5 and the open source small language model Phi-2(2.7B) have shown great promise to improve multi-modal model resource efficiency. However, despite the encouraging advancements made in the realm of visual language models, the pursuit of a genuinely optimal harmony between performance and efficiency remains an active and ongoing challenge.\nTo address this researchers have introduced Xmodel-VLM, a cutting-edge multimodal vision language model designed for efficient deployment on consumer GPU servers. The architecture of Xmodel-VLM, closely mirrors that of LLaVA-1.5. It consists of three key components: (1) a vision encoder : a pre-trained CLIP ViT-L/14 with a resolution of 336×336 (2) a lightweight language model (LLM) : an lightweight language model Xmodel-LM 1.1B from scratch based on LLaVA architecture. (3) a projector responsible for aligning the visual and textual spaces. It utilizes a two-layer MLP to strengthen the link between the vision encoder and LLM, employing the Mish activation function. This innovative projector, known as XDP, acts as both a connection enhancer and a downsampling mechanism, reducing visual tokens by 75%. XDP’s design emphasizes simplicity and effectiveness.\nThe training of Xmodel_VLM occurs in two main stages: Stage I - Pre-training: In this stage, the vision encoder is kept frozen The XDP projector and the LLM (Language Model) are both learnable. Stage II - Multi-task Training: Similar to Stage I, the vision encoder remains frozen, while both the XDP projector and the LLM are learnable. This stage involves training the model on multiple tasks simultaneously.\nDuring evaluation across a variety of datasets: VizWiz, SQAI, VQA, POPE, GQA, MMB, MMBCN, MMVet and MME Xmodel-VLM 1.1B demonstrates competitive performance against Qwen-7B, LLaMA-7B and Vicuna-13B despite having small model parameter.\nPaper : https://lnkd.in/gBK9W-2H"
  },
  {
    "objectID": "posts/Zamba/zamba.html",
    "href": "posts/Zamba/zamba.html",
    "title": "Zamba: A Compact 7B SSM Hybrid Model",
    "section": "",
    "text": "Recently, State-of-the-art Transformer-SSM hybrid Architecture has been a driving force in Open source LLMs. Inline with such trends researchers from Zyphra have launched Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale.\nThe Zamba architecture consists of a backbone of standard Mamba blocks connected to a shared attention and MLP block. This block is repeated every 6 Mamba blocks but has shared parameters, which enables Mamba to utilize more FLOPs for increased performance at the same memory cost. The input embeddings are always concatenated with the residual stream going into the shared attention block as this provides an additional path for the model to remember the inputs. After the block, a learnt linear projection maps the output back to the residual stream.\nDue to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay\nZamba is also the highest-performing SSM in the small 7B model range and the highest-performing dense SSM model available. Zamba matches state-of-the-art 7B models on many linguistic evals, while lagging slightly behind on tests of reasoning and in context learning, which may be due to the significant data disparity between Zamba and other leading ∼7B models.\nPaper : https://arxiv.org/pdf/2405.16712"
  },
  {
    "objectID": "posts/VeLoRA/VeLoRA.html",
    "href": "posts/VeLoRA/VeLoRA.html",
    "title": "VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections",
    "section": "",
    "text": "LLM Training and finetuning are still far too computationally and memory intensive tasks. Several techniques have been proposed to reduce these memory requirements, such as GaLore, gradient checkpointing, reversible backpropagation, parameter-efficient finetuning, quantization and activation offloading. While these methods are promising and lower the memory cost, they also might introduce a substantial computational overhead, are limited in their memory savings, or require specialized hardware.\nTo address these challenges researchers have introduced a novel approach for efficient training and finetuning, called Vector projected LoRA (VeLoRA). VeLoRA simply divides the tokens up into smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace during the forward pass. These features are then coarsely reconstructed during the backward pass to implement the update rules. By compressing and then reconstructing the activations on the fly, VeLoRA reduces the peak activation memory footprint to a tiny fraction of what is required to store the original activations. This enables fitting much larger models into limited GPU memory compared to approaches like GaLore or gradient checkpointing.\nVeLoRA memory-efficient algorithm consists of two components: (i) The grouping strategy to divide the original high-dimensional tokens into much smaller sub-tokens; and (ii) Fixed rank-1 projections of these sub-tokens using cheap heuristically initialized principal components. Given a large pre-trained model, above steps are applied to compress the intermediate activations saved during training while preserving most of the original model’s training dynamics.\nVeLoRA was evaluated on both moderately-sized vision transformers as well as in large language models. VeLoRA was found to significantly reduce memory requirements while improving the performance effectiveness on VTAB-1K, MMLU, GLUE, and C4 benchmarks outperforming state-of-the-art methods such as LoRA, QLoRA or GaLore.\nPaper : https://arxiv.org/pdf/2405.17991"
  },
  {
    "objectID": "posts/Nest/Nest.html",
    "href": "posts/Nest/Nest.html",
    "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
    "section": "",
    "text": "Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts.\nTo address these challenges researchers from Meta have introduced Nearest Neighbor Speculative Decoding (Nest), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources.\nThe Nest approach first locates the tokens in the corpus using the LM hidden states. The retrieval distribution pk-NN is dynamically interpolated with pLM based on the retriever’s uncertainty λt. The token and its n-gram continuation are then selected from the mixture distribution pM, while the final span length is determined by speculative decoding to remove undesired tokens. The spans incorporated in the final generation provide direct attribution and amortize the generation latency.\nAt each inference step, Nest performs content generation with three sub-steps: * Confidence-based interpolation: Adjusts output probabilities using a Relative Retrieval Confidence score, allowing dynamic adaptation to different tasks. * Dynamic span selection: Extends token selection to include a span of text when confidence in retrieval exceeds a threshold. * Relaxed speculative decoding: Evaluates selected spans based on mixture probability, accepting only highly probable prefixes.\nNest significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, Nest substantially improves the generation speed, achieving a 1.8× speedup in inference time when applied to Llama-2-Chat 70B.\nPaper : https://arxiv.org/pdf/2405.17976"
  },
  {
    "objectID": "posts/METRAG/METRAG.html",
    "href": "posts/METRAG/METRAG.html",
    "title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi–layered Thoughts",
    "section": "",
    "text": "Retrieval-augmented generation (RAG) has been pencil in pushing LLM use cases in the Knowledge management system. Nevertheless, existing retrieval-augmented generation approaches are typically similarity-based i.e., they retrieve documents from external corpus based on similarity. Simply aggregating the Top-k document without considering the relationships between them makes it difficult to capture the commonalities and characteristics among them and even confuse LLMs due to excessive text length thus incurring information loss and probably performance degradation\nHence similarity is not always the “panacea” and totally relying on similarity would sometimes degrade the performance of retrieval-augmented generation.\nTo address this researchers have come up with a novel approach called METRAG, a Multi–layEred Thoughts enhanced RetrievalAugmented Generation framework. METRAG endows retrieval-augmented generation with multi-layered thoughts by firstly embracing LLM’s supervision for utility-oriented thoughts and combining similarity and utility of documents for performance boosting and further pursuing compactness oriented thoughts via a task-adaptive summarizer, finally incorporating the derived multi-layered thoughts for answer generation.\nIn general, METRAG combines similarity- and utility-oriented approaches. It involves using a similarity model and a utility model to generate summaries, then distilling summary skills from a powerful teacher model (like GPT4). Finally, multiple generated summaries are evaluated using a reward model to improve alignment with the desired task. Extensive experiments on knowledge intensive tasks have demonstrated the superiority of the proposed METRAG.\nPaper : https://arxiv.org/pdf/2405.19893"
  },
  {
    "objectID": "posts/CoPE/CoPE.html",
    "href": "posts/CoPE/CoPE.html",
    "title": "Contextual Position Encoding: Learning to Count What’s Important",
    "section": "",
    "text": "The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but the attention mechanism inherently lacks ordered information and treats sequences as sets. Thus, it is necessary to have an additional mechanism for encoding position information. Position encoding (PE) achieves this by assigning an embedding vector to each position and adding that to the corresponding token representations.\nIn order to tie position measurement to more semantically meaningful units such as words, or sentences, one needs to take context into account. However, this is impossible with current PE methods as position addressing is computed independently of the context, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence.\nTo address these challenges researchers have proposed a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the i-th particular word, noun, or sentence.\nCoPE first determines which tokens to count using their context vectors. Specifically, given the current token as a query vector, CoPE computes a gate value for each previous token using their key vectors. Then it aggregates those gate values to determine the relative position of each token with respect to the current token. Unlike token positions, this contextual position can take fractional values, thus cannot have an assigned embedding. Instead, it interpolate embeddings that are assigned to integer values to compute position embeddings. Like the other PE methods, these position embeddings are then added to the key vectors, so a query vector can use them in the attention operation. Since contextual position can vary from query-to-query and layer-to-layer, the model can simultaneously measure distances in multiple units.\nDuring evaluation CoPE were applied to several toy tasks: counting, selective copying and the Flip-Flop task, where it outperforms token-based PE methods, especially in the case of out-of-domain generalization. To test real-world applicability, a language modeling task on Wikipedia text where applied with CoPE leading to better performance. The same performance gain is also observed when trained on code.\nPaper : https://arxiv.org/pdf/2405.18719"
  },
  {
    "objectID": "posts/MMLU-Pro/MMLU-Pro.html",
    "href": "posts/MMLU-Pro/MMLU-Pro.html",
    "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
    "section": "",
    "text": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. MMLU includes a broad range of exam questions from 57 subjects across STEM, the humanities, the social sciences, etc. However, the rapid progress of current LLMs has quickly led to performance saturation on MMLU. Since GPT-4 achieved 86.4% in March 2023, there has not been any significant progress on the benchmark. Even GPT-4o achieved 1% improvement on MMLU to obtain 87.4%.\nThis is due to the structure of the Multiple-Choice Machine Reading Comprehension (MMLU) dataset which provides only three options, potentially allowing LLMs to guess answers without understanding. Further, MMLU questions are often more about knowledge recall than reasoning, especially in STEM subjects, making them easier. Finally, some questions are unanswerable or wrongly annotated, limiting the dataset’s usefulness and affecting model performance.\nTo address these challenges researchers have introduced MMLU-Pro: a comprehensive benchmark designed for proficient-level multi-discipline language understanding and reasoning. MMLU-Pro spans 14 diverse domains including mathematics, physics, chemistry, law, engineering, psychology, and health, encompassing over 12,000 questions and thus meeting the breadth requirement. MMLUPro is distinctive from MMLU in the following aspects: * 1. MMLU-Pro has ten options, which contain 3x more distractors than MMLU. By increasing the distractor numbers, it significantly reduces the probability of correct guess by chance to boost the benchmark’s difficulty and robustness. * 2. MMLU-Pro increases the portion of challenging college-level exam problems. These questions require LLM to perform deliberate reasoning in different domains to derive the final answer. * 3. MLU-Pro integrates two rounds of expert reviews to reduce the noise of the dataset. The first round is based on expert verification. In the second round, it utilizes the SoTA LLMs to identify potential errors and employ annotators to perform more targeted verification.\nExperimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. In conclusion, MMLU-Pro is a more discriminative benchmark to better track progress of both present and upcoming LLMs.\nPaper : https://arxiv.org/pdf/2406.01574"
  },
  {
    "objectID": "posts/DITTO/DITTO.html",
    "href": "posts/DITTO/DITTO.html",
    "title": "Show, Don’t Tell: Aligning Language Models with Demonstrated Feedback",
    "section": "",
    "text": "Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, alignment can be challenging, especially for the largest and most competent LLMs which often require a large corpus of (un)acceptable behavior (on the order of ≈ 1K samples.\nIs it possible to align an LLM to a specific setting by leveraging a very small number (&lt; 10) of demonstrations as feedback ?\nWell that is what researchers from Stanford have addressed with Demonstration ITerated Task Optimization (DITTO), a framework for aligning LLMs to specific settings by providing a small number of demonstrations be drawn from a user’s existing interaction logs, or from direct edits made to LLM outputs. DITTO, scaffolds a handful of these demonstrations (&lt; 10) into a substantial dataset of preference comparisons, by treating users’ demonstrations as preferred over model output from both the original LLM and models’ earlier training iterations. This augmented dataset of demonstration-grounded comparisons can then be used to update the language model using an alignment algorithm like DPO.\nIn general, DITTO iteratively aligns LLMs to demonstrated behavior. When a user supplies demonstrations (through edits to a model’s output, past preferred interaction history, or writing examples from scratch), DITTO treats these demonstrations as preferred to all model behavior, including earlier iterations of the trained model. Using demonstrations as feedback allows for cheap generation of online comparison data and enables few-shot alignment with just a handful of samples.\nDITTO’s ability was evaluated to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, a user study was conducted soliciting a range of demonstrations from participants (N = 16). Across these benchmarks and user study, it was found that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.\nPaper : https://arxiv.org/pdf/2406.00888"
  },
  {
    "objectID": "posts/Block Transformer/Block Transformer.html",
    "href": "posts/Block Transformer/Block Transformer.html",
    "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
    "section": "",
    "text": "Generating tokens with transformer-based autoregressive language models (LMs) is costly due to the self-attention mechanism that attends to all previous tokens. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference.\nTo address this issue researchers have introduced Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. The Block Transformer architecture introduces a novel approach to modeling global dependencies by using self-attention between coarse blocks at lower layers and decoding fine-grained tokens within local blocks at upper layers. Here’s how it works:\n\nEmbedding Blocks: The architecture first embeds each block of input tokens into an input block embedding using a module called the embedder.\nBlock Decoder: This is an autoregressive transformer that operates on the block embeddings. It applies self-attention between blocks to decode a context block embedding, which contains information necessary for predicting the next block.\nToken Decoder: This component autoregressively decodes the token contents of the next block, applying local self-attention between only the tokens within the block. It relies solely on the output block embedding for global context information.\n\nThis design significantly reduces self-attention costs, making them linear to the total context length, and eliminates the need to prefill prompt tokens during inference.\nBy leveraging global and local modules, the Block Transformer architecture demonstrates 10–20x gains in inference throughput compared to vanilla transformers with equivalent perplexity. Block Transformer architecture introduces a new approach to optimize language model inference through novel application of global-to-local modeling.\nPaper : https://arxiv.org/pdf/2406.02657\nCode : https://github.com/itsnamgyu/block-transformer"
  },
  {
    "objectID": "posts/BoT/BoT.html",
    "href": "posts/BoT/BoT.html",
    "title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models",
    "section": "",
    "text": "Recently, various prompting methods such as CoT, ToT and GoT have been instrumental in improving reasoning performance of LLMs. All these methods can be broadly divided into two categories:\n\nsingle-query reasoning: these methods usually focus on prompt engineering and their reasoning process can be finished within a single query, such as CoT that appends the input query with ’Let’s think step by step’ to produce rationales for increasing reasoning accuracy.\nmulti-query reasoning: these methods focus on leveraging multiple LLM queries to elicit different plausible reasoning paths, thus decomposing a complex problem into a series of simpler sub-problems, such as Least-to-Most ToT and GoT.\n\nHowever, both single-query and multi-query reasoning processes are limited by their designed examples and reasoning structures, and they neglect to derive general and high-level guidelines or thoughts from previously-completed tasks.\nTo address these limitations, researchers have proposed Buffer of Thoughts (BoT), a novel and versatile thought augmented reasoning framework aimed at enhancing reasoning accuracy, efficiency and robustness of LLMs across various tasks. BoT uses a meta-buffer, a lightweight library housing a series of universal high-level thoughts (thought-template), which are distilled from different problem-solving processes and can be shared across tasks. Then, for each problem, a relevant thought template is retrieved and instantiated with a specific reasoning structure for efficient thought-augmented reasoning. In order to guarantee the scalability and stability of BoT, a buffer-manager is used to dynamically update the meta-buffer, which effectively enhances the capacity of meta-buffer as more tasks are solved.\nDuring extensive experiments on 10 challenging reasoning-intensive tasks BoT have shown significant performance improvements over previous SOTA methods: 11% on Game of 24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis demonstrates the superior generalization ability and model robustness of BoT, while requiring only 12% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Notably, it was found that Llama3-8B + BoT has the potential to surpass the Llama3-70B model in reasoning-intensive tasks.\nPaper : https://arxiv.org/pdf/2406.04271\nCode : https://github.com/YangLing0818/buffer-of-thought-llm"
  },
  {
    "objectID": "posts/Mixture-of-Agents/Mixture-of-Agents.html",
    "href": "posts/Mixture-of-Agents/Mixture-of-Agents.html",
    "title": "Mixture-of-Agents : Enhances Large Language Model Capabilities",
    "section": "",
    "text": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. However, despite the plethora of LLMs and their impressive achievements, they still face inherent constraints on model size and training data. At the same time, different LLMs possess unique strengths and specialize in various tasks.\nSo can we harness the collective expertise of multiple LLMs to create a more capable and robust model?\nAnswer lies in collaborativeness of LLMs — wherein an LLM tends to generate better responses when presented with outputs from other models. Based on these findings researchers have introduced the Mixture Of-Agents (MoA) methodology that leverages multiple LLMs to iteratively enhance the generation quality.\nThe structure of MoA is as follows: Initially, LLMs in the first layer, let say agents A1,1, …A1,n independently generate responses to a given prompt. These responses are then presented to agents in the next layer A2,1, …A2,n (which may reuse a model from the first layer) for further refinement. This iterative refinement process continues for several cycles until obtaining a more robust and comprehensive response.\nTo ensure effective collaboration among models and improve overall response quality, careful selection of LLMs for each MoA layer is crucial. This selection process is guided by two primary criteria: (a) Performance Metrics: The average win rate of models in layer i plays a significant role in determining their suitability for inclusion in layer i + 1. Therefore, selecting models based on their demonstrated performance metrics ensures higher-quality outputs. (b) Diversity Considerations: The diversity of model outputs is also crucial. Responses generated by heterogeneous models contribute significantly more than those produced by the same model. By leveraging these criteria — performance and diversity — MoA aims to mitigate individual model deficiencies and enhance overall response quality through collaborative synthesis.\nA comprehensive evaluation of the MoA framework was done using AlpacaEval 2.0, MT-Bench, FLASK benchmarks for assessing the response quality across various dimensions. The results demonstrate substantial improvements, achieving a new SOTA win rate of 65.8% on AlpacaEval 2.0 compared to the previous best of 57.5% achieved by GPT-4 Omni.\nPaper : https://arxiv.org/pdf/2406.04692"
  },
  {
    "objectID": "posts/HUSKY/HUSKY.html",
    "href": "posts/HUSKY/HUSKY.html",
    "title": "HUSKY: A Unified, Open-Source Language Agent for Multi-Step Reasoning",
    "section": "",
    "text": "Recent advances in the capabilities of large language models (LLMs) have led to the development of language agents to address complex, multi-step tasks. However, most existing agents are based on proprietary models or designed to target specific tasks, such as mathematics or multi-hop question answering.\nTo address this, researchers have introduced HUSKY, a holistic, open-source language agent that learns to reason over a unified action space to address a diverse set of complex tasks involving numerical, tabular, and knowledge-based reasoning. HUSKY solves these multi-step tasks by jointly predicting the next high-level step and tool with an action generator, and executing the action with the assigned expert model. This process repeats until it arrives at the final answer.\nHUSKY iterates between two stages. The first module in HUSKY is the action generator. Given the input question and the solution generated so far, the action generator jointly predicts the next high-level step to take and the associated tool. The tools forming the ontology of actions are [code], [math], [search] and [commonsense]. If the final answer to the question has been reached in the solution history, then the action generator returns the answer. Based on the tool assigned by the action generator, HUSKY calls the corresponding tool, executes the tool and re-writes the tool outputs optionally into natural language. Each tool is associated with an expert model - a code generator for [code], a math reasoner for [math], a query generator for [search] and a commonsense reasoner for [commonsense].\nDuring experimentation HUSKY generalizes across multiple tasks better than other language agents including FIREACT and LUMOS, and outperforms other agents in tasks of their own expertise. For example, HUSKY outperforms LUMOS on GSM-8K [6] by more than 20 points and FIREACT on HotpotQA [53] by 5 points. HUSKY also outperforms FINMA [51] on FinQA [5] by 9 points and CRITIC-70B [11] on TabMWP [21] by 1.8 points. On HUSKYQA, HUSKY with a 13B action generator scores within 1 points behind gpt-4o.\nIn conclusion, these results showcase a robust recipe for developing HUSKY, an open-source language agent that generalizes and achieves competitive performance across a wide array of multi-step reasoning tasks.\nPaper : https://arxiv.org/pdf/2406.06469"
  },
  {
    "objectID": "posts/TEXTGRAD/TEXTGRAD.html",
    "href": "posts/TEXTGRAD/TEXTGRAD.html",
    "title": "TEXTGRAD : Automatic “Differentiation” via Text",
    "section": "",
    "text": "There is an emerging paradigm shift in how AI systems are built these days. The new generation of AI applications are increasingly compound systems involving multiple sophisticated components, where each component could be an LLM-based agent, a tool such as a simulator, or web search. As a result, developing principled and automated optimization methods for such compound AI systems is one of the most important new challenges.\nTo optimize the new generation of AI systems, researchers from Stanford have introduced TEXTGRAD, a powerful framework performing automatic “differentiation” via text. Here we use differentiation and gradients as a metaphor for textual feedback from LLMs. In this framework, each AI system is transformed into a computation graph, where variables are inputs and outputs of complex (not necessarily differentiable) function calls. The feedback to the variables (dubbed ‘textual gradients’) are provided in the form of informative and interpretable natural language criticism to the variables; describing how a variable should be changed to improve the system. The gradients are propagated through arbitrary functions, such as LLM API calls, simulators, or external numerical solvers.\nIn general, TEXTGRAD backpropagation textual feedback provided by LLMs to improve individual components of a compound AI system. In TEXTGRAD framework, LLMs provide rich, general, natural language suggestions to optimize variables in computation graphs, ranging from code snippets to molecular structures. TEXTGRAD follows PyTorch’s syntax and abstraction and is flexible and easy-to-use. It works out-of-the-box for a variety of tasks, where the users only provide the objective function without tuning components or prompts of the framework.\nTEXTGRAD’s effectiveness and generality were evaluated across a diverse range of applications, from question answering and molecule optimization to radiotherapy treatment planning. Without modifying the framework, TEXTGRAD improves the zero-shot accuracy of GPT-4o in Google-Proof Question Answering from 51% to 55%, yields 20% relative performance gain in optimizing LeetCode-Hard coding problem solutions, improves prompts for reasoning, designs new druglike small molecules with desirable in silico binding, and designs radiation oncology treatment plans with high specificity. TEXTGRAD lays a foundation to accelerate the development of the next-generation of AI systems.\nPaper : https://arxiv.org/pdf/2406.07496\nCode : https://github.com/zou-group/textgrad"
  },
  {
    "objectID": "posts/Circuit Breaking/Circuit Breaking.html",
    "href": "posts/Circuit Breaking/Circuit Breaking.html",
    "title": "Improving Alignment and Robustness with Circuit Breakers",
    "section": "",
    "text": "The landscape of artificial intelligence (AI) has long been marred by the persistent threat of adversarial attacks, particularly those targeting neural networks. The rise of generative models has further complicated this issue. Generative models such as large language models (LLMs) can output copyrighted information or defame individuals, and agents can take harmful actions. Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks.\nNow researchers have come up with an alternative method called Circuit Breaking that fundamentally diverges from traditional defenses: instead of attempting to remove vulnerabilities to specific attacks, Circuit Breaker aims to directly circumvent the ability of the model to produce the harmful output in the first place.\nIt does so by using representation engineering (RepE), which connects the internal representations related to harmful outputs to circuit breakers so that when a model begins to generate such an output, its internal processes are interrupted, halting completion of the generation. Or this method is “short-circuiting” the harmful processes as one might put it. Because the representation used to generate a harmful output is independent of any attack capable of eliciting it, this approach is attack-agnostic, and sidesteps the need for additional training, costly adversarial fine tuning, or the use of auxiliary “guard”.\nWith circuit breakers, models are intrinsically safer and reduce their risks by removing intrinsic model hazards—their ability to produce harmful outputs—rather than removing specific vulnerabilities with adversarial training, and rather than attempting to reduce exposure to attacks with input filters.\nAdding circuit breakers using Representation Rerouting (RR) to refusal trained Llama-3- 8B-Instruct model leads to significantly lower attack success rate (ASR) over a wide range of unseen attacks on HarmBench prompts, while its capabilities on standard LLM benchmarks (MT Bench and MMLU) are largely preserved. RR directly targets the representations that give rise to harmful outputs and reroutes them to an orthogonal space. This reliably interrupts the model from completing the harmful generations even under strong adversarial pressure.\nPaper : https://arxiv.org/pdf/2406.04313"
  },
  {
    "objectID": "posts/LC-Boost/LC-Boost.html",
    "href": "posts/LC-Boost/LC-Boost.html",
    "title": "Improving Alignment and Robustness with Circuit Breakers",
    "section": "",
    "text": "Large language models (LLMs) have been instrumental in pushing the boundaries of various real-world applications mostly which are associated with long-sequence inputs, such as long-document question answering and summarization. Unfortunately, the learning and deployment of long context LLMs are still challenging mostly due to cost and overall accuracy.\nSo are long-context tasks are short-context solvable ?\nTo address this researchers have proposed a novel framework called LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the long-context tasks in a bootstrapping manner. LC-Boost operates with two critical reasoning steps: 1) Access : how to access the appropriate part of context within the input, 2) Utilize: how to make effective use of the accessed context. By adaptively accessing and utilizing the context based on the presented tasks, LC-Boost can serve as a general framework to handle diversified long-context processing problems.\nLet’s take the toy examples to better illustrate the mechanism of LC-Boost. There are two common approaches to tackle long-context problems: (A) the brute-force method based on long-LLMs, (B) the surrogate methods, like Naive RAG. Despite correctness, brute-force method is unnecessarily expensive due to the processing of the entire context simultaneously. whereas in case of Naive RAG It is hard to handle problems like information aggregation, which leads to the incomplete answer.\nIn contrast, LC-Boost is able to handle general long-context tasks thanks to the proper reasoning of how to access and utilize the long-context information based on each specific task (C), it can directly access the needed information via retrieval and generate the answer based on RAG. Further LG-Boot can also process the entire context in a divide-and-conquer manner (D), processes the long-context via sequential scan, which correctly solves the problem based on the comprehensively collected information.\nLC-Boost underwent extensive evaluation across real-world and synthetic tasks, including question-answering and summarization of lengthy documents. Results showed it matched or even exceeded the performance of brute-force methods, like GPT-4-128K, often due to its ability to eliminate irrelevant context. Notably, LC-Boost outperformed short-LLM surrogates with predefined context access, highlighting the significance of reasoning and adaptability in its success.\nPaper : https://arxiv.org/pdf/2405.15318"
  },
  {
    "objectID": "posts/segment auction/segment auction.html",
    "href": "posts/segment auction/segment auction.html",
    "title": "Ad Auctions for LLMs via Retrieval Augmented Generation",
    "section": "",
    "text": "Large language models (LLMs) have been making headway in various domains and now also in the field of computational advertising. Now with the integration of ads into the outputs of LLMs have presented an opportunity to support these services without compromising content integrity.\nTo do so researchers have introduced segment auction, a novel auction mechanism for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). In segment auction an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the RAG framework, and priced according to competing bids.\nIn simple terms, when a query is submitted by a user, relevant ads together with bids are retrieved from a database. The retriever forwards the bids to the auction, along with click probabilities (aligned with retrieval probabilities). The auction implements a randomized allocation rule based on these inputs, following the RAG framework, and the LLM bases its output on the winning ad. The auction can be run repeatedly for each segment, or it can compute several winners for multiple segments at once.\nSuch a RAG-based allocation rule is optimal for single-ad allocation per segment, focusing on a balanced efficiency and fairness, crucial for satisfying users and generating ad revenue in LLM outputs. For multi-ad allocation per segment a randomized RAG allocation rule is derived from deterministic truthful auctions, ensuring it is truthfully implementable. The main device for single- and multi-ad allocation is to perturb the bids with random additive offsets, drawing on ideas from discrete choice methods.\nDuring evaluation segment auctions were validated for feasibility and effectiveness over several ad auction scenarios using publicly available LLM APIs. While comparing single- and multi-allocation segment auctions against each other: repeated single-ad segment auctions found to generate higher revenue, where as multi-allocation auction leads to higher output quality, as measured by the cosine similarity between embeddings of the output omitting ads, and the output conditioned on ads.\nPaper : https://arxiv.org/pdf/2406.09459"
  },
  {
    "objectID": "posts/THEANINE/THEANINE.html",
    "href": "posts/THEANINE/THEANINE.html",
    "title": "THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation",
    "section": "",
    "text": "Nowadays Large language models (LLMs) with large context windows are capable of processing lengthy dialogue histories during prolonged interaction with users without additional memory modules; however, their responses tend to overlook or incorrectly recall information from the past. Such response failures are mainly due to (a) bias to the latest input (b) the absence of an important past event on the timeline.\nMotivated by these, researchers have introduced THEANINE, a framework of timeline-augmented chain-of-thought reasoning for response generation in long-term conversations. THEANINE, augments LLMs’ response generation with memory timelines – series of memories that demonstrate the development and causality of relevant past events. THEANINE manages memories with a graph structure, and follows below phases.\nIn Phase I, THEANINE connects each memory in a graph based on their relationships, forming a network where similar memories are linked together. It leverages an LLM to dynamically link memories based on their temporal and cause-effect common sense relations.\nIn Phase II-1, during memory retrieval for generating responses, THEANINE retrieves the entire memory timeline that represents the sequence of relevant events. In Phase II-2, to bridge the gap between offline memory construction and online deployment, THEANINE refines this retrieved timeline to provide tailored information that is most relevant to the ongoing conversation.\nFinally, in Phase III, for response generation, THEANINE leverages LLMs’ chain-of-thought (CoT) reasoning ability to reason over current conversation and refined timelines, conclude useful information, and generate the final response.\nTHEANINE was evaluated against Multi-Session Chat (MSC) and Conversation Chronicles (CC) long-term conversations datasets and under various Machine and Human Evaluation scenarios. In all such scenarios THEANINE responses that are more detail-rich, utilizes past memories efficiently and correctly, responses where human thinking makes sense and properly references past conversations.\nPaper : https://arxiv.org/pdf/2406.10996"
  },
  {
    "objectID": "posts/AGILE CODER/AGILE CODER.html",
    "href": "posts/AGILE CODER/AGILE CODER.html",
    "title": "THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation",
    "section": "",
    "text": "Software agents have emerged as promising tools for addressing complex software engineering tasks. However, existing works oversimplify software development workflows by following the waterfall model.\nCan software agents mimic real word software development workflow ?\nIntroducing AGILECODER, a multi-agent system that integrates Agile Methodology (AM) into the framework. This system assigns specific AM roles—such as Product Manager, Developer, and Tester—to different agents, who then collaboratively develop software based on user inputs. AGILECODER enhances development efficiency by organizing work into sprints, focusing on incrementally developing software through sprints.\nAGILECODER uses multiple agent such as Product Manager (PM), Scrum Master (SM), Developer (Dev), Senior Developer (SD) and Tester. Each role can be dynamically assigned and executed. The workflow starts with the Product Manager planning the backlog based on user requirements. The Scrum Master then reviews and finalizes the backlog, leading into development sprints consisting of Planning, Development, Testing, and Review phases. After evaluating progress, the Scrum Master decides on software readiness for delivery. If necessary, the process repeats in subsequent sprints until the software is deemed deliverable. at which point a termination signal concludes the development pipeline.\nThe development process incorporates the Execution Environment for running code during testing and the Dynamic Code Graph Generator (DCGG) for dynamically generating the Code Dependency Graph whenever the code is updated. The Execution Environment provides tracebacks to agents for code refinement, while the DCGG enables agents to retrieve relevant contexts for accurate code generation and correction.\nAGILECODER surpasses existing benchmarks, like ChatDev and MetaGPT, establishing a new standard and showcasing the capabilities of multi-agent systems in advanced software engineering environments.\nPaper : https://arxiv.org/pdf/2406.11912"
  },
  {
    "objectID": "posts/Self-MoE/Self-MoE.html",
    "href": "posts/Self-MoE/Self-MoE.html",
    "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts",
    "section": "",
    "text": "Today’s almost all LLMs are predominantly designed as monolithic architectures, these models rely extensively on large-scale data to embed generalized language capabilities across vast parameter spaces. This makes these LLMs susceptible to forgetting previously learned information when adapted to specialized tasks.\nHow can we build compositional LLMs that enjoy versatile expertise, while using minimal resources?\nIntroducing Self-MoE, an approach that transforms a monolithic model into a compositional modular system of self-specialized experts, called MiXSE (MiXture of Self-specialized Experts). Unlike LoRA, Self-MoE constructs individual lightweight expert modules from scratch using synthetic data. Each module is integrated with the base LLM, and the entire system is enhanced by a self-optimized routing mechanism. This allows for dynamic and capability-specific handling of various target tasks, enhancing overall capabilities, without extensive human-labeled data and added parameters, which LoRA required.\nSelf-MoE approach to building a compound system of specialized experts and a router in a self-improving manner and done in two phases: In the Self-Specialization phase, the base LLM is aligned with self-generated synthetic data for each target specialization, producing lightweight expert modules. In MiXSE where each self-specialized expert is dynamically engaged based on the decisions of the self-optimized router.\nEmpirical results show that Self-MoE demonstrates substantial improvements over the base LLM across diverse benchmarks such as knowledge, reasoning, math, and coding. It also consistently outperforms other methods, including instance merging and weight merging, while offering better flexibility and interpretability by design with semantic experts and routing.\nPaper: https://arxiv.org/pdf/2406.12034"
  },
  {
    "objectID": "posts/Whiteboard-of-Thought/Whiteboard-of-Thought.html",
    "href": "posts/Whiteboard-of-Thought/Whiteboard-of-Thought.html",
    "title": "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities",
    "section": "",
    "text": "Large language models have shown promising results in arithmetic and symbolic reasoning by expressing intermediate reasoning in text as a chain of thought, yet struggle to extend this capability to answer text queries that are easily solved by visual reasoning, even with extensive multimodal pretraining.\nSuch visual reasoning tasks demand visuals and one can leverage the abilities of multimodal large language models (MLLMs) to achieve this. By providing MLLMs the ability to create and reason with explicit visuals – like a whiteboard showing intermediate thoughts – unlocks capabilities resembling visual thinking.\nOn the same key idea researcher have introduced whiteboard-of-thought (WoT)’: a novel prompting method, which provide MLLMs with a metaphorical ‘whiteboard’ to draw out the results of intermediate reasoning steps as images, then prompt them to use their visual input capabilities to produce answers or perform further reasoning from the visuals made by the model itself. It is interesting to note that WoT leveraging models’ existing ability to write code with visual graphics libraries such as Turtle and Matplotlib proves sufficient to create visuals useful for solving visual reasoning tasks without requiring a single example.\nIn general WoT provides the MLLM with the input prompt - You write code to create visualizations using the {Matplotlib/Turtle} library in Python, which the user will run and provide as images. Do NOT produce a final answer to the query until considering the visualization - along with the query. The model then decides what visualization code to write based on the query. The resulting code is then passed to a runtime environment to render it in image form. In this case, one can use the Python interpreter with the previously mentioned visualization libraries.\nThis simple approach shows state-of-the-art results on four difficult natural language tasks that involve visual and spatial reasoning. Multiple settings were identified where GPT-4o using chain-of-thought fails dramatically, including more than one where it achieves 0% accuracy, while whiteboard-of-thought enables up to 92% accuracy in these same settings.\nPaper : https://arxiv.org/pdf/2406.14562"
  },
  {
    "objectID": "posts/LongRAG/LongRAG.html",
    "href": "posts/LongRAG/LongRAG.html",
    "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
    "section": "",
    "text": "In the traditional RAG framework, the basic retrieval units are normally short but the retriever needs to scan over a massive amount of units to find the relevant piece. Such an imbalanced puts too much pressure on the retriever, which needs to recall a huge amount of units, such as the top-100 or even more, combined with additional complex re-ranker to achieve great performance. Moreover, short retrieval units can lead to semantic incompleteness due to document truncation. This can lead to information loss, ultimately restricting the end performance.\nIn order to alleviate the imbalance, a new framework is proposed called LongRAG, which places greater emphasis on recall, aiming to retrieve relevant context with much coarse granularity. This shifts more burden from the retriever to the reader to extract the exact answers from the relevant context.\nLongRAG consists of two components: the “long retriever” and the “long reader”. Long Retriever will identify coarse relevant information for the given query by searching through all the long retrieval units in the corpus. The top 4 to 8 retrieval units are concatenated as the retrieved long context for the next step.\nLong Reader, here long reader will further extract answers from the concatenation of retrievals, which is normally around 30K tokens. Finally, a simple prompt is used with an existing long-context LM (like Gemini or GPT4) with the question to produce the answers.\nLongRAG processes the entire Wikipedia into 4K-token units, which is 30x longer than before. By increasing the unit size, it significantly reduces the total units from 22M to 600K. This significantly lowers the burden of retriever, which leads to a remarkable retrieval score: answer recall@1=71% on NQ (previously 52%) and answer recall@2=72% (previously 47%) on HotpotQA (full-wiki).\nThen the top-k retrieved units (≈ 30K tokens) are fed to an existing long-context LLM to perform zero-shot answer extraction. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA (full-wiki), which is on par with the SoTA model. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.\nPaper : https://arxiv.org/pdf/2406.15319"
  },
  {
    "objectID": "posts/Instruction Pre-Training/Instruction Pre-Training.html",
    "href": "posts/Instruction Pre-Training/Instruction Pre-Training.html",
    "title": "Instruction Pre-Training: Language Models are Supervised Multi Task Learners",
    "section": "",
    "text": "Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, Instruction tuning significantly enhances task generalization re-emphasizing the value of supervised multitask learning.\nSo can we utilize supervised multitask learning during pre-training ? To explore this researcher have introduced Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models (typically with 7B parameters). Instead of directly pre-training on raw corpora, Instruction PreTraining augments the corpora with instruction-response pairs generated by an instruction synthesizer, then pretrains LMs on the augmented corpora.\nDuring tuning, the instruction synthesizer learns to generate instruction-response pairs for a given raw text. The tuning data are curated to be highly diverse, enabling the synthesizer to generalize to unseen data. During inference, this tuned instruction synthesizer is used to generate instruction-response pairs for raw texts from pre-training corpora. This efficiency allows scale up task synthesis: augmenting the raw corpora with 200M instruction-response pairs across more than 40 task categories.\nFor evaluation, experiments were conducted in both general pretraining from scratch and domain-adaptive continual pre-training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3- 70B.\nPaper : https://arxiv.org/pdf/2406.14491\nCode : https://github.com/microsoft/LMOps"
  },
  {
    "objectID": "posts/LLM-Compiler/LLM_Compiler.html",
    "href": "posts/LLM-Compiler/LLM_Compiler.html",
    "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
    "section": "",
    "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored.\nTo address this gap, Meta has introduced a Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques.\nLLM Compiler models are specialized from Code Llama by training on 546 billion tokens of compiler-centric data in two stages. In the first stage the models are trained predominantly on unlabelled compiler IRs and assembly code. In the next stage the models are instruction fine-tuned to predict the output and effect of optimizations. LLM Compiler FTD models are then further fine-tuned on 164 billion tokens of downstream flag tuning and disassembly task datasets, for a total of 710 billion training tokens. During each of the four stages of training, 15% of data from the previous tasks is retained.\nMost interesting thing that stud out was model input (Prompt) and output (Label) during training and inference. The prompt contains unoptimized code. The label contains an optimization pass list, binary size, and the optimized code. To generate the label for the training prompt, the unoptimized code is compiled against multiple random pass lists. The pass list achieving the minimum binary size is selected, minimized and checked for correctness with PassListEval. The final pass list together with its corresponding optimized IR are used as labels during training. In a last step, the top 100 most often selected pass lists are broadcast among all programs. For deployment only the optimization pass list was generated which was fed into the compiler, ensuring that the optimized code is correct.\nPaper : link"
  },
  {
    "objectID": "posts/AutoRAG-HP/AutoRAG-HP.html",
    "href": "posts/AutoRAG-HP/AutoRAG-HP.html",
    "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) has emerged as a prominent framework for building ML/AI solutions with LLMs. Additional modules such as query rewriting, prompt compression, and query routing have been integrated with RAG to enhance its performance. However, such integration has led to additional complexity namely hyper-parameters within the modules and incorporating human-in loop feedback in the modules.\nTo address such challenges researchers have proposed the AutoRAGHP framework, which formulates the hyperparameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces.\nAutoRAG-HP uses two-level Hier-MAB in the context of jointly tuning of top-k (K), embedding model (E), and compression ratio (C) hyper-parameters. The high-level arm is responsible for selecting which hyper-parameter to tune, while the lower-level arms control the hyperparameter selection within the search space of each hyper-parameter.\nAfter pulling the two-level arms and observing the associated reward, the algorithm updates its estimate of the selected arm’s reward distribution using the new information, i.e., updating the mean reward estimation and the confidence interval based on the observed reward. Meanwhile, the reward distributions of other high- and low-level arms pulled in previous iterations also get updated. This process repeats for a predetermined number of iterations or until a stopping criterion is met.\nAutoRAG-HP was evaluated on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. AutoRAG-HP demonstrates that MAB-based online learning methods can achieve Recall@5 ≈ 0.8 for scenarios with prominent gradients in search space, using only ∼ 20% of the LLM API calls required by the Grid Search approach.\nPaper : https://arxiv.org/pdf/2406.19251"
  },
  {
    "objectID": "posts/MIRAI/MIRAI.html",
    "href": "posts/MIRAI/MIRAI.html",
    "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
    "section": "",
    "text": "Recent advancements in Large Language Models (LLMs) have enabled LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems such as predicting international events. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents’ forecasting capability and reliability.\nTo address this gap, researchers from UCLA Caltech have introduced MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. MIRAI benchmark consists of a collection of 991,759 GDELT event records, corresponding to 59,161 unique events and 296,630 unique news articles. The test set contains 705 query and answer pairs on forecasting an event of a given timestamp between two countries, with a 100 balanced test subset.\nLet consider LLM agent’s framework interaction with the multi-source environment using the ReAct strategy for forecasting a query event. The framework consists of three main steps: (1) Think: The agent analyzes the current status and plans the next action based on the query and the provided API specifications. (2) Act: The agent generates a Single Function call or a Code Block to retrieve and analyze relevant data from the database. (3) Execute: The Python interpreter runs the generated code with the API implementation and database and produces observations. These steps are iteratively performed until the agent reaches a final forecast for the future relation.\nNow MIRAI comprehensively evaluates the agents’ capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events.\nThrough comprehensive benchmarking, MIRAI strives to assess the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.\nPaper : https://arxiv.org/pdf/2407.01231"
  },
  {
    "objectID": "posts/MInference/MInference.html",
    "href": "posts/MInference/MInference.html",
    "title": "MInference: a Million-token inference on a single A100 machine",
    "section": "",
    "text": "The computational challenges of LLM inference remain a significant barrier to their widespread deployment, especially as context lengths continue to increase. Existing efficient methods for long-context LLMs have focused on KV-cache compression, static sparse attention (e.g., model compression, SSM, linear attention), or distributed serving. However, these methods struggle to achieve acceptable latency for million-token level prompts with low cost and a single A100 GPU.\nTo address this gap, researchers from Microsoft have introduced MInference, a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Attention, especially in long-context scenarios, is sparse and dynamic, i.e., the sparse patterns are largely different across inputs. This dynamic sparsity presents three unique spatial aggregation patterns that persist for all inputs: A-shape, Vertical-Slash, and Block-Sparse—that can be leveraged for efficient sparse computation on GPUs.\nMInference is a training-free efficient method for the pre-filling stage of long-context LLMs based on dynamic sparse attention. Specifically, leverage three types of static spatial aggregation patterns of dynamic sparse attention: A-shape, Vertical-Slash, and Block-Sparse. MInference first determines the optimal dynamic sparse pattern for each head offline using the Kernel-Aware Sparse Pattern Search algorithm. During inference, it dynamically approximates the dynamic sparse indices based on the head’s pattern. Finally, efficient dynamic sparse attention is computed using optimized GPU kernel.\nFor example, with the Vertical-Slash pattern, first the attention calculation between the last Q (Query vector) and K (Key vector) is used to estimate the optimal indices of vertical lines and slash lines. Then, the dynamic sparse compiler PIT and Triton are utilized to construct the Vertical-Slash FlashAttention kernel, accelerating the attention computation. For the A-shape, Vertical-Slash, and Block-Sparse patterns, first the mean pooling of Q and K in attention calculations is used. By leveraging the commutative property of mean pooling and MatMul, the block-spa is estimated. Then, Triton construct the Block-Sparse FlashAttention kernel, accelerating the attention computation.\nBy evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, Yi-200K, GLM-4-1M, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy.\nPaper : https://arxiv.org/pdf/2407.02490"
  },
  {
    "objectID": "posts/Best Practice RAG/BestPracticeRAG.html",
    "href": "posts/Best Practice RAG/BestPracticeRAG.html",
    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
    "section": "",
    "text": "Retrieval-augmented generation (RAG) techniques have proven to be effective in enhancing LLMs response quality, particularly in specialized domains. While many RAG approaches have been proposed, most of them still suffer from their complex implementation and prolonged response times.\nSo what are the best practices in Retrieval-Augmented Generation ?\nBefore we answer this let’s understand a typical RAG workflow, which mainly consists of following. Query Classification : Classify queries to determine the necessity of retrieval. Queries requiring retrieval proceed through the RAG modules; others are handled directly by LLMs.\nChunking : Chunk documents into smaller segments and is crucial for enhancing retrieval precision and avoiding length issues in LLMs. Below are the widely used chunking methods * Token-level Chunking is straightforward but may split sentences, affecting retrieval quality. * Semantic-level Chunking uses LLMs to determine breakpoints, context-preserving but time consuming. * Sentence-level Chunking balances preserving text semantics with simplicity and efficiency\nChunk Size : Larger chunks provide more context, enhancing comprehension but increasing process time. Smaller chunks improve retrieval recall and reduce time but may lack sufficient context. Typical sizes are 2048, 1024, 512, 256 and 128.\nChunking Techniques : like small-to-big and sliding windows improve retrieval quality by organizing chunk block relationships. Small-sized blocks are used to match queries, and larger blocks that include the small ones along with contextual information are returned.\nEmbedding Model : is crucial for effective semantic matching of queries and chunk blocks.\nMetadata Addition : Enhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve retrieval during hybrid search.\nVector Databases : store embedding vectors with their metadata, enabling efficient retrieval of documents relevant to queries through various indexing and ANN methods. Vector DB is usually selected based on four key criteria: multiple index types, billion-scale vector support, hybrid search, and cloud-native capabilities\nRetrieval Methods : Given a user query, the retrieval module selects the top-k relevant documents from a pre-built corpus based on the similarity between the query and the documents . Following 3 query retrieval method is used\nQuery Rewriting: refines queries to better match relevant documents. Inspired by the * Query Decomposition: retrieving documents based on sub-questions derived from the original query. * Pseudo-documents Generation: generates a hypothetical document based on the user query and uses the embedding of hypothetical answers to retrieve similar documents e.g. HyDE Combination of above method is also used such as HyDE with Different Concatenation of Documents and Query and Hybrid Search with Different Weight on Sparse Retrieval\nReranking Methods : enhance the relevance of the retrieved documents, ensuring that the most pertinent information appears at the top of the list. Two approaches are widely used * DLM Reranking: leverages deep language models (DLMs) for reranking * TILDE Reranking: calculates the likelihood of each query term independently by predicting token probabilities across the model’s vocabulary.\nDocument Repacking : module rearranged document after reranking using three methods: Forward method repacks documents by descending relevancy scores from the reranking phase. Reverse arranges them in ascending order and Slide placed at the head or tail of the input.\nSummarization : retrieved documents are crucial in the RAG pipeline. Summarization tasks can be Extractive, which segment text into sentences, then score and rank them based on importance or Abstractive, which compressors synthesize information from multiple documents to rephrase and generate a cohesive summary. Recomp, LongLLMLingua and Selective Context are some of the widely used methods.\nSearching for Best RAG Practices : * Query Classification Module: This module is referenced and contributes to both effectiveness and efficiency * Retrieval Module: “Hybrid” or “Original” methods are recommended, as they reduce latency while maintaining comparable performance. * Reranking Module: enhancing the quality of generated responses. * Repacking Module: The Reverse configuration exhibited superior performance * Summarization Module: Recomp demonstrated superior performance,\nThe experimental results demonstrate that each module contributes uniquely to the overall performance of the RAG system. The query classification module enhances accuracy and reduces latency, while the retrieval and reranking modules significantly improve the system’s ability to handle diverse queries. The repacking and summarization modules further refine the system’s output, ensuring high-quality responses across different tasks.\nPaper : https://arxiv.org/pdf/2407.01219"
  },
  {
    "objectID": "posts/Adam-mini/Adam-mini.html",
    "href": "posts/Adam-mini/Adam-mini.html",
    "title": "Adam-mini: Use Fewer Learning Rates To Gain More",
    "section": "",
    "text": "Adam(W) has become the de-facto optimizer for training large language models (LLMs). Despite its superior performance, Adam is expensive to use. Specifically, Adam requires the memory for its optimizer states: the first-order momentum m, and the second order momentum v. These in total take at least 2× the memory of the model size . This memory consumption has become a major burden in LLM training. For instance, to train a 7B model, Adam alone requires about 56 GB per GPU for m and v, and with the gradients included, a total of 86 GB is needed.\nTo address these challenges researchers have proposed Adam-mini, an optimizer that achieves on-par or better performance than AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., 1/ √ v). It was found that ≥ 90% of these learning rates in v could be harmlessly removed by partitioning the gradient vector into B sub-vectors according to the dense Hessian sub-blocks, and call it gb for b = [B]. For each gb, the following quantity is calculated below.\nvb = (1 − β2) ∗ mean(gb ◦ gb) + β2 ∗ vb, b = 1, · · · B\nThen η/√ vb were used as the learning rate for the parameters associated with gb and this corresponding method is Adam-mini that can outperform Adam, provided that sufficient resources are available to search it out.\nEmpirically, during verification Adam-mini performs on par or better than AdamW on various language models sized from 125M to 7B for pre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini also alleviates communication overheads among GPUs, thereby increasing throughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW when pre-training Llama2-7B on 2× A800-80GB GPUs, which saves 33% wall-clock time for pre-training.\nPaper : https://arxiv.org/pdf/2406.16793"
  },
  {
    "objectID": "posts/CLA/CLA.html",
    "href": "posts/CLA/CLA.html",
    "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
    "section": "",
    "text": "Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes.\nMulti-Query Attention (MQA) and Grouped-Query Attention (GQA) are some of the widely used methods to reduce the size of the KV cache. Both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy.\nNow the researcher from MIT-IBM has come up with another novel attention design called Cross-Layer Attention (CLA). CLA shares key and value heads between adjacent layers, yielding 2× reduction in KV cache while maintaining nearly the same accuracy as unmodified MQA.\nIn traditional attention, each layer computes its own separate K and V activations, which must be cached on a per-layer basis during autoregressive decoding. Whereas CLA computes key/value projections for only a subset of layers in the model; the attention blocks in layers without key/value projections reuse the KV activations of previous layers. Only the subset of layers with key/value projections contribute to the KV cache, allowing a reduction in memory footprint relative to traditional architectures which apply a separate key/value projection in each layer.\nIn experiments training 1B- and 3B-parameter models from scratch, CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, enabling inference with longer sequence lengths and larger batch sizes than would otherwise be possible.\nPaper : https://arxiv.org/pdf/2405.12981"
  },
  {
    "objectID": "posts/Composable Interventions/Composable Interventions.html",
    "href": "posts/Composable Interventions/Composable Interventions.html",
    "title": "Composable Interventions for Language Models",
    "section": "",
    "text": "Language models (LMs) exhibit striking capabilities on various important tasks but despite such high performance, LMs generated content are usually prone to be hallucinatory, factually incorrect, and harmful. To address this many recent works are focused on in-place updates for LMs called Interventions, which aim to update targeted properties of LMs applied, without impacting unrelated behaviors or adding excessive compute, after pretraining (and optional fine-tuning).\nUsually a LLM might require multiple such interventions over a time for inference- or memory-efficiently, knowledge editing, detoxification, and unlearning. To handle such requirements researchers have come up with a novel method called composable interventions, a framework to study the effects of using multiple interventions on the same language models, featuring new metrics and a unified codebase.\nWhen an intervention is applied to a model, it should not interfere with prior or future interventions. For example if a model is quantized it should not affect its knowledge editing intervention which was applied earlier. To manage this composable interventions framework use two metrics for composability: 1) Order-free Error, where an intervention is composable if its application leaves others’ success unimpacted, and 2) Order Sensitivity, where the combined success of multiple interventions should not depend on the order in which they are applied.\nDuring extensive experimentation, frameworks were evaluated on composing popular methods from three emerging intervention categories—knowledge editing, model compression, and machine unlearning. Results from 310 different compositions uncover meaningful interactions: compression hinders editing and unlearning, composing interventions hinges on their order of application, and popular general-purpose metrics are inadequate for assessing composability. Taken together, findings showcase clear gaps in composability, suggesting a need for new multi-objective interventions.\nPaper : https://arxiv.org/pdf/2407.064830"
  },
  {
    "objectID": "posts/ARMT/ARMT.html",
    "href": "posts/ARMT/ARMT.html",
    "title": "Associative Recurrent Memory Transformer",
    "section": "",
    "text": "Long sequence LLMs are some of the challenging models to work around as memory plays a crucial role processing extremely long contexts and utilizing remote past information. Various methods have emerged for extending transformers context length, including approaches based on transformer segment-level recurrence but none are able to successfully mitigate it.\nNow researchers have come up with a novel model called Associative Recurrent Memory Transformer (ARMT), a memory-augmented segment-level recurrent Transformer based on Recurrent Memory Transformer. Compared to RWKV and Mamba, which use association-based techniques, ARMT benefits from full local self attention and has constant time and space complexity of processing new segments, similar to RMT.\nDuring experimentation ARMT outperformed existing alternatives in associative retrieval tasks and set a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%.\nPaper : https://arxiv.org/pdf/2407.04841\nCode : https://github.com/RodkinIvan/associative-recurrent-memory-transformer"
  },
  {
    "objectID": "posts/FlashAttention-3/FlashAttention-3.html",
    "href": "posts/FlashAttention-3/FlashAttention-3.html",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
    "section": "",
    "text": "FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is used across various libs to accelerate Transformer training and inference. However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU (Hopper).\nNow researchers have introduced FlashAttention-3 which makes use of the following three new features of Hopper, using powerful abstractions from NVIDIA’s CUTLASS library. • WGMMA (Warpgroup Matrix Multiply-Accumulate), which makes use of the new Tensor Cores on Hopper, with much higher throughput than the older mma.sync instruction in Ampere (A100). • TMA (Tensor Memory Accelerator), a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound prediction • Low-precision with FP8. This doubles the Tensor Core throughput (e.g. 989 TFLOPS with FP16 and 1978 TFLOPS with FP8), but trades off accuracy by using fewer bits to represent floating point numbers.\nOverall, FlashAttention-3 utilize three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.\nFlashAttention-3, by incorporating above technique, 1.5-2.0x faster than FlashAttention-2 with FP16, up to 740 TFLOPS, i.e., 75% utilization of H100 theoretical max FLOPS. With FP8, FlashAttention-3 reaches close to 1.2 PFLOPS, with 2.6x smaller error than baseline FP8 attention.\nPaper : https://tridao.me/publications/flash3/flash3.pdf\nCode : https://github.com/Dao-AILab/flash-attention"
  },
  {
    "objectID": "posts/Metron/Metron.html",
    "href": "posts/Metron/Metron.html",
    "title": "Metron: Holistic Performance Evaluation Framework for LLM Inference Systems",
    "section": "",
    "text": "Serving large language models (LLMs) in production can incur substantial costs, which has prompted recent advances in inference system optimizations. Today, these systems are evaluated against conventional latency and throughput metrics such as\n\nTTFT : Time To First Token (TTFT) is the latency between the request arrival and the first output token generated by the system for the request. It includes the scheduling delay (time elapsed from request arrival to start of prompt processing) and the prompt processing time.\nTBT : Time Between Tokens (TBT) is the latency of every subsequent token generation in the decode phase. This metric directly influences the perceived speed of the model by users.\nTPOT : Time Per Output Token (TPOT) is the average time to generate an output token in the decode phase. It is calculated as the total decode time of a request normalized by the number of decode tokens generated.\nNormalized Latency : This is defined as the total execution time of a request normalized by the number of decode tokens. It includes the scheduling delay, prompt processing time and time to generate all the decode tokens\nCapacity : This is defined as the maximum request load (queries-per-second) a system can sustain while meeting certain latency targets (SLOs).\n\nHowever, these metrics fail to fully capture the nuances of LLM inference, leading to an incomplete assessment of user-facing performance crucial for real-time applications such as chat and translation.\nTo address the limitations of existing metrics, researchers have introduced Metron, a comprehensive framework for evaluating user-facing performance in LLM inference. At its core are two novel metrics:\n\nfluidity-index : When a request arrives in the system, it sets the deadlines for all future tokens. If a token is produced before the set deadline, the slack is carried forward and serves as a buffer for future tokens. When a token arrives late, the system gets penalized for all the missed deadlines, and the subsequent deadlines are reset to account for the autoregressive decoding process.\nfluid token generation rate complements fluidity-index by determining the maximum sustainable playback rate that maintains a specified level of fluidity (e.g., fluidity-index &gt; 0.9). That way fluid token generation rate enables black-box evaluation of LLM inference systems.\n\nCombined, these metrics provide a holistic view of LLM inference performance that more closely aligns with real-world user experience.\nPaper : https://arxiv.org/pdf/2407.07000"
  },
  {
    "objectID": "posts/AgentInstruct/AgentInstruct.html",
    "href": "posts/AgentInstruct/AgentInstruct.html",
    "title": "AgentInstruct: Toward Generative Teaching with Agentic Flows",
    "section": "",
    "text": "Synthetic data is becoming increasingly important for accelerating the development of language models, both large and small. Despite several successful use cases, researchers also raised concerns around model collapse and drawbacks of imitating other models. Effective use of synthetic data usually requires significant human effort in curating the data and mostly uses prompts and a powerful model such as GPT-4 or Llama to generate such post-training synthetic data.\nTo address these issues researchers have introduced AgentInstruct, an extensible agentic framework for automatically creating large amounts of diverse and high-quality synthetic data that surpasses underlying LLMs. The AgentInstruct comprises four primary stages:\n\nSeed Collection: This step involves gathering a varied set of initial sources such as textbook sections, web articles, and code snippets. These sources serve as the foundational material for creating new instructions.\nContent Transformation: Specialized agents process each seed into an intermediate form that facilitates the creation of instructions. These agents are capable of tasks like generating argument passages, debates, conversations, meeting transcripts, poems, and satirical content, among others.\nSeed Instruction Generation: In this phase, multiple agents take the transformed seed and generate a wide range of instructions using a predefined taxonomy of instruction types. For instance, in the field of reading comprehension, this taxonomy includes 43 types of questions, covering literal comprehension, critical analysis, and inference.\nInstruction Refinement: The final stage focuses on improving the complexity and quality of the generated instructions through an iterative process. Suggester-editor agent pairs work together: suggest agents propose ways to increase the complexity of instructions, while editor agents make corresponding modifications to enhance their quality.\n\nUsing AgentInstruct researchers were able to generate around 22 million instructions. These were integrated with 3.8 million instructions sourced elsewhere, resulting in a dataset totaling 25.8 million paired instructions. This extensive dataset was then employed to fine-tune the Mistral-7b model, leading to the development of the Orca-3 model.\nPaper : https://arxiv.org/pdf/2407.03502"
  },
  {
    "objectID": "posts/SHERL/SHERL.html",
    "href": "posts/SHERL/SHERL.html",
    "title": "SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning",
    "section": "",
    "text": "Parameter-efficient transfer learning (PETL) is widely used for domain adaptation of large pre-trained models to specific downstream tasks, greatly reducing trainable parameters while grappling with memory challenges during fine-tuning. To address it, memory-efficient series (METL) avoid back propagating gradients through the large backbone. However, they compromise by exclusively relying on frozen intermediate outputs and limiting the exhaustive exploration of prior knowledge from pre-trained models.\nNow researchers have come up with an innovative METL strategy called SHERL for resource-limited scenarios to decouple the entire adaptation into two successive and complementary processes. In the early route, intermediate outputs are consolidated via an anti-redundancy operation, enhancing their compatibility for subsequent interactions; thereby in the late route, utilizing minimal late pre-trained layers could alleviate the peak demand on memory overhead and regulate these fairly flexible features into more adaptive and powerful representations for new domains.\nDuring evaluation it was found that SHERLcombines the strengths of both parameter and memory-efficient techniques, performing on-par or better across diverse architectures with lower memory during fine-tuning.\nPaper : https://arxiv.org/pdf/2407.07523"
  },
  {
    "objectID": "posts/NeedleBench/NeedleBench.html",
    "href": "posts/NeedleBench/NeedleBench.html",
    "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?",
    "section": "",
    "text": "The capability of LLMs to process long texts is particularly crucial across various domains. Considering the critical role of LLMs in handling long texts, numerous approaches have been suggested to evaluate their long-context capabilities. Such as the Needle In A Haystack(NIAH) test, which uses a more diverse set of non-repetitive personal essays with a context window of 200K.\nSo one may ask: Does passing the ”needle-in-a-haystack” test—extracting key info from lengthy texts—really indicate that LLMs can handle complex real-world long context problems?\nTo answer this researcher from Tsinghua University have introduce NeedleBench, a framework consisting of a series of progressively more challenging tasks for assessing bilingual longcontext capabilities, spanning multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and different depth ranges, allowing the strategic insertion of critical data points in different text depth zones to rigorously test the retrieval and reasoning capabilities of models in diverse contexts.\nNeedleBench comprises following advanced long-context information capability evaluation methods, 1. Single-Needle Retrieval Task (S-RT): Tests LLMs’ ability to recall a single key information inserted at various positions in a long text, highlighting their precision in navigating and recalling single detail within extensive texts. 2. Multi-Needle Retrieval Task (M-RT): Explores LLMs’ ability to retrieve multiple pieces of related information scattered across a lengthy text, simulating complex real-world queries that require extracting several data points from comprehensive documents. 3. Multi-Needle Reasoning Task(M-RS): Evaluates LLMs’ ability for complex reasoning by extracting multiple pieces of information from long texts and using them to logically answer questions that demand an integrated understanding and reasoning of various text segments.\nFurthermore, researchers have also developed the Ancestral Trace Challenge (ATC) test as the simplified proxy for measuring multi-step logical reasoning. During evaluation it was found that current LLMs struggle to handle reasoning tasks with complex logical relationships, even with texts shorter than 2K tokens.\nPaper : https://arxiv.org/pdf/2407.11963\nCode : https://github.com/open-compass/opencompass"
  },
  {
    "objectID": "posts/E5-V/E5-V.html",
    "href": "posts/E5-V/E5-V.html",
    "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
    "section": "",
    "text": "With the development of Multimodal Large Language Models (MLLMs), there is an increasing need for embedding models to represent multimodal inputs. Although CLIP shows impressive results in text-image retrieval, it struggles to represent interleaved visual and language inputs.\nTo address this researchers from Microsoft have introduced E5-V, a new framework designed to adapt MLLMs for achieving universal multimodal embeddings. E5-V achieves this by using a prompt based method to explicitly instruct MLLMs to represent multimodal inputs into words. This unifies multimodal embeddings into the same space, which directly removes the modality gap in multimodal embeddings. Now MLLMs are able to achieve robust multimodal embedding performance through single modality training with only on text inputs. This eliminates the need for expensive multimodal training data collection.\nFurther, by focusing solely on text data, ES-V removes other components, such as the visual encoder, in the MLLMs during training and decreases the input size, significantly reducing the training by 95%. Apart from this there are several advantages to representing multimodal information with MLLMs: (1) MLLMs can initially learn to represent multimodal information according to their meanings with prompt, benefiting from interleaved visual and language training. (2) MLLMs are capable of representing interleaved visual and language inputs to handle tasks like composed image retrieval. (3) MLLMs have stronger language understanding and reasoning capabilities compared to CLIP.\nE5-V was validated across various tasks: text-image retrieval, composed image retrieval, sentence embeddings, and image-image retrieval. By comparing E5-V with the strong baselines of each task, It was found that E5-V was effective in representing multimodal information, which achieves competitive performance on all tasks as a universal multimodal embeddings model trained on text pairs only.\nPaper : https://arxiv.org/pdf/2407.12580\nCode : https://github.com/kongds/E5-V"
  },
  {
    "objectID": "posts/SA/SA.html",
    "href": "posts/SA/SA.html",
    "title": "Beyond KV Caching: Shared Attention for Efficient LLMs",
    "section": "",
    "text": "The efficiency of large language models (LLMs) remains a critical challenge, particularly in contexts where computational resources are limited. Traditional attention mechanisms in these models such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) made strides in reducing the key-value (KV) cache size by sharing keys and values across multiple heads within a layer. More recently, Cross-Layer Attention (CLA) has extended this concept by sharing keys and values across adjacent layers, further reducing memory requirements without substantially impacting model performance, while powerful these Attention mechanism still require significant computational and memory resources due to the necessity of recalculating and storing attention weights across different layers.\nTo address this researchers have introduced a novel method called Shared Attention (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing computed attention weights across multiple layers. Unlike previous methods that focus on sharing intermediate Key-Value (KV) caches, SA utilizes the isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining to reduce both the computational flops and the size of the KV cache required during inference.\nThe MQA and GQA methods share the Key and Value caches with the Query within the same layer to reduce memory usage. The CLA method extends this by sharing the Key and Value caches across different layers. Shared Attention, advances this concept further by sharing the attention weights across multiple layers.\nEmpirically implementing SA across various LLMs (Llama-7B and Llama-8B) results in minimal accuracy loss on standard benchmarks including GLUE and MMLU. Further, It was found that SA not only conserves computational resources but also maintains robust model performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained environments.\nPaper : https://arxiv.org/pdf/2407.12866\nCode : https://github.com/metacarbon/shareAtt"
  },
  {
    "objectID": "posts/Q-Sparse/Q-Sparse.html",
    "href": "posts/Q-Sparse/Q-Sparse.html",
    "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
    "section": "",
    "text": "Recently, considerable research work has been going towards reducing high computational cost and memory footprint of LLMs, especially during the inference stage. Sparsity is one of the promising approaches among all. Sparsity in LLMs can be used as weight sparsity, which prunes the model weights to save the computation or activation sparsity, which reduces the number of activated elements in the activation tensors. However there are challenges associated with sparsity in terms of model accuracy and efficiency again.\nIn order to address these issues and also study the full impact of sparsity researchers have introduced Q-Sparse, a simple yet effective approach to enable full sparsity of activations in LLMs. The major modification on LLMs is in the linear projection (i.e., matrix multiplication). For each linear projection, it has a top-K sparsification function that selects the top-K activations in the input tensor.\nY = (X ⊙ M) · WT (2)\nM = Topk (|X|) (3)\nwhere M ∈ R N×D is the mask tensor that indicates the top-K activations in the input tensor X in terms of the absolute values, ⊙ is the element-wise multiplication operation, and Top k is the function that selects the top-K elements in the tensors.\nFor the backpropagation, the straight through estimator is used to compute the gradients of the activations. Further, a squared ReLU function is also used for the feed-forward layers to further improve the sparsity of the activations. Q-Sparse can be used with both full-precision and quantized LLMs. To study the scaling law of sparsely-activated LLMs, we conduct a series of scaling experiments and derive an inference-optimal scaling law for sparsely-activated LLMs.\nQ-Sparse LLMs were evaluated under various settings, including training from scratch, continue-training, and fine-tuning. When training from scratch with 50B tokens, Q-Sparse matched dense baselines at 40% sparsity. BitNet b1.58 models with Q-Sparse outperformed dense baselines with the same compute budget. Fine-tuning results demonstrated that Q-Sparse models with around 4B activated parameters matched or exceeded the performance of dense 7B models, proving Q-Sparse’s efficiency and effectiveness across training scenarios.\nPaper : https://arxiv.org/pdf/2407.10969"
  },
  {
    "objectID": "posts/BOND/BOND.html",
    "href": "posts/BOND/BOND.html",
    "title": "BOND: Aligning LLMs with Best-of-N Distillation",
    "section": "",
    "text": "State-of-the-art large language models (LLMs) such as Gemin and GPT-4 are generally trained in three stages. First, LLMs are pre-trained on large corpora of knowledge using next-token prediction Second, the pre-trained models are fine-tuned to follow instructions via supervised fine-tuning (SFT) Lastly, reinforcement learning from human feedback (RLHF) is used to further increase the quality of generations. Fine-tuning LLMs with reinforcement learning (RL) is challenging, notably since it can cause forgetting of pre-trained knowledge and can cause reward hacking.\nBest-of-N inference-time strategy is widely used to handle this. Best-of-N sampling draw 𝑁 candidate generations from the reference SFT model and select the one with the highest reward according to the RM. This strategy empirically achieves excellent reward-KL trade-offs but increases the computational cost by a factor of 𝑁.\nNow researchers from Google’s proposed BOND approach aims at obtaining a fine-tuned policy that can directly sample the Best-of-N generation. This would inherit the quality of Best-of-N sampling, while requiring a single sample at inference time. This by distilling the Best-of-N strategy into the policy via online distribution matching.\nBOND first minimizes the forward KL divergence using samples from the Best-of-N strategy, leading to a standard imitation learning setup with a mode covering behavior. Then it minimizes the backward KL, leading to a new form of quantile-based advantage, which does not depend on the reward scale, and corresponds to a mode seeking behavior. Then, linear combination of forward and backward KL is used, also known as Jeffreys divergence, which retains the best of both approaches. Finally, this J-BOND (J for Jeffreys), a novel, stable, efficient and practical RLHF algorithm is used to align LLMs.\nPaper : https://arxiv.org/pdf/2407.14622"
  },
  {
    "objectID": "posts/Llama31/Llama31.html",
    "href": "posts/Llama31/Llama31.html",
    "title": "The Llama 3 Herd of Models",
    "section": "",
    "text": "The Llama 3.1 release marked a big milestone for LLM researchers and the open source AI community. Meta engineers trained Llama 3.1 on NVIDIA H100 Tensor Core GPUs. They significantly optimized their full training stack and pushed model training to over 16K H100 GPUs, making the 405B the first Llama model trained at this scale.\nTo scale training for such a large model across 16K GPU, Meta uses 4D parallelism—a combination of four different types of parallelism methods—to shard the model: (1) Tensor parallelism [TP] (2) Pipeline parallelism [PP] (3) Context parallelism [CP] and (4) Data parallelism [DP].\nTensor parallelism splits individual weight tensors into multiple chunks on different devices. Pipeline parallelism partitions the model vertically into stages by layers, so that different devices can process in parallel different stages of the full model pipeline. Context parallelism divides the input context into segments, reducing memory bottleneck for very long sequence length inputs. Data parallelism, which shards the model, optimizer, and gradients while implementing data parallelism which processes data in parallel on multiple GPUs and synchronizes after each training step.\nSo how does this 4D parallelism work ?\nTo understand this, let’s take an example of 16 GPUs clusters where GPUs are divided into parallelism groups in the order of [TP, CP, PP, DP], where DP stands for FSDP. Further, this 16 GPUs are configured with a group size of |TP|=2, |CP|=2, |PP|=2, and |DP|=2. A GPU’s position in 4D parallelism is represented as a vector, [D1, D2, D3, D4], where Di is the index on the i-th parallelism dimension. GPU0[TP0, CP0, PP0, DP0] and GPU1[TP1, CP0, PP0, DP0] are in the same TP group, GPU0 and GPU2 are in the same CP group, GPU0 and GPU4 are in the same PP group, and GPU0 and GPU8 are in the same DP group.\nPaper : https://lnkd.in/dufwtswu"
  },
  {
    "objectID": "posts/VILA2/VILA2.html",
    "href": "posts/VILA2/VILA2.html",
    "title": "VILA2: VILA Augmented VILA",
    "section": "",
    "text": "Visual language models (VLMs) have rapidly progressed, driven by the success of large language models (LLMs). However data curation of VLMs still remains under-explored. Given the costly nature of VLM training, most methods are confined with coarse-quality large-scale captioning image-text pairs (pretraining), followed by fine-grained small-scale supervised finetuning (SFT). Recent methods have observed rewarding distillation possibilities from GPT-4V and Gemini. However, the performance is upper bound by these models.\nSo is it possible that the VLM itself can remedy dataset deficiency and enhance its training ?\nTo address this, researchers from Nvidia have introduced VILA-augmented VILA (VILA2), a novel approach that includes a self-augment step and a specialist-augment step to iteratively improve data quality and model performance. VILA2 consists of two main steps: a self-augment step and a specialist augment step.\nIn the self-augment step, a VLM recaptions its own pre-training data to enhance data quality, and then retrains from scratch using this refined dataset to improve model performance. This process can iterate for several rounds. Once self-augmentation saturates, VILA2 employs several specialist VLMs finetuned from the self-augmented VLM with domain-specific expertise, to further infuse specialist knowledge into the generalist VLM through task-oriented recaptioning and retraining.\nIn general, VILA2 re-formulate visual language model (VLM) training with a model in the loop to remedy training data defficacy. Then start with validating design options in constructing a self-augmenting loop to improve on caption quality of the default training task. After the saturation of this process, VLM is challenge to generate data conforming to extra SFT-enabled tasks to further VLM learning.\nVILA2 based foundational model consistently improves the accuracy on a wide range of tasks over prior art, and achieves new state-of-the-art results on MMMU leaderboard among open-sourced models.\nPaper : https://arxiv.org/pdf/2407.17453"
  },
  {
    "objectID": "posts/LAMBDA/LAMBDA.html",
    "href": "posts/LAMBDA/LAMBDA.html",
    "title": "LAMBDA: A Large Model Based Data Agent",
    "section": "",
    "text": "Large Language Models (LLMs) have been instrumental in pushing innovation across multiple domains. However, despite these advancements, the current LLM paradigm encounters challenges and limitations in data science applications, particularly in domains that demand extensive expertise and advanced coding knowledge.\nTo address this researchers have introduced LAMBDA (A Large Model Based Data Agent), a novel open-source, code-free multi-agent data analysis system. LAMBDA is designed to address data analysis challenges in complex data-driven applications through the use of innovatively designed data agents that operate iteratively and generativity using natural language.\nAt the core of LAMBDA are two key agent roles: the programmer and the inspector, which are engineered to work together seamlessly. Specifically, the programmer generates code based on the user’s instructions and domain-specific knowledge, enhanced by advanced models. Meanwhile, the inspector debugs the code when necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a user interface that allows direct user intervention in the operational loop. Additionally, LAMBDA can flexibly integrate external models and algorithms through knowledge integration mechanisms, catering to the needs of customized data analysis.\nLAMBDA demonstrates superior performance on various machine learning (ML) datasets. Notable results with an accuracy of 100%, 98.07%, and 98.89% on datasets NHANES, Breast Cancer, and Wine respectively. To sum up, the main characteristics of LAMBDA are as follows: (1) Codingfree and natural language interface. (2) Integrating human intelligence and AI. (3) Reliability. (4) Automatic analysis report generation.\nPaper : https://arxiv.org/pdf/2407.17535"
  },
  {
    "objectID": "posts/CoD/CoD.html",
    "href": "posts/CoD/CoD.html",
    "title": "Chain of Diagnosis (CoD): Towards an Interpretable Medical Agent",
    "section": "",
    "text": "The field of medical diagnosis has undergone a significant transformation with the advent of large language models (LLMs), yet the challenges of interpretability within these models remain largely unaddressed. Although LLMs can offer rudimentary explanations for their decision, they lack a comprehensive process to explain why other potential diseases are excluded and to which extent of confidence it made such a decision. This highlights the need for an interpretable LLM solution for diagnosis.\nTo address this researchers have introduced Chain-of-Diagnosis (CoD) to enhance the interpretability of LLM-based medical diagnostics. CoD transforms the diagnostic process into a diagnostic chain that mirrors a physician’s thought process, providing a transparent reasoning pathway. Additionally, CoD outputs the disease confidence distribution to ensure transparency in decision making. This interpretability makes model diagnostics controllable and aids in identifying critical symptoms for inquiry through the entropy reduction of confidences.\nCoD Transforms the opaque decision-making process into a five-step diagnostic chain that reflects a physician’s thought process. 1. Step 1: Symptom Abstraction:, The first step summarizes the symptoms S of the patient’s question 2. Step 2: Disease Recall & Knowledge Integration: Next, CoD identifies the top-K potential diseases based on a disease retriever 3. Step 3: Diagnostic Reasoning: Now CoD generates the diagnostic reasoning process. 4. Step 4: Confidence Assessment: After generating T, CoD generates a confidence distribution. 5. Step 5: Decision Making: In the last step, a confidence threshold τ is set to control the decision making. The diagnostic task involves two decision types: 1) making a diagnosis Adiag(d), where d is the diagnosed disease, and 2) inquiring about a symptom Ainq(s), where s represents the symptom under inquiry.\nTo evaluate COD, DiagnosisGPT was developed, capable of diagnosing 9,604 diseases. Experimental results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring controllability in diagnostic rigor.\nPaper : https://arxiv.org/pdf/2407.13301\nCode : https://github.com/FreedomIntelligence/Chain-of-Diagnosis"
  },
  {
    "objectID": "posts/DDK/DDK.html",
    "href": "posts/DDK/DDK.html",
    "title": "DDK: Distilling Domain Knowledge for Efficient Large Language Models",
    "section": "",
    "text": "Despite the advance of large language models (LLMs) in various applications, it still faces significant challenges to propagate further due to high computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a high-performing LLM (i.e., the teacher model). The primary challenges in enhancing the performance of KD approaches on LLMs stem from two main aspects: (1) appropriately utilizing the data (2) stabilize the distillation process. Thus the right domain-specific data mixtures for KD is extremely critical for overall LLM performance.\nTo address this researchers have introduced Distill Domain Knowledge for LLMs (DDK), a novel methodology, which is designed to improve the performance of language models by addressing the gap between teacher and student models in different domains. It works by first measuring performance differences using a validation dataset, then recalculating these discrepancies periodically. DDK uses a guided sampling strategy to focus on data from various domains according to these discrepancies. Additionally, it incorporates a factor smooth updating mechanism to enhance stability and robustness. The goal is to reduce differences in output between the teacher and student models through minimized supervision loss.\nOverall first, the training dataset is divided into distinct domains based on predefined criteria. Then, DDK dynamically modulates the distribution of domain-specific data, augmenting the amount allocated to domains where the student model struggles the most. The proportions attributed to each domain are recalculated at distillation intervals by employing a factor smooth updating approach.\nExtensive evaluations show that DDK significantly improves the performance of student models, outperforming both continuously pretrained baselines and existing knowledge distillation methods by a large margin.\nPaper : https://arxiv.org/pdf/2407.16154"
  },
  {
    "objectID": "posts/DiT-MoE/DiT-MoE.html",
    "href": "posts/DiT-MoE/DiT-MoE.html",
    "title": "DiT-MoE : Scaling Diffusion Transformers to 16 Billion Parameters",
    "section": "",
    "text": "Recently, diffusion models (DiT) have emerged as powerful deep generative models in various domains, such as image, video and 3D objects. However, training and serving such models is expensive. This is partially because these deep networks are typically dense. Various techniques are used to address this issue and a sparse mixture of experts (MoE) are becoming increasingly popular among them as a practical implementation that employs a routing mechanism to control computational costs.\nNow researchers have proposed DiT-MoE, a sparse variant of the DiT architecture that is scalable and competitive with dense networks while exhibiting highly optimized inference. The DiT-MoE replaces a subset of the dense feedforward layers in DiT with sparse MoE layers, where each token of image patch is routed to a subset of experts, i.e., MLP layers.\nThe DiT-MoE includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection shows preference with spatial position and denoising time step, while insensitive with different class-conditional information; (ii) As the MoE layers go deeper, the selection of experts gradually shifts from specific spatial position to dispersion and balance. (iii) Expert specialization tends to be more concentrated at the early time step and then gradually uniform after half.\nExperiment results indicate that DiT-MoE matches the performance of state-of-the-art dense models, while requiring less time to inference. Alternatively, DiT-MoE-S can match the cost of DiT-B while achieving better performance. Further, DiT-MoE with synthesized image data, were able to scale at a 16.5B parameter, while only activating 3.1B parameters, attaining a new SoTA FID-50K score of 1.80 in 512×512 resolution settings.\nPaper : https://arxiv.org/pdf/2407.11633"
  },
  {
    "objectID": "posts/LazyLLM/LazyLLM.html",
    "href": "posts/LazyLLM/LazyLLM.html",
    "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
    "section": "",
    "text": "Standard prompt-based LLM inference has two sequential stages: prefilling and decoding. During the prefilling stage, the model computes and saves the KV cache of each token from the prompt, and predicts the first token and time taken during prefilling stage is usually referred as “time-to-first-token” (TTFT). During the decoding stage, the model reuses cached KVs to decode the next token iteratively until the stop criteria are met.\nFor long prompts, the KV cache must be computed for all tokens during the prefilling stage, which can significantly increase TTFT. Consequently, the prefilling stage may become a bottleneck in the generation process\nOne may ask, are all prompt tokens essential for generating the first token ?\nTo answer this, researcher have introduce LazyLLM, a novel method that selectively computes the KV for tokens important for the next token prediction in both the prefilling and decoding stages. Contrary to static pruning approaches that prune the prompt at once, LazyLLM allows language models to dynamically select different subsets of tokens from the context in different generation steps, even though they might be pruned in previous steps.\nLazyLLM starts with the full context and progressively prunes tokens to gradually reduce the number of computations towards the end of the model. LazyLLM allows the model to select different subsets of tokens from the context in different generation steps.\nAs comparison to standard LLM, which compute the KV cache of all input tokens at the prefilling stage, LazyLLM only selectively computes the tokens that are important to the next token prediction, deferring the computation of remaining tokens to later steps. LazyLLM significantly optimizes TTFT by reducing the amount of computation during prefilling. Moreover, as some tokens in the prompt are never selected by LazyLLM during the whole generation process. Moreover LazyLLM also reduces the total amount of computation and accelerates the overall generation.\nExtensive experiments on standard datasets across various tasks demonstrate that LazyLLM is a generic method that can be seamlessly integrated with existing language models to significantly accelerate the generation without fine-tuning. For instance, in the multi-document question-answering task, LazyLLM accelerates the prefilling stage of the LLama 2 7B model by 2.34× while maintaining accuracy.\nPaper : https://arxiv.org/pdf/2407.14057"
  },
  {
    "objectID": "posts/RAG Foundry/RAG Foundry.html",
    "href": "posts/RAG Foundry/RAG Foundry.html",
    "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
    "section": "",
    "text": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach.\nRAG FOUNDRY, an open-source framework for augmenting large language models for RAG use cases. RAG FOUNDRY integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings.\nThe RAG Foundry framework is designed for rapid prototyping and experimentation and predomintly consist of following four main modules: 1. Dataset Creation: Manages the creation and processing of datasets for RAG training and inference, including data normalization, aggregation, retrieval, API integration, and prompt creation. Data is saved in a consistent format. 2. Training: Utilizes Parameter-Efficient Fine-Tuning (PEFT) and Transfer Reinforcement Learning (TRL) for training models on augmented datasets. Trained models can be uploaded to the Hugging Face Hub. 3. Inference: Generates predictions using trained or untrained language models with the augmented datasets. 4. Evaluation: Assesses the outputs from the inference module using various metrics such as EM, F1, ROUGE, and BERTScore. Metrics can be customized and applied either locally or globally.\nRAG FOUNDRY framework have been found effective by augmenting and finetuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across TriviaQA, ASQA and PubmedQA knowledge-intensive datasets.\nPaper : https://arxiv.org/pdf/2408.02545\nCode : https://github.com/IntelLabs/RAGFoundry"
  },
  {
    "objectID": "posts/ReLiK/ReLiK.html",
    "href": "posts/ReLiK/ReLiK.html",
    "title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
    "section": "",
    "text": "Extracting structured information from unstructured text lies at the core of many Gen AI problems such as Information Retrieval, Knowledge Graph Construction, Knowledge Discovery, Automatic Text Summarization and so on. In most of such applications Entity Linking (EL) and Relation Extraction (RE) are very critical components, which are governed by three fundamental properties: Inference Speed, Flexibility, and Performance. While tremendous progress has recently been made on both EL and RE however these approaches only focus on at most two out of the aforementioned three properties simultaneously, hindering their applicability in multiple scenarios.\nTo address this researchers have introduced ReLiK, a Retriever-Reader architecture for both EL and RE, which can be divided into two main components: * The Retriever module, that is tasked to retrieve the possible Entities/Relations that can be extracted from a given input text. * The Readermodule, given the original input text and all the retrieved Entities/Relations (output of the Retriever), is tasked to connect them to the relevant spans in the text.\nReLiK enhances Inference Speed, Flexibility, and Performance of AI system by integrating several innovative features: 1. It uses a nonparametric memory with a Retriever component, reducing the number of parameters needed while maintaining high performance and fast inference speed. 2. It employs textual representations for entities and relations, enabling the model to handle unseen entities and relations more flexibly. 3. Its novel input formulation maximizes the contextualization capabilities of advanced language models like DeBERTa-v3, leading to improved performance and processing speed through simultaneous encoding and extraction of text and entities.\nPaper : https://arxiv.org/pdf/2408.00103\nCode : https://github.com/SapienzaNLP/relik?tab=readme-ov-file"
  },
  {
    "objectID": "posts/CODEXGRAPH/CODEXGRAPH.html",
    "href": "posts/CODEXGRAPH/CODEXGRAPH.html",
    "title": "CODEXGRAPH: Bridging Large Language Models and Code Repositories via Code Graph Databases",
    "section": "",
    "text": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications.\nTo mitigate these limitations, researcher have introduce CODEXGRAPH, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CODEXGRAPH enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation.\nCODEXGRAPH utilizes static analysis to extract code graphs from repositories using a task-agnostic schema that defines the nodes and edges within the code graphs. In these graphs, nodes represent source code symbols such as MODULE, CLASS, and FUNCTION, and each node is enriched with relevant meta-information. The edges between nodes represent the relationships among these symbols, such as CONTAINS, INHERITS, and USES. By leveraging the structural properties of graph databases, CODEXGRAPH enhances the LLM agent’s comprehension of code structures. CODEXGRAPH leverages repository code information and graph structures for global analysis and multi-hop reasoning, enhancing code task performance. When users provide code-related inputs, the LLM agent analyzes the required information from the code graphs, constructs flexible queries using graph query language, and locates relevant nodes or edges. This enables precise and efficient retrieval, allowing for effective scaling to larger repository tasks.\nCODEXGRAPH was evaluated across three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. With a unified graph database schema, CODEXGRAPH demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering.\nPaper : https://arxiv.org/pdf/2408.03910\nCode : https://github.com/modelscope/modelscope-agent/tree/master"
  },
  {
    "objectID": "posts/SENSE/SENSE.html",
    "href": "posts/SENSE/SENSE.html",
    "title": "Synthesizing Text-to-SQL Data from Weak and Strong LLMs",
    "section": "",
    "text": "Text-to-SQL has been one of the shout-out use cases in AI application development especially with close source LLM such as GPT4. However, the adoption of closed source LLMs introduces concerns pertaining to issues of openness, privacy, and substantial costs.\nDeveloping specialized text-to-SQL models built upon open-source LLMs remains a challenge due to the high cost of achieving text-to-SQL data, which relies on manual expert annotation. To overcome this, researchers have introduced a synthetic data approach that amalgamates strong data generated by larger, more potent models (strong models) with weak data produced by smaller, less well aligned models (weak models). This approach contributes to the improvement of domain generalization in text-to-SQL models and investigates the potential of weak data supervision through preference learning. Further, researchers have utilized the synthetic data approach for instruction tuning on open-source LLMs, yielding SENSE.\nSENSE, Integrating human annotated data with synthetic data from strong models for domain diversity, and weak models for preference learning, aligning with executors for enhanced text-toSQL performance.\nExtensive experiments demonstrate that SENSE achieves state-of-the-art performance on the SPIDER and BIRD benchmarks, thereby mitigating the performance disparity between open-source models and the methods derived from closed-source models.\nPaper : https://arxiv.org/pdf/2408.03256"
  },
  {
    "objectID": "posts/rStar/rStar.html",
    "href": "posts/rStar/rStar.html",
    "title": "rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers",
    "section": "",
    "text": "Despite their success, large language models face significant challenges in complex reasoning tasks. Although fine-tuning is shown to be an effective way to improve reasoning capability, most LLMs rely on fine-tuning data distilled or synthesized by superior models like GPT-4. Recently, Reasoning improvements without a superior teacher LLM method are getting more traction but have two major limitations: (1) LLMs often struggle to effectively explore the solution space during reasoning. (2) Even the self-exploration can find high quality reasoning steps, it is difficult for SLMs to tell which reasoning steps are of higher quality or determine which final answers are correct thus it is hard to effectively guide the self-exploration.\nNow researchers have introduced rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutually consistent, thus are more likely to be correct.\nIn general, self-play mutual reasoning is a generation-discrimination process: (1) a self-generator augments the target SLM to generate candidate reasoning trajectories using MCTS; (2) the discriminator uses another SLM to provide unsupervised feedback on each trajectory based on partial hints; (3) based on this feedback, the target SLM decides a final reasoning trajectory as the solution.\nExtensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8BInstruct.\nPaper : https://arxiv.org/pdf/2408.06195"
  },
  {
    "objectID": "posts/PAD/PAD.html",
    "href": "posts/PAD/PAD.html",
    "title": "PAD: Prioritize Alignment in Dataset Distillation",
    "section": "",
    "text": "Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information (Information Extraction) from the target dataset and embed (Information Embedding) it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. However, these methods introduce misaligned information in both information extraction and embedding stages.\nTo alleviate this, researchers have proposed Prioritize Alignment in Dataset Distillation (PAD), which aligns information from the following two perspectives. 1) Prune the target dataset according to the compression ratio to filter the information that can be extracted by the agent model. 2) Use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information. This simple strategy effectively filters out misaligned information and brings non-trivial improvement for mainstream matching-based distillation algorithms.\nThrough experiments, it was found that the two-step alignment strategy is effective for distillation methods based on matching gradients, distributions, and trajectories and the proposed novel method Prioritize Alignment in Dataset Distillation (PAD) achieves state-of-the-art (SOTA) performance.\nPaper : https://arxiv.org/pdf/2408.03360"
  },
  {
    "objectID": "posts/DeepSeek-Prover/DeepSeek-Prover.html",
    "href": "posts/DeepSeek-Prover/DeepSeek-Prover.html",
    "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
    "section": "",
    "text": "Recent advancements in large language models have significantly influenced mathematical reasoning and theorem proving in artificial intelligence. Despite notable progress in natural language domains, language models still encounter substantial challenges in formal theorem proving, which requires rigorous derivations satisfying formal specifications of the verification system.\nLLM in formal theorem proving uses two main approaches: proof-step generation and whole-proof generation. Proof-step generation involves predicting and verifying each individual tactic in a proof, often using tree search techniques. Whole-proof generation, on the other hand, creates a complete proof code directly from the theorem statement, which is more computationally efficient and requires less coordination between the model and the verifier.\nTo seamlessly integrate intermediate tactic states in proof-step generation while maintaining the simplicity and computational efficiency of whole-proof generation, researchers have developed a unified approach in DeepSeek-Prover-V1.5. This method combines the strengths of both proof-step and whole-proof generation techniques through a truncate-and-resume mechanism.\nThe process begins with standard whole-proof generation, where the language model completes the proof code following the theorem statement prefix. The Lean prover then verifies this code. If the proof is correct and complete, the procedure terminates. If an error is detected, the code is truncated at the first error message, and any subsequent code is discarded. The successfully generated proof code is then used as a prompt for the generation of the next proof segment.\nDeepSeek-Prover-V1.5 Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths.\nDeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeekProver-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark (63.5%) and the undergraduate level ProofNet benchmark (25.3%).\nPaper : https://arxiv.org/pdf/2408.08152"
  },
  {
    "objectID": "posts/xGen-MM/xGen-MM.html",
    "href": "posts/xGen-MM/xGen-MM.html",
    "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
    "section": "",
    "text": "Large Multimodal Models (LMMs) have attracted significant attention with their potential applications and emergent capabilities. However, recent works have demonstrated that large-scale and high-quality data are essential for training robust LMMs. Lack of such data has widened the gap between open-source models and proprietary ones in terms of access to open weights, training recipes, and curated datasets.\nIn response to these challenges, researcher have introduce xGen-MultiModal or xGen-MM (BLIP-3), a new framework designed to scale up LMM training by utilizing an ensemble of multimodal interleaved datasets, curated caption datasets, and other publicly available datasets. In xGen-MM (BLIP-3) uses scalable vision token sampler and simplifies the training objectives to focus solely on the auto-regressive loss of text tokens in a multimodal context. xGen-MM uses Free-form interleaved images and texts from the ensembled interleaved and caption datasets with each modality undergoing a separate tokenization process to be fed into the pre-trained LLM in natural order. A standard auto-regressive loss is then applied to the text tokens. The Vision Transformer is kept frozen during training, while all other parameters, including the token sampler and the pre-trained LLM, are trained.\nFurther, researchers have also released scaling up the training data: MINT-1T , a trillion-token scale interleaved dataset; BLIP3-KALE, a knowledge-augmented high-quality dense captions dataset. BLIP3-OCR-200M, a large-scale dataset with dense OCR annotations; and BLIP3-GROUNDING-50M, a large-scale visual grounding dataset.\nPaper : https://arxiv.org/pdf/2408.08872"
  },
  {
    "objectID": "posts/BAM/BAM.html",
    "href": "posts/BAM/BAM.html",
    "title": "BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts",
    "section": "",
    "text": "The Mixture of Experts (MoE) framework has become a popular architecture for large language models due to its superior performance over dense models. However, training MoEs from scratch in a large-scale regime is prohibitively expensive. Existing methods mitigate this by pre-training multiple dense expert models independently and using them to initialize an MoE. This is done by using experts’ feed-forward network (FFN) to initialize the MoE’s experts while merging other parameters. However, this method limits the reuse of dense model parameters to only the FFN layers, thereby constraining the advantages when “upcycling” these models into MoEs.\nTo address this shortcoming, researchers have proposed BAM (Branch-Attend-Mix), a simple yet effective method that makes full use of specialized dense models by not only using their FFN to initialize the MoE layers but also leveraging experts’ attention parameters fully by initializing them into a soft-variant of Mixture of Attention (MoA) layers. BAM operates in three phases. 1) Branching: Begin with a pre-trained dense seed model and create N copies of it. 2) Continued Pre-training: Continue to pre-train each copy independently on its own data mixture. This process yields specialized dense expert models. 3) Mixture Model Training: Utilize these specialized dense expert models to initialize both the FFN and attention experts of the mixture model. The router layers are initialized randomly. All other parameters are derived by averaging the corresponding layers in each of the dense experts. To further improve efficiency, BAM adopted a parallel attention transformer architecture to MoEs, which allows the attention experts and FFN experts to be computed concurrently.\nFurther, BAM uses two methods for upcycling attention parameters: 1) initializing separate attention experts from dense models including all attention parameters for the best model performance; and 2) sharing key and value parameters across all experts to facilitate for better inference efficiency.\nDuring experiments on seed models ranging from 590 million to 2 billion parameters demonstrate that BAM surpasses baselines in both perplexity and downstream task performance, within the same computational and data constraints.\nPaper : https://arxiv.org/pdf/2408.08274"
  },
  {
    "objectID": "posts/Transfusion/Transfusion.html",
    "href": "posts/Transfusion/Transfusion.html",
    "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
    "section": "",
    "text": "Multi-modal generative models need to be able to perceive, process, and produce both discrete elements (such as text or code) and continuous elements (e.g. image, audio, and video data). Language models excel with discrete data, such as text, by predicting the next token in a sequence, while diffusion models are best for continuous data, like images. Researchers are working on combining these by integrating diffusion models into language models to enhance performance in both areas.\nNow researchers have introduced Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion pretrained a transformer model on 50% text and 50% image data using a different objective for each modality: next token prediction for text and diffusion for images. The model is exposed to both modalities and loss functions at each training step. Standard embedding layers convert text tokens to vectors, while patchification layers represent each image as a sequence of patch vectors. By applying causal attention for text tokens and bidirectional attention for image patches. For inference, a decoding algorithm is used that combines the standard practices of text generation from language models and image generation from diffusion models.\nAt a high level Transfusion uses a single transformer perceives, processes, and produces data of every modality. Discrete (text) tokens are processed autoregressively and trained on the next token prediction objective. Continuous (image) vectors are processed together in parallel and trained on the diffusion objective. Marker BOI and EOI tokens separate the modalities\nExperiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. There has been significant performance gain by introducing modality-specific encoding and decoding layers, and even compressing each image to just 16 patches. Further, scaling Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.\nPaper : https://arxiv.org/pdf/2408.11039"
  },
  {
    "objectID": "posts/Minitron/Minitron.html",
    "href": "posts/Minitron/Minitron.html",
    "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
    "section": "",
    "text": "Training multiple multi-billion parameter large language models from scratch is extremely time-, data- and resource-intensive. However, recent work has demonstrated the effectiveness of combining weight pruning with knowledge distillation to significantly reduce the cost of training LLM model families. Now researchers from Nvidia use Minitron compression strategy on two state-of-the-art models: Llama 3.1 8B and Mistral NeMo 12B, compressing them down to 4B and 8B parameters.\nDue to unavailability of original training data, researchers used a slightly fine-tuned version of the teacher model then the distillation is performed by minimizing KL divergence on the logits, with the original model as the teacher and the pruned model as the student. Further two distinct pruning strategies: (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning is used, and evaluate the results on common benchmarks from the LM Evaluation Harness. The models are then aligned with NeMo Aligner and tested in instruct-tuned versions.\nMinitorn compression strategy yields state-of-the-art Llama-3.1-Minitron-4B models and MN-Minitron-8B which outperforms all similarly sized models across the board on common language modeling benchmarks. In terms of runtime inference performance measured using TensorRT-LLM, the MN-Minitron-8B model provides an average speedup of 1.2× over the teacher Mistral NeMo 12B model. Similarly, the Llama-3.1-Minitron-4B models provide an average speedup of 2.7× and 1.8× for the depth and width pruned variants, respectively, compared to the teacher Llama 3.1 8B model.\nPaper : https://arxiv.org/pdf/2408.11039"
  },
  {
    "objectID": "posts/Strategist/Strategist.html",
    "href": "posts/Strategist/Strategist.html",
    "title": "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search",
    "section": "",
    "text": "Recent studies have demonstrated how Large Language Models (LLMs) can be utilized to learn skills for improved decision-making in interactive environments. However, learning skills in adversarial environments with multiple agents presents a significant challenge for LLMs, as it requires accounting for the responses of other players or environment to their actions.\nTo address this researchers have introduced a new method called Strategist that utilizes LLMs to acquire new skills for playing multi-agent games through a self-improvement process. Strategist method gathers quality feedback through self-play simulations with Monte Carlo tree search and LLM-based reflection, which can then be used to learn high-level strategic skills such as how to evaluate states that guide the low-level execution.\nStrategic process contains two improvement steps in each improvement cycle – the (1) reflection and idea generation step and (2) the strategy improvement step. During the idea generation step, prompt is used in LLM to reflect on simulated self-play feedback from previously evaluated strategies and generate possible improvement ideas to the strategies and add them to the idea queue. During the strategy improvement step, a strategy is selected from the strategy tree and an improvement idea from the idea queue and prompt the LLM to improve the strategy using the improvement idea. The improved strategy is then evaluated via self-play simulations, and we use the feedback and reward signals from the simulation to help guide future improvements.\nThe general goal in Strategist decision-making setting is to learn a good policy function in a sequential decision-making setting (generally formulated as a partially observable Markov decision game (POMDG)), which can be done by improving strategies associated with the policy function.\nStrategist methods help train agents with better performance than both traditional reinforcement learning-based approaches and other LLM-based skill learning approaches in the games of Game of Pure Strategy (GOPS) and Resistance: Avalon.\nPaper : https://arxiv.org/pdf/2408.10635\nCode : https://github.com/jonathanmli/Avalon-LLM"
  },
  {
    "objectID": "posts/Show-o/Show-o.html",
    "href": "posts/Show-o/Show-o.html",
    "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
    "section": "",
    "text": "Over the past few years, significant advancements have blossomed in the two key pillars of multimodal intelligence: understanding and generation. Recent works have tried to form a unified system that can handle both multimodal understanding and generation. However, existing attempts mainly treat each domain independently and often involve individual models responsible for understanding and generation separately.\nSo can one single transformer handle both multimodal understanding and generation? or can one single transformer involve both autoregressive and diffusion modeling?\nTo address this researchers have introduced Show-o, which unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities.\nFirst the input data, regardless of its modalities, is tokenized and then prompted into a formatted input sequence. Then Show-o processes text tokens autoregressively with causal attention and image tokens in (discrete) denoising diffusion modeling via full attention, and then generates the desired output. Specifically, Show-o is capable of handling image captioning, visual question answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed modality generation.\nAcross various benchmarks, Show-o demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model.\nPaper : https://arxiv.org/pdf/2408.12528\nCode : https://github.com/showlab/Show-o"
  },
  {
    "objectID": "posts/Eagle/Eagle.html",
    "href": "posts/Eagle/Eagle.html",
    "title": "GEagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders",
    "section": "",
    "text": "The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts.\nIn order to address this issue researchers investigate the MLLM design space with multiple vision encoders, aiming to identify optimized design choices and improve the MLLM perception including various types of vision experts pre-trained on different tasks and resolutions. Next series of popular fusion strategies were compared under controlled settings, including Sequence Append, Channel Concatenation, LLaVA-HR, Mini-Gemini, and Deformable Attention. Finally, the optimal fusion design is further extended to multiple vision encoders to form a strong MLLM perception.\nResearchers then conclude their findings into a family of Vision-Centric High-Resolution Multimodal LLMs called Eagles, which presents a thorough exploration to strengthen multimodal LLM perception with a mixture of vision encoders and different input resolutions. The model contains a channel-concatenation-based “CLIP+X” fusion for vision experts with different architectures (ViT/ConvNets) and knowledge (detection/segmentation/OCR/SSL). The resulting family of Eagle models support up to over 1K input resolution and obtain strong results on multimodal LLM benchmarks, especially resolution-sensitive tasks such as optical character recognition and document understanding.\nPaper : https://arxiv.org/pdf/2408.15998"
  },
  {
    "objectID": "posts/ToxicDetector/ToxicDetector.html",
    "href": "posts/ToxicDetector/ToxicDetector.html",
    "title": "Efficient Detection of Toxic Prompts in Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) like ChatGPT and Gemini have significantly advanced natural language processing. However, these models can be exploited by malicious individuals who craft toxic prompts to elicit harmful or unethical responses. These individuals often employ jailbreaking techniques to bypass safety mechanisms, highlighting the need for robust toxic prompt detection methods. Existing detection techniques, both blackbox and whitebox, face challenges related to the diversity of toxic prompts, scalability, and computational efficiency.\nNow researchers have introduced ToxicDetector, a lightweight greybox method designed to efficiently detect toxic prompts in LLMs. ToxicDetector operates through a streamlined workflow that begins with the automatic creation of toxic concept prompts using LLMs from given toxic prompt samples. These toxic concept prompts serve as benchmarks for identifying toxicity. For each input prompt, ToxicDetector extracts embedding vectors from the last token of every layer of the model and calculates the inner product with the corresponding concept embedding. The highest inner product value for each layer is then combined to form a feature vector. This feature vector is then fed into an MLP classifier, which outputs a binary decision indicating whether the prompt is toxic or not. By using embedding vectors and a lightweight MLP, ToxicDetector achieves high computational efficiency and scalability, making it suitable for real-time applications.\nDuring evaluation on various versions of the LLama models, Gemma-2, and multiple datasets, Toxic Detector achieves a high accuracy of 96.39% and a low false positive rate of 2.00%, outperforming state-of-the-art methods. Additionally, Toxic Detector’s processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications. ToxicDetector achieves high accuracy, efficiency, and scalability, making it a practical method for toxic prompt detection in LLMs.\nPaper : https://arxiv.org/pdf/2408.11727"
  },
  {
    "objectID": "posts/Generative Verifiers/Generative Verifiers.html",
    "href": "posts/Generative Verifiers/Generative Verifiers.html",
    "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
    "section": "",
    "text": "While large language models (LLMs) demonstrate remarkable capabilities, they often confidently make logical and factual mistakes, which can invalidate the entire solution. A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs.\nTo overcome this limitation, researchers from Google have instead proposed training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional inference-time computation via majority voting for better verification.\nGenerative verifiers, namely GenRM and GenRM-CoT, start with a given question and a candidate solution, GenRM directly finetunes an LLM to answer the question ‘Is the answer correct (Yes/No)?’ via SFT on the next-token response corresponding to either ‘Yes’ or ‘No’. During inference, the verifier score is obtained by extracting the probability of the ‘Yes’ token. In comparison, GenRM-CoT finetunes a LLM to produce verification chain-of-thought (CoT) rationale before yielding the final Yes/No token. At test-time, multiple samples of CoT are rational and use majority voting to compute the average probability of ‘Yes’, enabling GenRM-CoT to utilize additional inference-compute for better verification.\nGemma-based was verified on algorithmic and grade-school math reasoning tasks, and GenRM outperforms discriminative verifiers and LLM-asa-Judge, showing a 16−64% improvement in the percentage of problems solved with Best-of-N. Furthermore, GenRM was able to scale favorably across dataset size, model capacity, and inference-time compute.\nPaper : https://arxiv.org/pdf/2408.15240"
  },
  {
    "objectID": "posts/DocOwl2/DocOwl2.html",
    "href": "posts/DocOwl2/DocOwl2.html",
    "title": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding",
    "section": "",
    "text": "Multimodel Large Language Models(MLLMs) have achieved promising OCR free Document Understanding performance by increasing the supported resolution of document images. However, this comes at the cost of generating thousands of visual tokens for a single document image, leading to excessive GPU memory and slower inference times, particularly in multi-page document comprehension.\nTo address these challenges, researchers have proposed DocOwl2, a High-resolution Doc Compressor module to compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. DocOwl2 strengthen multi-page document comprehension ability and balance both token efficiency and question-answering performance.\nDocOwl2 leverages a Shape-adaptive Cropping Module and a low-resolution vision encoder to encode high-resolution document images. Then, it utilizes a vision-to-text module H-Reducer to ensemble horizontal visual features and align the dimension of vision features with Large Language Models. Furthermore, a high-resolution compressor is designed to greatly reduce the number of visual features while maintaining most visual information. Finally, compressed visual tokens of multiple images/pages are concatenated with text instructions and input to a Large Language Model for multimodal understanding.\nDocOwl2 sets a new state-of-the-art across multi-page document understanding benchmarks and reduces first token latency by more than 50%, demonstrating advanced capabilities in multi-page questioning answering, explanation with evidence pages, and cross-page structure understanding. Additionally, compared to single-image MLLMs trained on similar data, DocOwl2 achieves comparable single-page understanding performance with less than 20% of the visual tokens.\nPaper : https://arxiv.org/pdf/2409.03420"
  },
  {
    "objectID": "posts/GraphRAG auto-tuning/GraphRAG auto-tuning.html",
    "href": "posts/GraphRAG auto-tuning/GraphRAG auto-tuning.html",
    "title": "GraphRAG auto-tuning provides rapid adaptation to new domains",
    "section": "",
    "text": "GraphRAG uses large language models (LLMs), guided by a set of domain-specific prompts, to create a comprehensive knowledge graph that details entities and their relationships and then uses semantic structure of the data to generate responses to complex queries.\nManually creating and tuning such a set of domain-specific prompts is time-consuming. To streamline this process, researchers from Microsoft have developed an Auto-tuning tool in GraphRAG, which automated generation and fine tuning of domain-specific prompts.\nAuto-tuning starts by sending a sample of the source content to the LLM, which first identifies the domain and then creates an appropriate persona—used with downstream agents to tune the extraction process. Once the domain and persona are established, several processes occur in parallel to create custom indexing prompts. This way, the few-shot prompts are generated based on the actual domain data and from the persona’s perspective.\nAuto-tuning follows a human-like approach; provided an LLM with a sample of text data (e.g., 1% of 10,000 chemistry papers) and instructed it to produce the prompts it deemed most applicable to the content. Now, with these automatically generated and tuned prompts, one can immediately apply GraphRAG to a new domain of choosing, confident that we’ll get high-quality results.\nBlog : GraphRAG auto tuning"
  },
  {
    "objectID": "posts/MemoRAG/MemoRAG.html",
    "href": "posts/MemoRAG/MemoRAG.html",
    "title": "MemoRAG: moving towards next-gen rag via memory-inspired knowledge discovery",
    "section": "",
    "text": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access external databases, thereby enhancing the generation quality of large language models (LLMs) through optimized context. However, the existing retrieval methods are constrained inherently, as they can only perform relevance matching between explicitly stated queries and well-formed knowledge, but unable to handle tasks involving ambiguous information needs or unstructured knowledge. Consequently, existing RAG systems are primarily effective for straightforward question-answering tasks.\nTo address this researchers have proposed MemoRAG, a novel retrieval augmented generation paradigm empowered by long-term memory. For a given input query Standard RAG struggles to accurately locate the necessary evidence due to the implicit nature of the input query, resulting in a less accurate answer. Whereas, MemoRAG constructs a global memory over the whole database. When presented with the query, MemoRAG first recalls relevant clues, enabling useful information to be retrieved and thus leading to a precise and comprehensive answer.\nTo achieve this MemoRAG adopts a dual-system architecture. On the one hand, it employs a light but long range LLM to form the global memory of the database. Once a task is presented, it generates draft answers, cluing the retrieval tools to locate useful information within the database. On the other hand, it leverages an expensive but expressive LLM, which generates the ultimate answer based on the retrieved information.\nDuring experiment, MemoRAG achieves superior performance across a variety of evaluation tasks, including both complex ones where conventional RAG fails and straightforward ones where RAG is commonly applied.\nPaper : https://arxiv.org/pdf/2409.05591"
  },
  {
    "objectID": "posts/AWM/AWM.html",
    "href": "posts/AWM/AWM.html",
    "title": "Agent Workflow Memory (AWM)",
    "section": "",
    "text": "Recently, LLM-based agents have shown promise for real-world tasks like web navigation, but they still struggle with complex, long-term tasks. Unlike these models, humans however excel at tackling intricate tasks by learning and reusing past experiences to guide future actions.\nTo build agents that can similarly benefit from this process researchers have introduced Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations.\nAWM starts with a basic set of built-in actions and solves new tasks in a streaming manner, continuously inducing workflows from the task at hand, e.g., learning to “find a place by its name” from the first few examples. Moreover, AWM continues to build more complex workflows on top of new experiences and previously acquired workflows. For example, the “find a place by its name” workflow, once induced, effectively serves as a subgoal to build a more complex workflow “get the zip code of a place.” Such continual learning mechanisms create a snowball effect to induce and apply increasingly complex workflows while expanding the agent memory, often yielding a substantial performance gap over a vanilla agent that does not adapt.\nFurther, AWM flexibly applies to both offline and online scenarios offline: when additional (e.g., training) examples are available, agents induce workflows from ground-truth annotated examples and online: without any auxiliary data, agents induce workflows from past experiences on the fly.\nWhile evaluating, AWM was experimented on two major web navigation benchmarks — Mind2Web and WebArena. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.\nPaper : https://arxiv.org/pdf/2409.07429"
  },
  {
    "objectID": "posts/OneGen/OneGen.html",
    "href": "posts/OneGen/OneGen.html",
    "title": "OneGen: efficient one-pass unified generation and retrieval for llms",
    "section": "",
    "text": "Despite the recent advancements in Large Language Models (LLMs), which have significantly enhanced the generative capabilities for various NLP tasks, LLMs still face limitations in directly handling retrieval tasks. However, many practical applications demand the seamless integration of both retrieval and generation.\nTo address this researchers have introduced a novel and efficient One-pass Generation and retrieval framework (OneGen), designed to improve LLMs’ performance on tasks that require both generation and retrieval. The proposed framework bridges the traditionally separate training approaches for generation and retrieval by incorporating retrieval tokens generated autoregressive. This enables a single LLM to handle both tasks simultaneously in a unified forward pass.\nOneGen uses special tokens called retrieval tokens and allocates the retrieval task to retrieve tokens generated in an autoregressive manner. During training, retrieval tokens only participate in representation fine tuning through contrastive learning, whereas other output tokens are trained using language model objectives. At inference time, retrieval tokens are used for efficient retrieving on demand.\nOneGen was evaluated on two distinct types of composite tasks, RAG and Entity Linking, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, results show that integrating generation and retrieval within the same context preserves the generative capabilities of LLMs while improving retrieval performance. Thus making OneGen first of its kind to enable LLMs to conduct vector retrieval during the generation.\nPaper : https://arxiv.org/pdf/2409.05152\nCode : https://github.com/zjunlp/OneGen"
  },
  {
    "objectID": "posts/scot/scot.html",
    "href": "posts/scot/scot.html",
    "title": "Self-Harmonized Chain of Thought",
    "section": "",
    "text": "Chain-of-thought (CoT) prompting reveals that large language models are capable of performing complex reasoning via intermediate steps. CoT methods in large language models typically use two prompting paradigms: Zero-shotCoT, approach utilizes straightforward prompts like “Let’s think step by step” to generate a sequential thought process before yielding an answer. Whereas the Few-shot-CoT approach makes use of human-crafted, step-by-step demonstrations to guide the model’s reasoning process.\nThis approach sometimes leads to reasoning errors to reasoning errors, highlighting the need to diversify demonstrations to mitigate its misleading effects. However, diverse demonstrations pose challenges for effective representations.\nTo address this researchers have proposed ECHO (Self-Harmonized Chain of Thought), a self-harmonized chain-of-thought prompting method. ECHO consists of three main steps: * question clustering: partition questions of a given dataset into a few clusters based on their similarity * demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-shotCoT. * demonstration unification: one demonstration is randomly selected for rationale update in each iteration, while the remaining demonstrations serve as in-context examples.\nThis unification process forces each rationale to learn from the remaining ones to build a coherent pattern. The process iteratively cycles through each demonstration once per iteration and continues over multiple iterations\nDuring evaluation across three different reasoning domains ECHO demonstrates better overall performance (+2.8%) than other baselines. ECHO diverse solution paths into a uniform and effective solution pattern.\nPaper : https://arxiv.org/pdf/2409.04057"
  },
  {
    "objectID": "posts/RetrievalAttention/RetrievalAttention.html",
    "href": "posts/RetrievalAttention/RetrievalAttention.html",
    "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
    "section": "",
    "text": "Transformer-based large Language Models (LLMs) become increasingly important in various domains. However, the quadratic time complexity of attention operation poses a significant challenge for scaling to longer contexts due to the extremely high inference latency and GPU memory consumption for caching key-value (KV) vectors.\nTo address this researcher has proposed RetrievalAttention, a training-free approach to accelerate attention computation. To leverage the dynamic sparse property of attention, RetrievalAttention builds approximate nearest neighbor search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most relevant ones via vector search during generation. Due to the out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf ANNS indexes still need to scan O(N) (usually 30% of all keys) data for accurate retrieval, which fails to exploit the high sparsity. RetrievalAttention first identifies the OOD challenge of ANNS-based attention, and addresses it via an attention-aware vector search algorithm that can adapt to queries and only access 1–3% of data, thus achieving a sub-linear time complexity.\nTo optimize resource utilization, RetrievalAttention retains KV vectors in the GPU memory following static patterns, while offloading the majority of KV vectors to CPU memory for index construction. During token generation, RetrievalAttention efficiently retrieves critical tokens using vector indexes on the CPU and merges the partial attention results from both the CPU and GPU. This strategy enables RetrievalAttention to perform attention computation with reduced latency and minimal GPU memory utilization.\nRetrievalAttention were evaluated for accuracy and efficiency on both commodity GPUs (4090) and high-end GPUs (A100) on three long-context LLMs across various long-context benchmarks like ∞-Bench and RULER. For the 128K context on the 4090 GPU, RetrievalAttention achieves 4.9× and 1.98× decoding-latency reduction compared to the retrieval method based on exact KNN and traditional ANNS indexing respectively, while maintaining the same accuracy as full attention. RetrievalAttention only needs 16GB GPU memory for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).\nPaper : https://arxiv.org/pdf/2409.10516"
  },
  {
    "objectID": "posts/Promptrieve/Promptrieve.html",
    "href": "posts/Promptrieve/Promptrieve.html",
    "title": "Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models",
    "section": "",
    "text": "Modern information retrieval (IR) models generally match queries to passages based on a single semantic similarity score. This can make the search experience confusing for users, who often have to experiment with keywords, filters, and refine their searches multiple times to find the exact information they need.\nTo overcome this researchers have proposed Promptriever, which can handle complex instructions including detailed relevance definitions and zero-shot prompting techniques that act as a form of zero-shot hyperparameter optimization, similar to prompting LMs. Promptriever is a bi-encoder retriever with LLaMA-2 7B LLM as backbone and trained on a synthetic dataset of ~500K query-passage relevance pairs augmented with instance-level instructions.\nTo generate instruction-based retrieval data for Promptriever training researchers took the initial query and relevant passage and prompted an LM to generate an instruction that would match that query. Then LM generates an example relevant and non-relevant passage for that query and instruction. With these two sets of instructions being generated: Instruction-positive, (query, passage) which fulfills the extra requirement and Instruction-negative, where the (query, passage) pair is highly relevant if viewed in isolation, but, the addition of a carefully constructed instruction significantly decreases that relevance.Further, multiple types of instructions (both in length and style) is generated for training set diversity.\nDuring evaluation Promptriever not only achieves strong performance on standard retrieval tasks, but also follows instructions. Further, large gains (reaching SoTA) on detailed relevance instructions (+14.3 p-MRR / +3.1 nDCG on FollowIR) was observed. Significantly increased robustness to lexical choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR) were also observed. Overall Promptriever demonstrates that retrieval models can be controlled with prompts on a per query basis, setting the stage for future work aligning LLM prompting techniques with information retrieval.\nPaper : https://arxiv.org/pdf/2409.11136"
  },
  {
    "objectID": "posts/jina-embeddings-v3/jina-embeddings-v3.html",
    "href": "posts/jina-embeddings-v3/jina-embeddings-v3.html",
    "title": "Training Language Models to Self-Correct via Reinforcement Learning",
    "section": "",
    "text": "Recently, jina.ai have released jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Additionally, Matryoshka Representation Learning is integrated into the training process, allowing flexible truncation of embedding dimensions without compromising performance.\nThe architecture of jina-embeddings-v3 is based on the XLM-RoBERTa model, with several key modifications. FlashAttention 2 is integrated for enhanced computational efficiency, while RoPE extends support for sequences up to 8192 tokens. Task-specific LoRA adapters are used to optimize embeddings for various tasks. The model’s input consists of two parts: the text, which is the long document to be embedded, and the task type. jina-embeddings-v3 supports four tasks and implements five adapters to choose from: retrieval.query and retrieval.passage for query and passage embeddings in asymmetric retrieval tasks, separation for clustering and reranking tasks, classification for classification tasks, and text-matching for tasks involving semantic similarity, such as STS or symmetric retrieval.\nEvaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks.\nPaper : https://arxiv.org/pdf/2409.10173"
  },
  {
    "objectID": "posts/SCoRe/SCoRe.html",
    "href": "posts/SCoRe/SCoRe.html",
    "title": "Training Language Models to Self-Correct via Reinforcement Learning",
    "section": "",
    "text": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches toward self-correcting LLMs either rely on prompt-engineering or fine-tuning models specifically for self-correction, which usually require oracle “teacher” supervision to guide the process of self-correction.\nTo this end, researchers from Google Deepmind have developed a multi-turn online reinforcement learning (RL) approach, Self-Correction via Reinforcement Learning (SCoRe), which trains only a single model that can both produce a response to a reasoning problem and also correct errors despite not receiving any oracle feedback. More importantly, SCoRe teaches this ability to model entirely by training on self-generated data, without any oracle.\nHow does SCoRe work? SCoRe tackles the challenges of supervised fine-tuning (SFT) by employing online multi-turn reinforcement learning (RL). It generates its own training data to mitigate distribution mismatches between training and inference. SCoRe is trained in two stages to prevent minimal editing strategies. The first stage optimizes a model initialization focused on correction performance while maintaining closeness to the base model. The second stage employs multi-turn RL to enhance responses, using a reward bonus to incentivize improvements from the first attempt to the second. This approach ensures the model learns to self-correct effectively rather than just slightly editing initial responses.\nSCoRe was evaluated on Gemini 1.0 Pro and 1.5 Flash models, achieving state-of-the-art self-correction performance, improving the base models’ self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.\nPaper : https://arxiv.org/pdf/2409.12917"
  },
  {
    "objectID": "posts/Contextual Retrieval/Contextual Retrieval.html",
    "href": "posts/Contextual Retrieval/Contextual Retrieval.html",
    "title": "Introducing Contextual Retrieval",
    "section": "",
    "text": "In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context.\nTo address this Anthropic has introduced Contextual Retrieval, which solves this problem by prepending chunk-specific explanatory context to each chunk before embedding (“Contextual Embeddings”) and creating the BM25 index (“Contextual BM25”).\nLet’s first understand how we can build RAG solutions more accurately, which retrieve the most applicable chunks by combining the embeddings and BM25 techniques. It usually start with following steps: * (1) Break down the knowledge base (the “corpus” of documents) into smaller chunks of text, usually no more than a few hundred tokens; * (2) Create TF-IDF encodings and semantic embeddings for these chunks; * (3) Use BM25 to find top chunks based on exact matches; * (4) Use embeddings to find top chunks based on semantic similarity; * (5) Combine and deduplicate results from (3) and (4) using rank fusion techniques; * (6) Add the top-K chunks to the prompt to generate the response.\nBy leveraging both BM25 and embedding models, traditional RAG systems can provide more comprehensive and accurate results, balancing precise term matching with broader semantic understanding.\nNow let’s implement Contextual Retrieval, by writing a prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document. The resulting contextual text, usually 50-100 tokens, is prepended to the chunk before embedding it and before creating the BM25 index. Retrieval accuracy can further be improved using a reranking model. Whereas Prompt Caching to reduce the costs of Contextual Retrieval as you don’t need to pass in the reference document for every chunk.\nDuring evaluation Contextual Retrieval reduces the number of failed retrievals by 49% and, when combined with reranking, by 67%. Further, Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35% (5.7% → 3.7%). These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks.\nBlog : https://www.anthropic.com/news/contextual-retrieval\nCode : https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb"
  },
  {
    "objectID": "posts/bge-en-icl/bge-en-icl.html",
    "href": "posts/bge-en-icl/bge-en-icl.html",
    "title": "Making Text Embedders Few-Shot Learners",
    "section": "",
    "text": "LLM-based embedding models have demonstrated remarkable improvements in in-domain accuracy and generalization, particularly when trained using supervised learning approaches . However, despite these advances, embedding models still struggle to follow unseen task instructions and execute complex retrieval tasks.\nOn other hand LLMs with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context.\nSo can we leverage the ICL feature in LLMs to enhance the process of text embedding generation?\nTo this end, researchers have introduced a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. This approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks.\nFinally, all this is done through fewshot contrastive training. Consider a query-passage pair (qi , pi) in an embedding task. first construct an example template as follows:\n⟨Instruct⟩ {task definition} ⟨query⟩ {qi} ⟨response⟩ {pi}\nHere, ”task definition” represents the description of the specific embedding task. This example template is applied to new input queries for each embedding task. For a relevant querypassage pair (q+, p +), the modified query q + exp is constructed as follows:\n{example 1} ... {example n} ⟨Instruct⟩ {task definition} ⟨query⟩ {q +} ⟨response⟩\nAll modified queries and passages in the corpus are encoded using the same LLM to obtain their embedding representations. Specifically, [EOS] token is appended to the end of the input modified queries and passages, feeding them into the LLM to obtain embeddings (hq + exp , hp+ ) by extracting the final layer’s [EOS] vector. Lastly a standard InfoNCE loss function L is applied, utilizing both in-batch negatives and hard negatives for training.\nThis approach necessitates no modifications to the model’s architecture; instead, it involves altering the prompt on the query side to include in-context learning features in the embedding generation task. Despite its simplicity, it proves highly effective on the MTEB and AIR-Bench benchmarks.\nPaper : https://arxiv.org/pdf/2409.15700"
  },
  {
    "objectID": "posts/SOS-BENCH/SOS-BENCH.html",
    "href": "posts/SOS-BENCH/SOS-BENCH.html",
    "title": "Style over Substance: failure modes of LLM judges in alignment benchmarking.",
    "section": "",
    "text": "Recently LLM-judge benchmarks such as MT-Bench, Alpaca Eval, and Arena-Hard-Auto have been a go to tool to simultaneously automate evaluation of LLMs while also aligning with human preference. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences.\nSo one may ask – do LLM-judge preferences translate to progress on other, more concrete metrics for alignment, and if not, why not?\nTo answer this research have introduced SOS-BENCH, the largest standardized, reproducible LLM meta-benchmark to date. SOS-BENCH, a new alignment benchmark with ground truth, designed to gauge progress on alignment with helpful, honest, harmless (HHH) principles. SOS-BENCH combines 19 existing world knowledge, instruction following, and safety benchmarks for a holistic view of model performance\nThe LLM-judge pipeline is more complex than that of standard benchmarks, rather than relying on an objective ground truth, preference benchmarks substitute the preferences of a judge. This introduces new potential confounds: (1) the choice of judge, (2) the instructions to the judge, and (3) implicit biases which can affect a judge’s stated preferences independent of any instructions.\nExperimentation was carried out on a series of post-trained LLAMA-3-8B base models, LLAMA-3 base without post-training, opt-125m, and several GPT checkpoints. The LLM-judge benchmark was ArenaHard-Auto, with standard settings, which uses GPT-4-0314 as a baseline model and GPT-4-1106-preview as a judge.\nFinally, researcher have summarize their finding as follow (1) LLM judgments do not correlate with concrete measures of safety, world knowledge, and instruction following; (2) LLM judges have powerful implicit biases, prioritizing style over factuality and safety; and (3) the supervised fine-tuning (SFT) stage of post-training, and not the PO stage, has the greatest impact on alignment, with data scaling and prompt diversity as the driving factors\nPaper : https://arxiv.org/pdf/2409.15268"
  },
  {
    "objectID": "posts/MaskLLM/MaskLLM.html",
    "href": "posts/MaskLLM/MaskLLM.html",
    "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
    "section": "",
    "text": "Large Language Models (LLMs) have demonstrated remarkable effectiveness across a diverse range of tasks. However, LLMs are usually distinguished by their massive parameter counts, which typically result in significant redundancy. One effective and practical approach to address this issue is semi-structured pruning, which introduces N:M sparsity into LLMs to improve both memory and computational efficiency.\nRecently, approaches such as SparseGPT and Wanda, utilize a small calibration set and carefully designed importance criteria to identify such redundant parameters. However , two substantial challenges remain: Firstly, the small calibration set is insufficient to represent the comprehensive knowledge embedded in LLMs and Secondly, using handcrafted criteria as a proxy for the true discrepancy inevitably results in errors.\nTo address this researchers have introduced MaskLLM, a learnable pruning method that establishes Semi-structured (or “N:M”) Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks.\nMaskLLM was assessed using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model’s 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM’s learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains.\nPaper : https://arxiv.org/pdf/2409.17481"
  },
  {
    "objectID": "posts/HyperAgent/HyperAgent.html",
    "href": "posts/HyperAgent/HyperAgent.html",
    "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale",
    "section": "",
    "text": "Large Language Models (LLMs) have revolutionized software engineering (SE), demonstrating remarkable capabilities in various coding tasks. While recent efforts have produced autonomous software agents based on LLMs for end-to-end development tasks, these systems are typically designed for specific SE tasks.\nResearchers have now introduced HyperAgent, a novel generalist multi-agent system designed to address a wide spectrum of SE tasks across different programming languages by mimicking human developers’ workflows.\nThe HyperAgent framework comprising four specialized agents—Planner, Navigator, Code Editor, and Executor—HyperAgent manages the full lifecycle of SE tasks, from initial conception to final verification.\n\nPlanner: Acts as the central decision-maker, processing task prompts and generating strategies. It coordinates the activities of the other agents and iteratively refines plans until tasks are completed or a limit is reached.\nNavigator: Focuses on fast information retrieval from codebases, using IDE-like tools to quickly address challenges in private or unfamiliar repositories.\nEditor: Handles code modification and generation across files. It creates and applies code patches based on input from the Planner, using various editing tools.\nExecutor: Validates solutions and reproduces issues, managing environment setup and testing through an interactive shell and access to documentation. Together, these agents streamline task management, code navigation, editing, and validation processes.\n\nThrough extensive evaluations, HyperAgent achieves state-of-the-art performance across diverse SE tasks: it attains a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods. Furthermore, HyperAgent demonstrates superior performance in code generation at repository scale (RepoExec), and in fault localization and program repair (Defects4J), often outperforming specialized systems.\nPaper : https://arxiv.org/pdf/2409.16299"
  },
  {
    "objectID": "posts/Backtracking/Backtracking.html",
    "href": "posts/Backtracking/Backtracking.html",
    "title": "Backtracking Improves Generation Safety",
    "section": "",
    "text": "LLM has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text.\nHow can we meaningfully improve language model safety, given that models will likely always produce some unsafe generations?\nTo address this researchers have proposed - backtracking, a novel technique that allows language models to “undo” and recover from their own unsafe generation through the introduction of a special [RESET] token. The introduction of this token allows the model to discard previously generated unsafe content and begin a new generation from a safer point. This backtracking mechanism can be incorporated into existing training frameworks, such as SFT or Direct Preference Optimization (DPO), enhancing the model’s ability to detect and recover from unsafe outputs. Unlike traditional prevention-based techniques, backtracking focuses on correction, enabling the model to adjust its behavior in real time.\nThe backtracking approach allows the language model to monitor its output and recognize when it begins to generate unsafe content. When this happens, the model emits a [RESET] token, which signals it to discard the hazardous portion of the text and restart from a safe position. This method is innovative in its ability to prevent a cascade of harmful content and its adaptability. The researchers trained their models using SFT and DPO techniques, ensuring that backtracking could be applied across various architectures and models. Incorporating this into standard language model training provides a seamless way for models to self-correct during the generation process without requiring manual intervention.\nDuring evaluations, the Llama-3-8B model trained with backtracking demonstrated a significant safety improvement, reducing the rate of unsafe outputs from 6.1% to just 1.5%. Similarly, the Gemma-2-2B model reduced unsafe output generation from 10.6% to 6.1%. Notably, these safety improvements did not come at the cost of the model’s usefulness. Overall, the backtracking method offers a novel solution to the problem of unsafe language model generations. Enabling models to discard unsafe outputs and generate new, safer responses addresses a critical gap in current safety techniques.\nPaper : MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models"
  },
  {
    "objectID": "posts/RULER/RULER.html",
    "href": "posts/RULER/RULER.html",
    "title": "RULER : A Model-Agnostic Method to Control Generated Length for Large Language Models",
    "section": "",
    "text": "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users’ needs due to their inherent difficulty in accurately perceiving numerical constraints.\nTo explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model’s performance in adhering to specified response lengths. Furthermore, researchers have also introduced RULER as a model-agnostic method designed to improve the ability of large language models (LLMs) to follow instructions, particularly in generating responses of specific lengths. It uses Meta Length Tokens (MLTs) to control response lengths, enabling LLMs to meet target lengths during both training and inference. A dataset, DMLT, was created for training, where models learn to generate MLTs and corresponding responses. During inference, RULER generates MLTs based on the desired response length or creates one if none is provided.\nSpecifically, RULER equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, RULER can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization.\nComprehensive experiments show the effectiveness of RULER across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of RULER.\nPaper : Backtracking Improves Generation Safety"
  },
  {
    "objectID": "posts/Structrag/Structrag.html",
    "href": "posts/Structrag/Structrag.html",
    "title": "Structrag: Boosting Knowledge Intensive Reasoning Of Llms Via Inference-Time Hybrid Information Structurization",
    "section": "",
    "text": "Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation.\nRecently, LLMs have been explored for human-like thinking processes to transform scattered information into various structure formats during inference, thereby better serving knowledge-intensive reasoning tasks.\nMotivated by this, researchers have proposed StructRAG, which employs a hybrid information structuring mechanism to construct and utilize structured knowledge in the most suitable format based on task requirements.\nThe StructRAG framework consists of three sequential modules aimed at effectively processing and utilizing structured knowledge. 1. Hybrid Structure Router: This module identifies the most suitable structure type for a given task based on the question and document information. 2. LLM-based Scattered Knowledge Structurizer: This component converts raw documents into structured knowledge, leveraging strong comprehension and generation capabilities. 3. Structured Knowledge Utilizer: This final module handles complex questions by decomposing them and extracting relevant knowledge, enhancing the accuracy of the final answer. Together, these modules optimize the use of structured knowledge for knowledge-intensive reasoning tasks.\nFurthermore, in order to get a high-performance hybrid structure router, training data were construct by a synthesizing-simulating-judging pipeline and then implement preference training via DPO algorithm. Experiments on extensive knowledge-intensive reasoning tasks demonstrate that StructRAG is an effective solution, which reaches the SOTA performance and can achieve large improvement in badly information-scattered scenarios.\nPaper: StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-Time Hybrid Information Structurization"
  },
  {
    "objectID": "posts/BitNeta48/BitNeta48.html",
    "href": "posts/BitNeta48/BitNeta48.html",
    "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
    "section": "",
    "text": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. This is mainly achieved by utilizing activation sparsity, which reduces the inference FLOPs and the I/O of weight by pruning the activation entries with smaller magnitudes. In addition to sparsification, the activation quantization approach is also used to accelerate the matrix multiplication.\nContinuing on this work researchers have introduced BitNet a4.8, a hybrid quantization and sparsification strategy that enables 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, BitNet a4.8 employs 4-bit activations for the inputs to attention and FFN, while utilizing sparsification with 8 bits for intermediate states. To improve the training efficiency, BitNet a4.8 is trained from 8-bit to 4-bit activations with a two-stage recipe, which requires only a few training tokens to adapt BitNet b1.58 to the low-bit activations at the end of training.\nExtensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.\nPaper : https://arxiv.org/pdf/2411.04965"
  },
  {
    "objectID": "posts/NEKO/NEKO.html",
    "href": "posts/NEKO/NEKO.html",
    "title": "NEKO: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts",
    "section": "",
    "text": "The challenge in building a general-purpose post recognition error corrector, which is required to evaluate your fine tuned models on a custom dataset, is how to train a model effectively on diverse domain datasets. The solution involves learning domain-specific features and integrating this knowledge into one model. However, previous approaches have used separate correction models for each domain, which leads to a substantial increase in the number of model parameters.\nTo address this, researchers from Nvidia have proposed NEKO (geNErative multi-tasK error cOrrection), a new form multi-task model to boost post-recognition results over speech, text, and visual inputs.NEKO leverages a pre-trained MoE model to drive diverse tasks and cross-domain knowledge. The key idea is to continuously pretrain the MoE model on a mixture of error correction datasets, with each expert specializing in a specific domain. This task-oriented MoE fine-tuning approach enables the experts to capture task-specific features while allowing knowledge sharing through the router\nArchitecturally NeKo integrates MoE layers within a Transformer architecture. In a MoE layer, each input token is assigned to a subset of experts by a gating network (router). The output of the MoE layer is the weighted sum of the outputs of the selected experts, where the weights are determined by the gating network. During inference, NeKOo does not assume knowledge of the specific task an input belongs to and each token is routed to the top-2 experts solely based on their router probabilities.\nNEKO could work for (i) post automatic speech recognition (ASR) correction, (ii) post speech translation (ST) and machine translation (MT) correction, and (iii) post optical character recognition (OCR) correction. NeKo discovered new state-of-the-art results in (iv) zero-shot ASR correction and performs competitively as a general-purpose (v) multi-task corrector.\nExperiments on the Open ASR Leaderboard show that NeKo achieved a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.\nPaper: NEKO: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts"
  },
  {
    "objectID": "posts/JanusFlow/JanusFlow.html",
    "href": "posts/JanusFlow/JanusFlow.html",
    "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
    "section": "",
    "text": "Recently there has been growing trends of developing sophisticated LLM models specialized in both image comprehension and text-to-image generation. This is achieved typically by incorporating either diffusion models or vector-quantized autoregressive models. Another approach builds upon recent breakthroughs in rectified flow models, which provide a simple framework for generative modeling while delivering exceptional empirical performance.\nNow researchers have proposed JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Architecturally JanusFlow requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. For visual understanding, the LLM performs autoregressive next-token prediction to generate responses. For image generation, the LLM employs images with rectified flow. Starting from Gaussian noise at 𝑡 = 0, the LLM iteratively updates 𝑧𝑡 by predicting velocity vectors until reaching 𝑡 = 1. We omit the VAE encoder, the skip connection leveraged in generation and the linear layer after 𝑓𝑒𝑛𝑐 for simplicity.\nTo further optimize JanusFlow’s performance, two key strategies have been implemented: First, a separate vision encoder is maintained for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, the intermediate representations between generation and understanding modules are aligned during training, strengthening semantic coherence in the generation process.\nExtensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. Specifically, on text-to-image generation benchmarks, MJHQ FID-30k, GenEval and DPG-Bench, JanusFlow achieves scores of 9.51, 0.63 and 80.09%. In multimodal comprehension benchmarks, JanusFlow attains scores of 74.9, 70.5 and 60.3 on MMBench, SeedBench, and GQA, respectively, exceeding specialized models such as LLaVA-v1.5 and Qwen-VL-Chat.\nPlease refer to the research paper for more details."
  },
  {
    "objectID": "posts/SEALONG/SEALONG.html",
    "href": "posts/SEALONG/SEALONG.html",
    "title": "SEALONG: Large Language Models Can Self-Improve in Long-context Reasoning",
    "section": "",
    "text": "Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements.\nSo can LLMs self improve in long-context reasoning ?\nTo address this issue, researchers have proposed a Self-improving method for rEAsoning over LONGcontexts (SEALONG). SEALONG consists of two stages: self-supervision creation and fine-tuning. Given a long context and a corresponding query, multiple outputs are sampled, each assigned a score based on Minimum Bayes Risk. Fine-tuning is then conducted using either the highest-scoring output for supervised fine-tuning or both high-scoring and low-scoring outputs for preference optimization. The idea is that reasoning paths differing from the majority are more likely to be hallucinations. Based on this, one can improve model performance by either fine-tuning with high-quality outputs or using preference optimization that incorporates both high- and low-quality outputs.\nExtensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG. Specifically, SEALONG raises the score of Llama-3.1-8B Instruct from 50.8 to 55.0. Additionally, SEALONG enabled Qwen-2.5-14BInstruct to outperform its 32B variant In comparison to previous synthetic data, SEALONG demonstrated notable improvement without requiring human or expert model annotation.\nPaper : Large Language Models Can Self-Improve in Long-context Reasoning"
  },
  {
    "objectID": "posts/IOPO/IOPO.html",
    "href": "posts/IOPO/IOPO.html",
    "title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization",
    "section": "",
    "text": "Recently, the ability to follow complex instructions with multiple constraints is gaining increasing attention as LLMs are deployed in sophisticated real-world applications. However, most benchmarks lay emphasis on evaluating LLMs’ ability to follow complex instructions, and lack of algorithms tailored for enhancing the corresponding ability.\nTo address this researchers have introduced IOPO (Input-Output Preference Optimization), a novel technique to empower large language models (LLMs) with complex instruction following capabilities. Typically, LLMs are trained on a massive amount of text data, which allows them to become very capable at generating human-like text. However, this training process doesn’t necessarily ensure that the model will behave in alignment with human values and preferences when given complex instructions.\nIOPO tries to address this by explicitly optimizing the LLM’s preferences during training. The researchers collect information about what users consider “good” or “desirable” responses to certain inputs, and then use this to guide the model’s training. This helps the LLM learn to prioritize outputs that are more aligned with human preferences.\nThe core innovation of IOPO is to optimize the LLM’s preferences over input-output pairs during training, rather than just maximizing the likelihood of the training data. This is achieved through a two-stage training process: 1. Preference Collection: The researchers first collect information about user preferences for different input-output pairs, using techniques like FIPO and Triple Preference Optimization. 2. Preference Optimization: The LLM is then trained using a novel objective function that encourages the model to generate outputs that are more aligned with the collected user preferences. This is implemented through direct preference optimization techniques.\nExtensive experiments on both in-domain and out of-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on out of-domain data compared to SFT and DPO respectively.\nPaper : IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization"
  },
  {
    "objectID": "posts/FlexibleGuardrail/FlexibleGuardrail.html",
    "href": "posts/FlexibleGuardrail/FlexibleGuardrail.html",
    "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
    "section": "",
    "text": "Large Language Models (LLMs) are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that isn’t available in pre-production.\nTo address this researchers have introduced Flexible Guardrail Development Methodology, which applied to the challenge of off-topic prompt detection for LLMs. This is achieved in 3 phases. First, by thoroughly defining the problem space qualitatively and leveraging an LLM to generate a diverse set of prompts, a synthetic dataset is constructed that serves both as a benchmark and a training resource for off-topic guardrails. Second, a Fine-tuning embedding or cross-encoder model on this synthetic data outperforms heuristic approaches by reducing false positives, and enhancing potential adoption. Finally, by framing this as classifying whether the user prompt is relevant to the system prompt, our guardrail generalises effectively to other misuse categories, including jailbreak and harmful prompts.\nExperiments demonstrate that the proposed flexible, data-free guardrail development methodology is effective in detecting offtopic prompts. The fine-tuned classifiers outperform baseline methods in both precision and recall, reducing false positives and enhancing adaptability. By framing the detection task as assessing the relevance between the system prompt and the user prompt, our guardrail generalises effectively to other misuse categories, including jailbreak and harmful prompts.\nPaper : A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection"
  },
  {
    "objectID": "posts/VisualLens/VisualLens.html",
    "href": "posts/VisualLens/VisualLens.html",
    "title": "VisualLens: Personalization through Visual History",
    "section": "",
    "text": "Imagine a personal assistant observing what you do in your daily life. When you ask for recommendations on anything from restaurants and activities to movies, books, and products, based on her in-depth understanding of you she will provide suggestions tailored specifically to your tastes.\nNow researchers from Meta have put this in action with the introduction of VisualLens, a novel approach that extracts, filters, and refines image representations, and leverages these signals for personalization.\nAt the core of the solution is a set of models that effectively extract the essential signals from the visual history to infer the user’s preferences. First, given a recommendation request, VisualLens selectively retrieves only the most relevant images, thereby reducing unnecessary noises and distractions. Second, to effectively capture the signals a photo may convey, VisualLens uses not only visual embeddings but also text captions and aspect words extracted from the image. Third, VisualLens adopts an iterative refinement process that continually improves aspect extraction to better reflect user interests and inform recommendations. By jointly training aspect word extraction and prediction tasks within a unified model, VisualLens not only reduces the overall parameter size, but also enhances the model’s capability to understand and utilize the visual history for accurate recommendation.\nA comprehensive experimental study shows promising recommendation quality of VisualLens. It achieved 82-91% Hit@10 on the Google Review-V and Yelp-V benchmarks, out-performing state-of-the-art UniMP by ∼10%. Even compared with GPT-4o, 8B model outperforms on all metrics, improving Hit@3 by 1.6% and 4.6% respectively on the two benchmarks.\nPaper : VisualLens: Personalization through Visual History"
  },
  {
    "objectID": "posts/StarAttention/StarAttention.html",
    "href": "posts/StarAttention/StarAttention.html",
    "title": "VisualLens: Personalization through Visual History",
    "section": "",
    "text": "Recent Large Language Models (LLMs) can support contexts up to millions of tokens in length However, processing such long sequences with LLMs requires substantial computational and memory resources due to the quadratic complexity of the self-attention mechanism. To address these challenges, various techniques have been proposed such as Flash Attention and Ring Attention.\nIn many long-context tasks, the input consists of a long context followed by a short query and a short answer. The information needed for answering the query is often localized within small parts of the context, meaning context tokens need only attend to nearby tokens, while query tokens need to attend to all prior tokens.\nBased on this observation, researchers have introduced Star Attention, a novel algorithm for efficient LLM long-context inference. Star Attention utilizes a two-phase approach 1. Context Encoding: The context is divided into contiguous blocks and distributed across “context” hosts, with each host also receiving a copy of the first block (an “anchor block”). Hosts compute self-attention only for their assigned blocks, without communicating with each other, reducing attention complexity from quadratic to linear with respect to context length. 2. Query Encoding and Token Generation: The query is replicated across all hosts where it initially attends to the KV cache on each host. Global attention is then computed by aggregating the results at a designated “query” host by efficiently communicating a single vector and scalar per token from each context host. Only the query host updates its KV cache during this stage.\nStar Attention enables the context length to scale linearly with the number of hosts by distributing the context processing across multiple hosts. Star Attention is compatible with most Transformer Based LLMs trained with global attention, operating seamlessly out-of-the-box without additional model fine-tuning.\nDuring Evaluating Star Attention for Llama3.1-8B and Llama3.1-70B on several long-context benchmarks. Star Attention achieves up to 11 times faster inference while maintaining 95-100% of the baseline accuracy. Furthermore, Star Attention can be combined with other LLM optimization methods like Flash Attention or KV cache compression, allowing for additional speedup enhancements during inference.\nPaper: Star Attention: Efficient Long-Context Inference with Non-Global Attention"
  },
  {
    "objectID": "posts/LongKey/LongKey.html",
    "href": "posts/LongKey/LongKey.html",
    "title": "LongKey: Keyphrase Extraction for Long Documents",
    "section": "",
    "text": "In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving a gap in processing long-context documents.\nIntroducing LongKey, a novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses a max-pooling embedder to enhance keyphrase candidate representation. LongKey operates considering three stages:\nFirst, Initial word embedding is generated for long documents using the Longformer model, which handles extended contexts through sliding local and global attention mechanisms. It uses a tokenizer based on RoBERTa to convert documents into tokens, which are then processed by Longformer to create embeddings. Large documents are split into chunks of up to 8,192 tokens, and their embeddings are concatenated to form a unified representation.\nSecond, Keyphrase candidate embedding is context-sensitive, with each word represented by the first token to reduce computation. To create a single embedding for each word, only the first token of the word is used, reducing computational complexity. A convolutional network then generates n-gram embeddings (up to length 5), enhancing keyphrase relevance and ranking. This approach improves the precision and relevance of keyphrase extraction.\nFinally, candidate scoring are assigned ranking scores, with higher scores indicating keyphrases that better represent the document’s content. The model fine-tunes performance by optimizing ranking and chunking losses, using ground-truth keyphrases as positive samples and others as negative samples. Linear layers are used to generate ranking scores by converting candidate embeddings into single values.\nValidated on the comprehensive LDKP datasets and six diverse datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods marking an advancement in keyphrase extraction for varied text lengths and domains.\nPaper : LongKey: Keyphrase Extraction from Long Documents"
  },
  {
    "objectID": "posts/ETA/ETA.html",
    "href": "posts/ETA/ETA.html",
    "title": "Efficient Track Anything",
    "section": "",
    "text": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices.\nTo address this limitation, researchers from meta have proposed EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size.EfficientTAM takes a vanilla lightweight ViT image encoder for frame feature extraction. An efficient memory cross-attention is then used to further improve the efficiency of EfficientTAM by leveraging the strong locality of memory spatial embeddings, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. EfficientTAM is fully trained on SA-1B (image) and SA-V (video) for unified image and video segmentation.\nDuring evaluation on multiple video segmentation benchmarks including semi supervised VOS and promptable video segmentation, EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ∼2x speedup on A100 and ∼2.4x parameter reduction. On segment anything image tasks, EfficientTAMs also perform favorably over original SAM with ∼20x speedup on A100 and ∼20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, EfficientTAMs can run at ∼10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.\nPaper : Efficient Track Anything"
  },
  {
    "objectID": "posts/OHRBench/OHRBench.html",
    "href": "posts/OHRBench/OHRBench.html",
    "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
    "section": "",
    "text": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises.\nTo overcome this, researchers have introduced OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG.\nOHRBench and evaluation protocol consist of the following (1) Benchmark Dataset: collect PDF documents from six domains, extract human-verified ground truth structured data, and generate Q&As derived from multimodal document elements. (2) RAG Knowledge Base: OCR Processed Structured Data for benchmarking current OCR solutions and Perturbed Structured Data for assessing the impact of different OCR noise types. (3) Evaluation of OCR impact on each component and the overall RAG system.\nTo better understand OCR’s impact on RAG systems, let us understand two primary types of OCR noise: Semantic Noise, resulting from prediction errors exerts significant impact, and Formatting Noise, arising from non-uniform document element representation affects specific retrievers and LLMs differently. offering valuable insights for developing RAG-tailored OCR solutions and noise robust models.\nFurthermore, employing Vision-Language Models (VLMs) without OCR in RAG systems can be an effective alternative. VLM can improve the performance by up to 24.5% and approach the performance of the ground truth text baseline, indicating its promising potential of applying VLMs in RAG systems.\nPaper : OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation"
  },
  {
    "objectID": "posts/PaliGemma2/PaliGemma2.html",
    "href": "posts/PaliGemma2/PaliGemma2.html",
    "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
    "section": "",
    "text": "Google has released the PaliGemma 2 family of models, an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. PailGemma 2 combines the SigLIP-So400m vision encoder with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model.\nPaliGemma 2 models is pretrained with a vocabulary that includes localization tokens (for detection) and segmentation tokens (to define a binary mask inside a bounding box) at three resolutions (224px2 , 448px2 and 896px2 ) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. PaliGemma 2 processes a 224px2 / 448px2 /896px2 image with a SigLIP-400m encoder with patch size 14px2 , yielding 256/1024/ 4096 tokens. After a linear projection, the image tokens are concatenated with the input text tokens and Gemma 2 autoregressively completes this prefix with an answer.\nPaliGemma 2 models were evaluated by increasing the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.\nPaper : PaliGemma 2: A Family of Versatile VLMs for Transfer"
  },
  {
    "objectID": "posts/LiFT/LiFT.html",
    "href": "posts/LiFT/LiFT.html",
    "title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment",
    "section": "",
    "text": "Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences, which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions.\nTo address this researchers have proposed LIFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. At the core LIFT fine-tuning pipeline consist of three key steps of our: (1) Human Feedback Collection: Where it generate video-text pairs using prompts expanded from random category words with an LLM, then annotate them to create LIFT-HRA. (2) Reward Function Learning: a visual-language model LIFT-CRITIC, is trained to predict human preference scores across three dimensions, learning the reward function from the dataset. (3) T2V Model Alignment: LIFT-CRITIC evaluates the T2V-generated videos, assigns scores, and maps them into a reward weight to fine-tune the T2V model, aligning it with human preferences.\nFurther, researchers have constructed a Human Rating Annotation dataset, LIFTHRA, consisting of approximately 10k human annotations, each including a score and its corresponding reason. Based on this, a reward model LIFT-CRITIC is trained to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, the learned reward function is then leveraged to align the T2V model by maximizing the reward-weighted likelihood.\nAs a case study, researchers apply LIFT pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.\nPaper : LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment"
  },
  {
    "objectID": "posts/Coconut/Coconut.html",
    "href": "posts/Coconut/Coconut.html",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "section": "",
    "text": "Large language models (LLMs) are restricted to reason in the “language space”, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs.\nTo explore the potential of LLM reasoning in an unrestricted latent space, researchers from meta have introduced Coconut (Chain of Continuous Thought). Coconut involves a simple modification to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token. This modification frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully differentiable. To enhance the training of latent reasoning, Coconut employs a multi-stage training strategy, which effectively utilizes language reasoning chains to guide the training process.\nUnlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-first search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works.\nExperimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For mathematical reasoning, using continuous thoughts improves accuracy, similar to reasoning chains in language. This suggests that more continuous thoughts could help solve increasingly complex problems. In logical reasoning tasks (e.g., ProntoQA and ProsQA), Coconut and its variants outperform language-based reasoning chains, while using fewer tokens during inference. These results highlight the potential of latent reasoning and offer insights for future research.\nPaper : Coconut: Training Large Language Models to Reason in a Continuous Latent Space"
  },
  {
    "objectID": "posts/MiniMax-01/MiniMax-01.html",
    "href": "posts/MiniMax-01/MiniMax-01.html",
    "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
    "section": "",
    "text": "Recently, Long context LLMs have been pinnacle in further advancement of generative ai in various fields. Now researchers have introduced the MiniMax-01 series long context LLMs, including MiniMax-Text-01 and MiniMax-VL-01.\nMiniMax-Text-01 is a powerful language model boasting 456 billion total parameters, with 45.9 billion activated per token. To unlock its long-context capabilities, it adopts a hybrid architecture integrating Lightning Attention, Softmax Attention, and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies like Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, and Expert Tensor Parallel (ETP), its training context length extends to 1 million tokens, and it can handle up to 4 million tokens during inference.\nBuilding on MiniMax-Text-01’s prowess, researchers have also developed MiniMax-VL-01 for enhanced visual capabilities. It uses the “ViT-MLP-LLM” framework common in multimodal LLMs. It is initialized and trained using three key components: a 303-million-parameter Vision Transformer (ViT) for visual encoding, a randomly initialized two-layer MLP projector for image adaptation, and MiniMax-Text-01 as the base LLM. This model features a dynamic resolution mechanism. Input images are resized according to a pre-set grid, with resolutions ranging from 336×336 to 2016×2016, while maintaining a 336×336 thumbnail. The resized images are split into non - overlapping patches of the same size. These patches and the thumbnail are encoded separately and then combined to form a full image representation. As a result, MiniMax-VL-01 has achieved top-level performance on multimodal leaderboards, demonstrating its edge in complex multimodal tasks.\nExperiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering a 20-32 times longer context window.\nPaper : MiniMax-01: Scaling Foundation Models with Lightning Attention"
  },
  {
    "objectID": "posts/Mind Evolution/Mind Evolution.html",
    "href": "posts/Mind Evolution/Mind Evolution.html",
    "title": "Mind Evolution: Evolving Deeper LLM Thinking",
    "section": "",
    "text": "Recently Google have released an evolutionary search strategy for scaling inference time compute in Large Language Model called Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available.\nMind Evolution is a genetic search strategy that evolves a diverse population of candidate solutions, leveraging an LLM to generate, recombine and refine solution candidates based on feedback from an evaluator. The overall process is analogous to combining divergent thinking (free-flowing parallel idea exploration) with convergent thinking (idea evaluation and selection), considered as hallmarks of intelligent problem solving behavior.\nThe core components of Mind Evolution involved: (1). the specific choices for the selection and migration operations; (2). the set of prompts that implement the initialization, recombination (crossover and mutation), and island reset operations with an LLM; (3). the fitness function that evaluates the quality of a given solution and optionally provides feedback on issues detected. The overall evolution process is repeated until a valid solution is found, or until 𝑁gens generations have been completed, after which the best scoring candidate is returned.\nFor instance let consider an example problem from the Trip Planning task in Natural Plan, with the predicted plans from Mind Evolution and the baselines. 1-Pass and Best-of-N both make mistakes on the number of days to stay, but satisfy the requirements of being in Madrid and Santorini on specific days. The Sequential-Revision+ plan omits the annual show in Madrid and plans a non-existent flight, but is correct in the number of days. In contrast, the Mind Evolution plan satisfies all specified requirements\nControlling for inference cost, It was found that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks.\nPaper: https://arxiv.org/pdf/2501.09891"
  },
  {
    "objectID": "posts/DeepSeek-R1/DeepSeek-R1.html",
    "href": "posts/DeepSeek-R1/DeepSeek-R1.html",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models via Reinforcement Learning",
    "section": "",
    "text": "A typical training process for LLMs consists of three phases: (1) Pre-training: In this stage, LLMs are pre-trained on vast amounts of text and code to learn general-purpose knowledge. (2) Supervised Fine-tuning: In this stage, the model is fine-tuned on an instruction dataset and finally (3) Reinforcement Learning from Human Feedback (RLHF), where the model is trained based on human feedback.\nSo what’s so different about DeepSeek-R1? Well, to train DeepSeek-R1-Zero the supervised fine-tuning stage is completely omitted. To run reinforcement learning at a large scale, a rule-based reinforcement learning method is employed called Group Relative Policy Optimization (GRPO). Given a model to train and an input problem, the input is fed into the model, and a group of outputs is sampled. Each output consists of a reasoning process and an answer. The GRPO method observes these sampled outputs and trains the model to generate the preferred options by calculating a reward for each output using predefined rules: (1) Accuracy: One set of rules calculates an accuracy reward. (2) Format: Another type of rule creates format rewards.\nThis rule-based mechanism, which does not use a neural model to generate rewards, simplifies and reduces the cost of the training process, making it feasible at a large scale. Through reinforcement learning, the model naturally learns to allocate more thinking time when solving reasoning tasks. Amazingly, this occurs without any external adjustments and yes this is the `Aha Moment’ that the AI community is all discussing about.\nHowever, DeepSeek-R1-Zero suffers from Readability Issues and Language Consistency. To overcome this, researchers have introduced DeepSeek-R1, which is trained on a four phase pipeline.\nCold Start (Phase 1): Incorporating a supervised fine-tuning phase on small, high-quality dataset helps DeepSeek-R1 mitigate the readability issues observed in the initial model. Reasoning Reinforcement Learning (Phase 2): This phase applies the same large-scale reinforcement learning that was reviewed for the previous model to enhance the model’s reasoning capabilities. Rejection Sampling and Supervised Fine-Tuning (Phase 3): In this phase only correct and readable samples are retained. Diverse Reinforcement Learning Phase (Phase 4): This final phase Rule-based rewards are utilized for tasks that allow that, such as math. For other tasks, a LLM provides feedback to align the model with human preferences.\nIn conclusion, a 32 billion parameters distilled model has demonstrated impressive performance, making it a viable smaller alternative with high reasoning capabilities and yes also triggering 108 Billion USD stock selloff :)\npaper: DeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models via Reinforcement Learning"
  },
  {
    "objectID": "posts/DuoGuard/DuoGuard.html",
    "href": "posts/DuoGuard/DuoGuard.html",
    "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
    "section": "",
    "text": "The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages.\nTo address this gap, researchers have proposed DuoGuard, a guardrail LLM trained with two-player reinforcement learning framework designed to enhance multilingual safeguard for large language models (LLMs). DuoGuard enables the co-evolution of a generator and a guardrail model adversarially to produce high-quality synthetic data for multilingual guardrail training. DuoGuard theoretically formalizes interaction as a two-player game, proving convergence to a Nash equilibrium.\nOverall DuoGuard two-player training pipeline consists of a generator that produces synthetic data from seed data.Then the classifier makes predictions and measures these examples as being predicted correctly or incorrectly based on their seed data label. Finally the model is trained with a generator with DPO to create increasingly challenging examples, which in turn improve the classifier through iterative training.\nEmpirical evaluations show that model DuoGuard outperforms state-of-theart models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5× faster at inference with a significantly smaller model (0.5B). it achieves substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for low resource languages in a collected real dataset. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety.\nPaper : DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails"
  },
  {
    "objectID": "posts/RTV/RTV.html",
    "href": "posts/RTV/RTV.html",
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
    "section": "",
    "text": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning LLMs with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked.\nTo address this gap researchers have started exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity and have introduced a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. Further researchers have also proposed a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, it was found that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate these methods’ effectiveness and scalability.\nThe proposed RLHF pipeline consists of two sequential phases: (1) Reward Model Training, where we construct three complementary reward models—namely, the Bradley-Terry (BT) model, the Generative Reward Model (GenRM), and Reasoning Task Verifiers (RTV). Specifically, the BT model is trained on pairwise comparisons to capture human preferences, while the GenRM assigns explicit reward scores aligned with these preferences using either ground-truth solutions (for reasoning tasks) or the best-of-N selections identified by the BT model (for general tasks). The RTV component implements specialized validators tailored to specific task requirements, such as code-execution sandboxes for evaluating programming tasks; and (2) Reinforcement Learning Optimization, in which the language model is iteratively optimized using PPO under guidance from both GenRM and RTV. This stage leverages carefully selected training prompts identified through our Pre-PPO prompt-selection method and employs strategic optimization techniques to robustly enhance model performance and alignment.\nResults show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Such strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.\nPaper : Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback"
  },
  {
    "objectID": "posts/BitNetv2/BitNetv2.html",
    "href": "posts/BitNetv2/BitNetv2.html",
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs",
    "section": "",
    "text": "Recently pioneering work like BitNet b1.58 demonstrated that 1.58-bit LLMs can match full-precision performance while drastically reducing inference costs (latency, memory, throughput, energy) and it’s all possible due to lower bit-width activations, which is crucial for maximizing hardware utilization, particularly for efficient kernel design in batched inference scenarios.\nHowever research highlights a key challenge: the non-uniform distribution of activations within LLMs. While inputs to attention and feed-forward network (FFN) layers often exhibit Gaussian-like distributions amenable to quantization, their intermediate states (outputs before final projection) contain significant outliers, hindering aggressive low-bit quantization.\nTo address this researchers have introduced BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. The core innovation is H-BitLinear, a novel linear layer replacing the standard output projections in attention and down projections in FFNs. H-BitLinear applies an online Hadamard transformation before activation quantization. This strategically reshapes the sharp, outlier-prone distributions of intermediate states into more manageable, Gaussian-like forms, significantly reducing the impact of outliers in 1.58-bit models.\nBitNet v2 is implemented using LLaMAlike components, including RMS normalization, SwishGLU and removing all bias. Compared to BitNet, H-BitLinear is used for Wo in attention and Wdown in FFN layers to deal with outlier channels of intermediate states. BitNet v2 is trained with 1.58-bit weights and INT8 activations from scratch, then continue-trained with INT4 activations for all linear layers except input/output embedding.\nExtensive experiments demonstrate that 4-bit BitNet v2 variant achieves performance comparable to BitNet a4.8 while offering superior computational efficiency for batched inference scenarios.\nPaper : BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs"
  },
  {
    "objectID": "posts/Falcon-H1/Falcon-H1.html",
    "href": "posts/Falcon-H1/Falcon-H1.html",
    "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance",
    "section": "",
    "text": "Hybrid state-space models (SSMs) like Jamba, Samba, Zamba, and Hymba combine the strengths of two different architectures. They merge attention mechanisms, which are great at understanding relationships over long sequences of data, with state-space models, which are efficient at blending information from the entire sequence. This fusion allows the models to effectively handle long-range dependencies while maintaining computational efficiency. Building on these insights, TII have introduced Falcon-H1—an innovative series of large language models that feature a novel parallel hybrid architecture integrating transformer-style attention with Mamba-based state-space models (SSMs). Falcon-H1 harnesses the complementary strengths of both mechanisms to deliver faster inference, lower memory usage, and state-of-the-art performance across a wide array of benchmarks. Falcon-H1 architecture uses Attention and SSM run in parallel within each block; their outputs are concatenated before the block’s output projection. The number of SSM/Attention heads can be flexibly tuned. Such a parallel hybrid design has the freedom to choose the ratio of attention and SSM channels, and is able to keep a small share of attention heads for precision while SSMs handle most of the work. The Falcon-H1 series of models, ranging from 0.5B to 34B parameters, combines Transformer and Mamba architectures. Notably, the Falcon-H1-34B-Instruct model delivers performance on par with or better than larger models like Qwen3-32B, Qwen2.5-72BInstruct, and LLaMA3.3-70B-Instruct, despite being roughly half their size.\nPaper : arXiv\nModel : falcon-h1"
  },
  {
    "objectID": "posts/MemAct/MemAct.html",
    "href": "posts/MemAct/MemAct.html",
    "title": "Memory As Action (MemAct) : Autonomous Context Curation For Long-Horizon Agentic Tasks",
    "section": "",
    "text": "For agentic tasks demanding long-horizon reasoning and complex tool use, such as deep research and software engineering, the effectiveness of a Large Language Model (LLM) is fundamentally constrained by distracting or irrelevant context. Therefore, the critical bottleneck for long-horizon tasks shifts from merely expanding memory capacity to actively curating its contents. To achieve such a meta-task Context Curation mechanism agent required to learn to autonomously manage its own working memory by strategically selecting, integrating, and pruning information to maintain a focused and goal-relevant reasoning trace.\nRecent advances in long-context methods, enabled by techniques such as positional-encoding scaling, long-context sample synthesis and sparse/linear attention mechanisms, have successfully expanded the capacity of an agent’s working memory. However, simply increasing the context window does not guarantee improved reasoning performance.\nTo address this researchers have introduced Memory-as-Action (MemAct), a framework that treats context curation as a sequence of learnable memory-editing operations. Rather than passively accumulating an ever-growing prefix, the agent learns to decide when to retain, compress, or discard segments of history, and may insert summaries to maintain coherence. These transformations are applied through explicit function-call actions, enabling the agent to develop memory strategies that improve reasoning efficiency.\nIn conventional memory management memory operations—such as selection, compression, and summarization—are governed by handcrafted heuristics or external controllers. These behaviors remain decoupled from the agent’s core decision-making process. In contrast, the Memory-as-Action (MemAct) framework integrates such operations into the policy itself, enabling the agent to learn when and how to edit its own working memory as part of a unified decision loop. This formulation supports goal-directed, policy-driven memory management.\nPaper : Memory As Action (MemAct) : Autonomous Context Curation For Long-Horizon Agentic Tasks"
  }
]